{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A very important aspect of supervised and semi-supervised machine learning is the quality of the labels produced by human labelers. Unfortunately, humans are not perfect and in some cases may even maliciously label things incorrectly. In this assignment, you will evaluate the impact of incorrect labels on a number of different classifiers.\n",
    "\n",
    "We have provided a number of code snippets you can use during this assignment. Feel free to modify them or replace them.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "The dataset you will be using is the [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). This dataset was created by Ronny Kohavi and Barry Becker and was used to predict whether a person's income is more/less than 50k USD based on census data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Start by loading and preprocessing the data. Remove NaN values, convert strings to categorical variables and encode the target variable (the string <=50K, >50K in column index 14)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.544210Z",
     "start_time": "2025-09-27T05:00:22.730515Z"
    }
   },
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import numba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.core.interchange.dataframe_protocol import Column, DataFrame\n",
    "from pandas.core.util.hashing import hash_pandas_object\n",
    "from setuptools.config.pyprojecttoml import load_file\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#import kshingle as ks\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple, NamedTuple\n",
    "import re\n",
    "import json\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.658138Z",
     "start_time": "2025-09-27T05:00:24.550417Z"
    }
   },
   "source": [
    "# This can be used to load the dataset\n",
    "data = pd.read_csv(\"adult.csv\", header=0, na_values='?')\n",
    "data = data.dropna()\n",
    "\n",
    "data = data.convert_dtypes()\n",
    "\n",
    "numericals = data.select_dtypes(include=[np.number]).columns\n",
    "categoricals = data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "data[categoricals] = data[categoricals].astype('category')\n",
    "\n",
    "encoder = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categoricals)], remainder='passthrough')\n",
    "\n",
    "#dt = encoder.fit_transform(data)\n",
    "\n",
    "#for c in data.columns:\n",
    "#    if data[c].dtype == 'object':\n",
    "#        data[c] = pd.Categorical(data[c])\n",
    "\n",
    "#print(dt)\n",
    "data.head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country salary  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.700878Z",
     "start_time": "2025-09-27T05:00:24.698347Z"
    }
   },
   "source": [
    "print(data.dtypes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                  Int64\n",
      "workclass         category\n",
      "fnlwgt               Int64\n",
      "education         category\n",
      "education-num        Int64\n",
      "marital-status    category\n",
      "occupation        category\n",
      "relationship      category\n",
      "race              category\n",
      "sex               category\n",
      "capital-gain         Int64\n",
      "capital-loss         Int64\n",
      "hours-per-week       Int64\n",
      "native-country    category\n",
      "salary            category\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data classification\n",
    "Choose at least 4 different classifiers and evaluate their performance in predicting the target variable. \n",
    "\n",
    "#### Preprocessing\n",
    "Think about how you are going to encode the categorical variables, normalization, whether you want to use all of the features, feature dimensionality reduction, etc. Justify your choices \n",
    "\n",
    "A good method to apply preprocessing steps is using a Pipeline. Read more about this [here](https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/) and [here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf). \n",
    "\n",
    "<!-- #### Data visualization\n",
    "Calculate the correlation between different features, including the target variable. Visualize the correlations in a heatmap. A good example of how to do this can be found [here](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec). \n",
    "\n",
    "Select a features you think will be an important predictor of the target variable and one which is not important. Explain your answers. -->\n",
    "\n",
    "#### Evaluation\n",
    "Use a validation technique from the previous lecture to evaluate the performance of the model. Explain and justify which metrics you used to compare the different models. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.766367Z",
     "start_time": "2025-09-27T05:00:24.763160Z"
    }
   },
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define your preprocessing steps here\n",
    "steps = []\n",
    "\n",
    "# Combine steps into a ColumnTransformer\n",
    "ct = ColumnTransformer(steps)\n",
    "\n",
    "# show the correlation between different features including target variable\n",
    "def visualize(data, ct):\n",
    "    pass\n",
    "\n",
    "# Apply your model to feature array X and labels y\n",
    "def apply_model(model, X, y):    \n",
    "    # Wrap the model and steps into a Pipeline\n",
    "    pipeline = Pipeline(steps=[('t', ct), ('m', model)])\n",
    "    \n",
    "    # Evaluate the model and store results\n",
    "    return evaluate_model(X, y, pipeline)\n",
    "\n",
    "# Apply your validation techniques and calculate metrics\n",
    "def evaluate_model(X, y, pipeline):\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label perturbation\n",
    "To evaluate the impact of faulty labels in a dataset, we will introduce some errors in the labels of our data.\n",
    "\n",
    "\n",
    "#### Preparation\n",
    "Start by creating a method which alters a dataset by selecting a percentage of rows randomly and swaps labels from a 0->1 and 1->0. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.801421Z",
     "start_time": "2025-09-27T05:00:24.799366Z"
    }
   },
   "source": [
    "\"\"\"Given a label vector, create a new copy where a random fraction of the labels have been flipped.\"\"\"\n",
    "def pertubate(y: np.ndarray, fraction: float) -> np.ndarray:\n",
    "    copy = data.copy()\n",
    "    # Flip fraction*len(data) of the labels in copy\n",
    "    return copy"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "Create a number of new datasets with perturbed labels, for fractions ranging from `0` to `0.5` in increments of `0.1`.\n",
    "\n",
    "Perform the same experiment you did before, which compared the performances of different models except with the new datasets. Repeat your experiment at least 5x for each model and perturbation level and calculate the mean and variance of the scores. Visualize the change in score for different perturbation levels for all of the models in a single plot. \n",
    "\n",
    "State your observations. Is there a change in the performance of the models? Are there some classifiers which are impacted more/less than other classifiers and why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.850521Z",
     "start_time": "2025-09-27T05:00:24.848238Z"
    }
   },
   "source": [
    "# Code"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations + explanations: max. 400 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "1)  Discuss how you could reduce the impact of wrongly labeled data or correct wrong labels. <br />\n",
    "    max. 400 words\n",
    "\n",
    "\n",
    "\n",
    "    Authors: Youri Arkesteijn, Tim van der Horst and Kevin Chong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "From part 1, you will have gone through the entire machine learning workflow which are they following steps:\n",
    "\n",
    "1) Data Loading\n",
    "2) Data Pre-processing\n",
    "3) Machine Learning Model Training\n",
    "4) Machine Learning Model Testing\n",
    "\n",
    "You can see these tasks are very sequential, and need to be done in a serial fashion. \n",
    "\n",
    "As a small perturbation in the actions performed in each of the steps may have a detrimental knock-on effect in the task that comes afterwards.\n",
    "\n",
    "In the final part of Part 1, you will have experienced the effects of performing perturbations to the machine learning model training aspect and the reaction of the machine learning model testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Data Discovery\n",
    "\n",
    "You will be given a set of datasets and you are tasked to perform data discovery on the data sets.\n",
    "\n",
    "<b>The datasets are provided in the group lockers on brightspace. Let me know if you are having trouble accessing the datasets</b>\n",
    "\n",
    "The process is to have the goal of finding datasets that are related to each other, finding relationships between the datasets.\n",
    "\n",
    "The relationships that we are primarily working with are Join and Union relationships.\n",
    "\n",
    "So please implement two methods for allowing us to find those pesky Join and Union relationships.\n",
    "\n",
    "Try to do this with the datasets as is and no processing.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Discovery algorithm:\n",
    "1. I scan each database with read_csv.\n",
    "2. I flatten each database, convert it to one string\n",
    "3. I shingle with k=8\n",
    "4. I calculate the Jaccard similarity between all pairs of shingles\n",
    "5. I return the list of relatedness between databases\n",
    "\n",
    "Comments:\n",
    "1. File sizes are manageable enough to directly calculate Jaccard similarity without relying on MinHash\n",
    "2. Tables 10 and 11 are the same\n",
    "3. For k=8, most similar tables are table_13.csv <> table_7.csv | plain_8=0.1207 | plain_containment_8 = 0.2582 | minhash_8=0.1328\n",
    "4.\n",
    "table_6.csv <> table_5.csv | plain_8=0.0958 | plain_containment_8 = 0.2043 | minhash_8=0.0703\n",
    "table_6.csv <> table_2.csv | plain_8=0.0858 | plain_containment_8 = 0.2544 | minhash_8=0.0859\n",
    "Significant Jaccard containment scores"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.880810Z",
     "start_time": "2025-09-27T05:00:24.879525Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.907045Z",
     "start_time": "2025-09-27T05:00:24.902842Z"
    }
   },
   "source": [
    "\n",
    "@numba.jit(nopython=True)\n",
    "def kshingle_manual(s, k=6):\n",
    "   sh = []\n",
    "   for i in range(len(s) - k + 1):\n",
    "       sh.append(s[i:i + k])\n",
    "   return sh\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:24.917924Z",
     "start_time": "2025-09-27T05:00:24.916063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "csv_path = \"./lake33\"\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:38.755691Z",
     "start_time": "2025-09-27T05:00:24.926134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "ColStruct = namedtuple(\"ColStruct\", [\"index\", \"col_name\", \"value\"])\n",
    "\n",
    "df_dict = {}\n",
    "df_col_dict = defaultdict(list)\n",
    "dirty = False\n",
    "\n",
    "for root, dirs, files in os.walk(csv_path):\n",
    "    for csv_file in files:\n",
    "        if os.path.splitext(csv_file)[1] == \".csv\":\n",
    "            file_path = os.path.join(root, csv_file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=None, engine='python', on_bad_lines='skip', header=0)\n",
    "                df = df.dropna().astype(object)\n",
    "                # print(df)\n",
    "            except Exception:\n",
    "                continue\n",
    "            # Serialize whole file (flattened)\n",
    "            df_str = df.to_numpy().flatten()\n",
    "            if not dirty:\n",
    "                df_str = \" \".join(str(x) for x in df_str).replace(\"\\n\", \" \")\n",
    "                df_str = re.sub(r\"\\s+\", \" \", df_str).strip()\n",
    "            df_dict[csv_file] = df_str\n",
    "\n",
    "            # Serialize each column for this file\n",
    "            df_col_dict[csv_file].clear()\n",
    "            for col_idx, col in enumerate(df.columns):\n",
    "                col_arr = df[col].to_numpy().flatten()\n",
    "                if not dirty:\n",
    "                    col_str = \" \".join(str(x) for x in col_arr).replace(\"\\n\", \" \")\n",
    "                    col_str = re.sub(r\"\\s+\", \" \", col_str).strip()\n",
    "                else:\n",
    "                    col_str = col_arr\n",
    "                df_col_dict[csv_file].append(ColStruct(index=col_idx, col_name=str(col), value=col_str))\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "22s to read files"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:00:39.292451Z",
     "start_time": "2025-09-27T05:00:39.288801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "num_perm = 128\n",
    "\n",
    "large_prime = np.uint64((1 << 61) - 1) # See datasketch.MinHash. Does not have to be a Mersenne Prime.\n",
    "\n",
    "max_hash = 2 ** 32 - 1\n",
    "\n",
    "def get_minwise_perm_params(num_perm=128):\n",
    "    local_seed = 1\n",
    "    gen = np.random.Generator(np.random.PCG64(local_seed)) # Apparently better than np.random.RandomState()\n",
    "    return np.array([\n",
    "        (\n",
    "            gen.integers(1, large_prime, dtype=np.uint64),\n",
    "            gen.integers(0, large_prime, dtype=np.uint64)\n",
    "        ) for _ in range(num_perm)\n",
    "    ],\n",
    "    dtype=np.uint64).T # datasketch.MinHash\n",
    "\n",
    "\n",
    "\n",
    "def minwise_independent_linear_permuation(hsh, num_perm=128): # (Bohman et. al., 2000)\n",
    "    a_vals, b_vals = get_minwise_perm_params(num_perm)\n",
    "    permuted_hashes = np.bitwise_and((a_vals * hsh + b_vals) % large_prime, max_hash) # datasketch.MinHash\n",
    "    return permuted_hashes\n",
    "\n",
    "def hash_shingle(shingle):\n",
    "    shingle_hash = hashlib.blake2b(shingle.encode(\"utf8\"),digest_size=8)\n",
    "    hashee = int.from_bytes(shingle_hash.digest())\n",
    "    return np.uint64(hashee)\n",
    "\n",
    "def MinHash_manual(shingle_set, num_perm=128):\n",
    "    minhash = np.ones(num_perm) * np.inf\n",
    "    for shingle in tqdm(shingle_set):\n",
    "        hash_val = hash_shingle(shingle)\n",
    "        permuted_hashes = minwise_independent_linear_permuation(hash_val)\n",
    "        minhash = np.minimum(minhash, permuted_hashes)\n",
    "    return minhash\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T03:57:31.985290Z",
     "start_time": "2025-09-27T03:57:28.336428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "minhash_files = {}\n",
    "minhash_cols = defaultdict(list)\n",
    "lookup = {}\n",
    "mh_file_ind = 0\n",
    "\n",
    "from CMinHash import CMinHash\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for fname, text in df_dict.items():\n",
    "    mh = MinHash()\n",
    "\n",
    "    for shingle in tqdm(kshingle_manual(text)):\n",
    "        mh.update(shingle.encode(\"utf8\"))\n",
    "\n",
    "    #for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "        #text_ks = kshingle_manual(col_value)\n",
    "        #for shingle in text_ks:\n",
    "        #    mh.update(shingle.encode(\"utf8\"))\n",
    "        #minhash_cols[fname].append((col_idx, col_name, mh))\n",
    "\n",
    "    minhash_files[fname] = mh\n",
    "\n",
    "\n",
    "print(f\"Mh files {minhash_files}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [00:00<00:00, 132092.88it/s]\n",
      "100%|██████████| 3325/3325 [00:00<00:00, 213899.92it/s]\n",
      "  2%|▏         | 299001/12188826 [00:01<00:52, 226154.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     14\u001B[39m mh = MinHash()\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m shingle \u001B[38;5;129;01min\u001B[39;00m tqdm(kshingle_manual(text)):\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     mh.update(shingle.encode(\u001B[33m\"\u001B[39m\u001B[33mutf8\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m#for (col_idx, col_name, col_value) in df_col_dict[fname]:\u001B[39;00m\n\u001B[32m     20\u001B[39m     \u001B[38;5;66;03m#text_ks = kshingle_manual(col_value)\u001B[39;00m\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m#for shingle in text_ks:\u001B[39;00m\n\u001B[32m     22\u001B[39m     \u001B[38;5;66;03m#    mh.update(shingle.encode(\"utf8\"))\u001B[39;00m\n\u001B[32m     23\u001B[39m     \u001B[38;5;66;03m#minhash_cols[fname].append((col_idx, col_name, mh))\u001B[39;00m\n\u001B[32m     25\u001B[39m minhash_files[fname] = mh\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:01:22.469195Z",
     "start_time": "2025-09-27T05:01:22.442401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "minhash_files = {}\n",
    "cminhash_files = {}\n",
    "minhash_cols = defaultdict(list)\n",
    "lookup = {}\n",
    "mh_file_ind = 0\n",
    "import copy\n",
    "from typing import Callable, Generator, Iterable, List, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasketch.hashfunc import sha1_hash32\n",
    "from CMinHash import CMinHash\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "hashvalue_byte_size = len(bytes(np.int64(42).data))"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:28:46.817369Z",
     "start_time": "2025-09-27T05:28:46.810880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numba as nb\n",
    "import numpy as np\n",
    "\n",
    "@nb.njit(cache=True)\n",
    "def rot_right_u64(phv: np.ndarray): # Author: Berken Tekin\n",
    "    \"\"\"Rotate right the hash values in-place.\"\"\"\n",
    "    # Transposing array here instead crashes the code\n",
    "    # phv shape: (R,_) with dtype=uint64\n",
    "    R = phv.shape[0]\n",
    "    #x = phv[0]\n",
    "    #new_head = x & np.uint64(1)\n",
    "    #phv[0] = (x >> np.uint64(1)) | (new_head << np.uint64(hashvalue_byte_size * 8 - 1))\n",
    "    for i in range(1, R): # I just don't rotate vector 0\n",
    "        x = phv[i-1]\n",
    "        new_head = x & np.uint64(1)\n",
    "        phv[i] = (x >> np.uint64(1)) | (new_head << np.uint64(hashvalue_byte_size * 8 - 1))\n",
    "    return phv\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T17:44:50.485401Z",
     "start_time": "2025-09-27T17:44:50.474278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@nb.njit(cache=True)\n",
    "def rot_right_u64_v2(phv: np.ndarray): # Author: Berken Tekin\n",
    "    \"\"\"Rotate right the hash values in-place.\"\"\"\n",
    "    # Transposing array here instead crashes the code\n",
    "    # phv shape: (R,_) with dtype=uint64\n",
    "    R = phv.shape[0]\n",
    "    #x = phv[0]\n",
    "    #new_head = x & np.uint64(1)\n",
    "    #phv[0] = (x >> np.uint64(1)) | (new_head << np.uint64(hashvalue_byte_size * 8 - 1))\n",
    "    for i in range(0, R): # I just don't rotate vector 0\n",
    "        x = np.copy(phv[i])\n",
    "        new_head = x & np.uint64(1)\n",
    "        phv[i] = (x >> np.uint64(1)) | (new_head << np.uint64(hashvalue_byte_size * 8 - 1))\n",
    "    return phv\n"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T18:09:14.020294Z",
     "start_time": "2025-09-27T18:09:14.016073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "@nb.njit(cache=True)\n",
    "def rot_right_u64_v3(d: np.uint64, phv: np.ndarray): # Author: Berken Tekin\n",
    "    \"\"\"Rotate right the hash values in-place.\"\"\"\n",
    "    # Transposing array here instead crashes the code\n",
    "    # phv shape: (R,_) with dtype=uint64\n",
    "    R = phv.shape[0]\n",
    "    #x = phv[0]\n",
    "    #new_head = x & np.uint64(1)\n",
    "    #phv[0] = (x >> np.uint64(1)) | (new_head << np.uint64(hashvalue_byte_size * 8 - 1))\n",
    "    for i in range(0, R): # I just don't rotate vector 0\n",
    "        x = np.copy(phv[i])\n",
    "        new_head = x & (~0 >> (64 - d))\n",
    "        phv[i] = (x >> np.uint64(d)) | (new_head << np.uint64(hashvalue_byte_size * 8 - d))\n",
    "    return phv\n"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T18:34:24.746529100Z",
     "start_time": "2025-09-27T18:14:21.536245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "@nb.njit(cache=True)\n",
    "def rot_right_u64_v2b(phv: np.ndarray): # Author: Berken Tekin\n",
    "    \"\"\"Rotate right the hash values in-place.\"\"\"\n",
    "    # Transposing array here instead crashes the code\n",
    "    # phv shape: (R,_) with dtype=uint64\n",
    "    R = phv.shape[0]\n",
    "    x = phv[0]\n",
    "    new_head = x & np.uint64(1)\n",
    "    phv[0] = (x >> np.uint64(1)) | (new_head << np.uint64(hashvalue_byte_size * 8 - 1))\n",
    "    for i in range(1, R): # I just don't rotate vector 0\n",
    "        x = phv[i-1]\n",
    "        new_head = x & np.uint64(1)\n",
    "        phv[i] = (x >> np.uint64(1)) | (new_head << np.uint64(hashvalue_byte_size * 8 - 1))\n",
    "    return phv\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T19:05:29.159822Z",
     "start_time": "2025-09-27T19:05:29.067104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# The size of a hash value in number of bytes\n",
    "hashvalue_byte_size = len(bytes(np.int64(42).data))\n",
    "\n",
    "# http://en.wikipedia.org/wiki/Mersenne_prime\n",
    "_mersenne_prime = np.uint64((1 << 61) - 1) # Does not have to be a Mersenne Prime, just a large one\n",
    "_max_hash = np.uint64((1 << 32) - 1)\n",
    "_hash_range = 1 << 32\n",
    "class CMinHashTest(CMinHash):\n",
    "    def __init__(self, num_perm: int = 128, seed: int = 1, hashfunc: Callable = sha1_hash32,\n",
    "                 hashobj: Optional[object] = None, hashvalues: Optional[Iterable] = None,\n",
    "                 permutations: Optional[Tuple[Iterable, Iterable]] = None, c_minhash_rotations: Optional[int] = None) -> None:\n",
    "        super().__init__(num_perm, seed, hashfunc, hashobj, hashvalues, permutations, c_minhash_rotations)\n",
    "        if c_minhash_rotations is not None: # Hard code\n",
    "            self.c_minhash_rotations = c_minhash_rotations\n",
    "        if hashvalues is not None:\n",
    "            num_perm = len(hashvalues)\n",
    "        if num_perm > _hash_range:\n",
    "            # Because 1) we don't want the size to be too large, and\n",
    "            # 2) we are using 4 bytes to store the size value\n",
    "            raise ValueError(\n",
    "                \"Cannot have more than %d number of\\\n",
    "                    permutation functions\"\n",
    "                % _hash_range\n",
    "            )\n",
    "        self.seed = seed\n",
    "        self.num_perm = num_perm\n",
    "        # Check the hash function.\n",
    "        if not callable(hashfunc):\n",
    "            raise ValueError(\"The hashfunc must be a callable.\")\n",
    "        self.hashfunc = hashfunc\n",
    "        # Check for use of hashobj and issue warning.\n",
    "        if hashobj is not None:\n",
    "            warnings.warn(\n",
    "                \"hashobj is deprecated, use hashfunc instead.\", DeprecationWarning\n",
    "            )\n",
    "        # Initialize hash values\n",
    "        if c_minhash_rotations is not None:\n",
    "            self.hashvalues = self._init_hashvalues(num_perm)\n",
    "        elif hashvalues is not None:\n",
    "            self.hashvalues = self._parse_hashvalues(hashvalues)\n",
    "        else:\n",
    "            self.hashvalues = self._init_hashvalues(num_perm)\n",
    "        # Initalize permutation function parameters\n",
    "        if permutations is not None:\n",
    "            self.permutations = permutations\n",
    "        else:\n",
    "            self.permutations = self._init_permutations(1)\n",
    "        if len(self) != len(self.permutations[0]) and self.c_minhash_rotations is None:\n",
    "            raise ValueError(\"Numbers of hash values and permutations mismatch\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def pf(self, hv):\n",
    "        a, b = self.permutations\n",
    "        return np.bitwise_and((hv * a + b) % _mersenne_prime, _max_hash)\n",
    "\n",
    "    def update_batch_cminhash_sigma_pi(self, b: Iterable) -> None:\n",
    "        \"\"\"Update this MinHash with new values.\n",
    "        The values will be hashed using the hash function specified by\n",
    "        the `hashfunc` argument in the constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.c_minhash_rotations is None:\n",
    "            raise ValueError(\"You need to specify c_minhash_rotations\")\n",
    "\n",
    "        hv = np.array([self.hashfunc(_b) for _b in b], dtype=np.uint64, ndmin=2).T\n",
    "\n",
    "        phv = self.pf(hv)\n",
    "        phv = np.repeat(phv, self.c_minhash_rotations, axis=1).T\n",
    "        phv = rot_right_u64_v2b(phv).T\n",
    "\n",
    "        self.hashvalues = np.vstack([phv, self.hashvalues]).min(axis=0)\n",
    "\n",
    "    def update_cminhash_sigma_pi(self, b: Iterable) -> None:\n",
    "        \"\"\"Update this MinHash with new values.\n",
    "        The values will be hashed using the hash function specified by\n",
    "        the `hashfunc` argument in the constructor.\n",
    "        \"\"\"\n",
    "        if self.c_minhash_rotations is None:\n",
    "            raise ValueError(\"You need to specify c_minhash_rotations\")\n",
    "\n",
    "        hv = self.hashfunc(b)\n",
    "\n",
    "        phv = self.pf(hv)\n",
    "        phv = np.repeat(phv.astype(np.uint64), self.c_minhash_rotations, axis=1).T\n",
    "        phv = rot_right_u64(phv).T\n",
    "\n",
    "\n",
    "        self.hashvalues = np.minimum(phv, self.hashvalues)\n",
    "\n",
    "for fname, text in df_dict.items():\n",
    "    cmh = CMinHashTest(num_perm=64, c_minhash_rotations=64)\n",
    "    mh = MinHash(num_perm=64)\n",
    "\n",
    "    ks = kshingle_manual(text)\n",
    "    max_buf=128\n",
    "    for i in tqdm(range(0, len(ks), max_buf), desc=f\"{fname} file cminhash\"):\n",
    "        bufsize = min(len(ks)-i, max_buf)\n",
    "        cmh.update_batch_cminhash_sigma_pi([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "    for (col_idx, col_name, col_value) in tqdm(df_col_dict[fname], desc=f\"{fname} cminhash\"):\n",
    "        cmh_col = CMinHashTest(num_perm=64, c_minhash_rotations=64)\n",
    "        ks = kshingle_manual(col_value)\n",
    "        cmh_col.update_batch_cminhash_sigma_pi([col_value.encode(\"utf8\")])\n",
    "\n",
    "\n",
    "\n",
    "    cminhash_files[fname] = cmh\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0, len(ks), max_buf), desc=f\"{fname} file minhash\"):\n",
    "        bufsize = min(len(ks)-i, max_buf)\n",
    "        mh.update_batch([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "    for (col_idx, col_name, col_value) in tqdm(df_col_dict[fname], desc = f\"{fname} minhash\"):\n",
    "        mh_col = MinHash(num_perm=64)\n",
    "        ks = kshingle_manual(col_value)\n",
    "        mh_col.update_batch([col_value.encode(\"utf8\")])\n",
    "\n",
    "\n",
    "    minhash_files[fname] = mh\n",
    "\n",
    "\n",
    "print(f\"Mh files {minhash_files}\")\n",
    "\n",
    "pickle.dump(minhash_files, open(\"minhash.pkl\", \"wb\"))\n",
    "pickle.dump(cminhash_files, open(\"cminhash.pkl\", \"wb\"))\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MinHash.__init__() takes from 1 to 7 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[112]\u001B[39m\u001B[32m, line 93\u001B[39m\n\u001B[32m     90\u001B[39m         \u001B[38;5;28mself\u001B[39m.hashvalues = np.minimum(phv, \u001B[38;5;28mself\u001B[39m.hashvalues)\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m fname, text \u001B[38;5;129;01min\u001B[39;00m df_dict.items():\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m     cmh = \u001B[43mCMinHashTest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_perm\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc_minhash_rotations\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     94\u001B[39m     mh = MinHash(num_perm=\u001B[32m64\u001B[39m)\n\u001B[32m     96\u001B[39m     ks = kshingle_manual(text)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[112]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mCMinHashTest.__init__\u001B[39m\u001B[34m(self, num_perm, seed, hashfunc, hashobj, hashvalues, permutations, c_minhash_rotations)\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, num_perm: \u001B[38;5;28mint\u001B[39m = \u001B[32m128\u001B[39m, seed: \u001B[38;5;28mint\u001B[39m = \u001B[32m1\u001B[39m, hashfunc: Callable = sha1_hash32,\n\u001B[32m     10\u001B[39m              hashobj: Optional[\u001B[38;5;28mobject\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m, hashvalues: Optional[Iterable] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m     11\u001B[39m              permutations: Optional[Tuple[Iterable, Iterable]] = \u001B[38;5;28;01mNone\u001B[39;00m, c_minhash_rotations: Optional[\u001B[38;5;28mint\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnum_perm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhashfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhashobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhashvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpermutations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc_minhash_rotations\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m c_minhash_rotations \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: \u001B[38;5;66;03m# Hard code\u001B[39;00m\n\u001B[32m     14\u001B[39m         \u001B[38;5;28mself\u001B[39m.c_minhash_rotations = c_minhash_rotations\n",
      "\u001B[31mTypeError\u001B[39m: MinHash.__init__() takes from 1 to 7 positional arguments but 8 were given"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T18:54:44.623135Z",
     "start_time": "2025-09-27T18:54:44.573281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute Jaccard similarities to table_1.csv\n",
    "hash_scores = {f: minhash_files[f].jaccard(minhash_files['table_1.csv']) for f in minhash_files}\n",
    "chash_scores = {f: cminhash_files[f].jaccard(cminhash_files['table_1.csv']) for f in cminhash_files}\n",
    "\n",
    "# Sort keys for consistent plotting\n",
    "files_sorted = sorted(hash_scores.keys())\n",
    "hash_vals = [hash_scores[f] for f in files_sorted]\n",
    "chash_vals = [chash_scores.get(f, None) for f in files_sorted]\n",
    "\n",
    "# Print values\n",
    "print(minhash_files['table_2.csv'].jaccard(minhash_files['table_1.csv']))\n",
    "print(cminhash_files['table_2.csv'].jaccard(cminhash_files['table_1.csv']))\n",
    "for f in files_sorted:\n",
    "    print(f\"{f} {hash_scores[f]}\")\n",
    "for f in sorted(cminhash_files.keys()):\n",
    "    print(f\"{f} {cminhash_files[f].jaccard(cminhash_files['table_1.csv'])}\")\n",
    "\n",
    "# Plot MinHash similarities\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(files_sorted, hash_vals, marker='o', label='MinHash Jaccard')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Jaccard similarity')\n",
    "plt.title('MinHash vs table_1.csv')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot C-MinHash similarities\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Filter to only files present in cminhash\n",
    "files_ch_sorted = sorted(cminhash_files.keys())\n",
    "chash_vals_plot = [chash_scores[f] for f in files_ch_sorted]\n",
    "plt.plot(files_ch_sorted, chash_vals_plot, marker='o', color='orange', label='C-MinHash Jaccard')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Jaccard similarity')\n",
    "plt.title('C-MinHash vs table_1.csv')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot compute Jaccard given MinHash with                    different numbers of permutation functions",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[105]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Compute Jaccard similarities to table_1.csv\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m hash_scores = {f: \u001B[43mminhash_files\u001B[49m\u001B[43m[\u001B[49m\u001B[43mf\u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjaccard\u001B[49m\u001B[43m(\u001B[49m\u001B[43mminhash_files\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtable_1.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m minhash_files}\n\u001B[32m      5\u001B[39m chash_scores = {f: cminhash_files[f].jaccard(cminhash_files[\u001B[33m'\u001B[39m\u001B[33mtable_1.csv\u001B[39m\u001B[33m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m cminhash_files}\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Sort keys for consistent plotting\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\dsait4000-assignment-1-group34\\Lib\\site-packages\\datasketch\\minhash.py:206\u001B[39m, in \u001B[36mMinHash.jaccard\u001B[39m\u001B[34m(self, other)\u001B[39m\n\u001B[32m    201\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    202\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCannot compute Jaccard given MinHash with\u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[32m    203\u001B[39m \u001B[33m            different seeds\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    204\u001B[39m     )\n\u001B[32m    205\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m) != \u001B[38;5;28mlen\u001B[39m(other):\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    207\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCannot compute Jaccard given MinHash with\u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[32m    208\u001B[39m \u001B[33m            different numbers of permutation functions\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    209\u001B[39m     )\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(np.count_nonzero(\u001B[38;5;28mself\u001B[39m.hashvalues == other.hashvalues)) / \u001B[38;5;28mfloat\u001B[39m(\n\u001B[32m    211\u001B[39m     \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m    212\u001B[39m )\n",
      "\u001B[31mValueError\u001B[39m: Cannot compute Jaccard given MinHash with                    different numbers of permutation functions"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "minhash_files = {}\n",
    "minhash_cols = defaultdict(list)\n",
    "lookup = {}\n",
    "mh_file_ind = 0\n",
    "\n",
    "for fname, text in df_dict.items():\n",
    "    text_ks = kshingle_manual(text)\n",
    "    minhash_files[fname] = MinHash_manual(text, num_perm=128)\n",
    "    for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "        text_ks = kshingle_manual(col_value)\n",
    "        minhash_col = MinHash_manual(text, num_perm=128)\n",
    "        tup = (col_idx, col_name, minhash_col)\n",
    "        minhash_cols[fname].append(tup)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "OneHash experiment\n",
    "I will try the OneHash variant"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def OneHash_auto(shingle_set, num_perm=128):\n",
    "    mh = MinHash(num_perm=1)\n",
    "    mh.update_batch(shingle_set.encode(\"utf8\") for shingle_set in tqdm(shingle_set))\n",
    "    return mh.digest()[:num_perm]\n",
    "\n",
    "def OneHash_manual(shingle_set, num_perm=128):\n",
    "    for shingle in tqdm(shingle_set):\n",
    "        hash_val = hash_shingle(shingle)\n",
    "        permuted_hashes = minwise_independent_linear_permuation(hash_val, num_perm=1)\n",
    "        minhash = permuted_hashes\n",
    "    return np.sort(minhash)[:num_perm]\n",
    "    return minhash\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "onehash_files = {}\n",
    "onehash_cols = defaultdict(list)\n",
    "lookup = {}\n",
    "mh_file_ind = 0\n",
    "\n",
    "for fname, text in df_dict.items():\n",
    "    text_ks = kshingle_manual(text)\n",
    "    onehash_files[fname] = (OneHash_auto(text, num_perm=128))\n",
    "    #for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "    #    text_ks = kshingle_manual(col_value)\n",
    "    #    onehash_col = OneHash_manual(text, num_perm=128)\n",
    "    #    tup = (col_idx, col_name, onehash_col)\n",
    "    #    onehash_cols[fname].append(tup)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Takes a minute or two"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "60000 it/s"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def discovery_algorithm_parse_tables(base_dir):\n",
    "    datasets = {}\n",
    "    ds = {}\n",
    "    if not os.path.isdir(base_dir):\n",
    "        return datasets  # return empty if folder not found\n",
    "    for fname in os.listdir(base_dir):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(base_dir, fname)\n",
    "            try:\n",
    "                df = pd.read_csv(fpath, sep=None, engine='python', on_bad_lines='skip')\n",
    "            except Exception:\n",
    "                continue\n",
    "            df = df.dropna().astype(object)\n",
    "            datasets[fname] = df.to_numpy().flatten()\n",
    "            string = \" \".join(str(x) for x in datasets[fname]).replace(\"\\n\", \" \")\n",
    "            # merge consecutive whitespaces (book advice)\n",
    "            string = re.sub(' +', ' ', string)\n",
    "            ds[fname] = string\n",
    "    return ds\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_shingles(text, k=8):\n",
    "    return set(kshingle_manual(text, k))\n",
    "\n",
    "def discovery_algorithm():\n",
    "    \"\"\"Function should be able to perform data discovery to find related datasets\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of pairs of related datasets\n",
    "    \"\"\"\n",
    "\n",
    "    base_dir = \"./lake33\"\n",
    "    ds = discovery_algorithm_parse_tables(base_dir)\n",
    "\n",
    "\n",
    "    def jaccard_set(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        return intersection / union\n",
    "\n",
    "    def jaccard_containment(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        def jacc_c1(set1, set2):\n",
    "            if len(set1) == 0:\n",
    "                return 0.0\n",
    "            return intersection / len(set1)\n",
    "        def jacc_c2(set1, set2):\n",
    "            if len(set2) == 0:\n",
    "                return 0.0\n",
    "            return intersection / len(set2)\n",
    "        return max(jacc_c1(set1, set2), jacc_c2(set1, set2))\n",
    "\n",
    "\n",
    "    # Cache shingles and MinHash per file per k to avoid recomputing\n",
    "    #    ks_range = range(8, 9)  # log20 suggests 7.92 (chapter 3 rule of thumb)\n",
    "    shingles_k = {fname: {} for fname in ds}\n",
    "    minhash_k = {fname: {} for fname in ds}\n",
    "    minhashlsh_k = {fname: {} for fname in ds}\n",
    "\n",
    "    for fname, text in ds.items():\n",
    "        for k in range(8,9):\n",
    "            kshingle = set(kshingle_manual(text, 8)) #log20\n",
    "            shingles_k[fname][k] = kshingle\n",
    "            mh = MinHash(num_perm=128)\n",
    "            for sh in kshingle:\n",
    "                try:\n",
    "                    mh.update(sh.encode(\"utf8\"))\n",
    "                except Exception:\n",
    "                    mh.update(str(sh).encode(\"utf8\"))\n",
    "            minhash_k[fname][k] = mh\n",
    "\n",
    "    # MinHash LSH\n",
    "    for fname, text in ds.items():\n",
    "        for k in range(8,9):\n",
    "            minhashlsh_k[fname][k] = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "            minhashlsh_k[fname][k].insert(fname, minhash_k[fname][k])\n",
    "\n",
    "    # Print plain Jaccard (set-based) and MinHash estimated Jaccard side by side for each k\n",
    "    filenames = list(ds.keys())\n",
    "    for i in range(len(filenames)):\n",
    "        for j in range(i + 1, len(filenames)):\n",
    "            f1 = filenames[i]\n",
    "            f2 = filenames[j]\n",
    "            parts = [f\"{f1} <> {f2}\"]\n",
    "            for k in range(8, 9):\n",
    "                s1 = shingles_k[f1][k]\n",
    "                s2 = shingles_k[f2][k]\n",
    "                plain_j = jaccard_set(s1, s2)\n",
    "                try:\n",
    "                    est_j = minhash_k[f1][k].jaccard(minhash_k[f2][k])\n",
    "                except Exception:\n",
    "                    est_j = None\n",
    "                est_lsh_j = minhashlsh_k[f1][k].jaccard(minhashlsh_k[f2][k], query_id=f1)\n",
    "                parts.append(f\"{plain_j:.4f}\") # Plain\n",
    "                parts.append(f\"{jaccard_containment(s1, s2):.4f}\")\n",
    "                parts.append(f\"{est_j if est_j is None else f'{est_j:.4f}'}\")\n",
    "                parts.append(f\"{jaccard_containment(s1, s2):.4f}\")\n",
    "            print(\" | \".join(parts))\n",
    "\n",
    "\n",
    "discovery_algorithm()\n",
    "print()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Book way"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#import kshingle as ks\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import re\n",
    "from collections import namedtuple\n",
    "import randomhash\n",
    "import json\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# python\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Iterable, Set, List\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataItem:\n",
    "    name: str\n",
    "    whole_file: bool\n",
    "\n",
    "    # Factory helpers for convenience\n",
    "    @staticmethod\n",
    "    def from_array(name: str, arr: Iterable[Any]) -> \"ArrayItem\":\n",
    "        return ArrayItem.create(name, arr)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(name: str, text: str) -> \"StringItem\":\n",
    "        return StringItem.create(name, text)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_shingles(name: str, shingles: Iterable[str]) -> \"ShingleItem\":\n",
    "        return ShingleItem.create(name, shingles)\n",
    "\n",
    "    # Abstract-like conversions (implemented in subclasses)\n",
    "    def to_string(self) -> \"StringItem\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def to_array(self) -> \"ArrayItem\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def to_shingles(self, k: int = 8) -> \"ShingleItem\":\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ArrayItem(DataItem):\n",
    "    value: np.ndarray  # 1D flattened array\n",
    "\n",
    "    @staticmethod\n",
    "    def create(name: str, arr: Iterable[Any]) -> \"ArrayItem\":\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            val = arr.flatten()\n",
    "        else:\n",
    "            val = np.array(list(arr), dtype=object).flatten()\n",
    "        return ArrayItem(name, val)\n",
    "\n",
    "    def to_array(self) -> \"ArrayItem\":\n",
    "        return self\n",
    "\n",
    "    def to_shingles(self, k: int = 8) -> \"ShingleItem\":\n",
    "        return self.to_string().to_shingles(k=k)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StringItem(DataItem):\n",
    "    value: str\n",
    "\n",
    "    @staticmethod\n",
    "    def create(name: str, text: str, normalize_spaces: bool = True) -> \"StringItem\":\n",
    "        if normalize_spaces:\n",
    "            text = re.sub(r\"\\s+\", \" \", text.replace(\"\\n\", \" \")).strip()\n",
    "        return StringItem(name, text)\n",
    "\n",
    "    def to_string(self) -> \"StringItem\":\n",
    "        return self\n",
    "\n",
    "    def to_shingles(self, k: int = 8) -> \"ShingleItem\":\n",
    "        s = self.value\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k must be positive\")\n",
    "        shingles: Set[str] = set(s[i : i + k] for i in range(max(0, len(s) - k + 1)))\n",
    "        return ShingleItem.create(self.name, shingles)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ShingleItem(DataItem):\n",
    "    value: frozenset\n",
    "\n",
    "    @staticmethod\n",
    "    def create(name: str, shingles: Iterable[str]) -> \"ShingleItem\":\n",
    "        clean = (str(sh) for sh in shingles)\n",
    "        return ShingleItem(name, frozenset(clean))\n",
    "\n",
    "    def to_string(self) -> \"StringItem\":\n",
    "        # Deterministic order for reproducibility\n",
    "        text = \" \".join(sorted(self.value))\n",
    "        return StringItem.create(self.name, text)\n",
    "\n",
    "    def to_array(self) -> \"ArrayItem\":\n",
    "        return ArrayItem.create(self.name, list(self.value))\n",
    "\n",
    "    def to_shingles(self, k: int = 8) -> \"ShingleItem\":\n",
    "        return self"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 0: import DBs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def _shingle(text: str, k: int = 8) -> Set[str]:\n",
    "    assert k > 0, \"k must be positive\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    n = len(text)\n",
    "    if n == 0:\n",
    "        return set()\n",
    "    return {text[i:i + k] for i in range(max(0, n - k + 1))}\n",
    "\n",
    "class CsvFile: # Like S_1 in book, or a collection of [S_1, S_2...] in terms of columns.\n",
    "    minhash: MinHash\n",
    "    minhash_cols: Dict[str, MinHash]\n",
    "    filename: str\n",
    "    all_string: str\n",
    "    column_strings: Dict[str, str]\n",
    "    all_shingles: Set[str]\n",
    "    column_shingles: Dict[str, Set[str]]\n",
    "\n",
    "\n",
    "    # Shingling helper\n",
    "\n",
    "\n",
    "    def to_string(self):\n",
    "        return self.all_string\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_minhash(self, curr_set):\n",
    "        for sh in curr_set:\n",
    "            try:\n",
    "                self.minhash.update(sh.encode(\"utf8\"))\n",
    "            except Exception:\n",
    "                self.minhash.update(str(sh).encode(\"utf8\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def np_to_string(n):\n",
    "        s = \" \".join(str(x) for x in n.flatten())\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "\n",
    "    def __init__(self, mhf, fp):\n",
    "\n",
    "        df = pd.read_csv(fp, sep=None, engine='python', on_bad_lines='skip', header=0)\n",
    "        df = df.dropna().astype(object)\n",
    "        self.filename = os.path.basename(fp)\n",
    "        self.all_string = np_to_string(df.to_numpy())\n",
    "        self.all_shingles = _shingle(self.all_string, k=8)\n",
    "\n",
    "        self.minhash = mhf\n",
    "        self.apply_minhash(self.minhash, self.all_shingles)\n",
    "\n",
    "        # Build serialized strings per column\n",
    "        self.column_strings = {}\n",
    "        for col in df.columns:\n",
    "            col_arr = df[col].to_numpy()\n",
    "            col_str = self.np_to_string(col_arr)\n",
    "            self.column_strings[str(col)] = self.np_to_string(col_arr)\n",
    "            self.column_shingles[str(col)] = _shingle(col_str, k=8)\n",
    "\n",
    "            self.minhash_cols[str(col)] = MinHash(num_perm=128)\n",
    "            self.apply_minhash(self.minhash_cols[str(col)], self.column_shingles[str(col)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CsvFilesKB:\n",
    "    files: Dict[str, CsvFile]\n",
    "\n",
    "    def load_file(self, mhf, fp):\n",
    "        new_file = CsvFile(mhf, fp)\n",
    "        fname = new_file.filename\n",
    "        self.files[fname] = new_file\n",
    "\n",
    "    def add_csv_files(self, directory):\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    fpath = os.path.join(root, file)\n",
    "                    try:\n",
    "                        self.load_file(fpath)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "    def jaccard_data(self):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.files = {}\n",
    "\n",
    "    def __add__(self, other):\n",
    "        self.files.update(other.files)\n",
    "        self.calculateData()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.files = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class KBRunner:\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def np_to_string(arr):\n",
    "    arr_t = \" \".join(str(x) for x in arr.flatten()).replace(\"\\n\", \" \")\n",
    "    return re.sub(' +', ' ', arr_t)\n",
    "\n",
    "def read_files(dir):\n",
    "    data_items = List\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "\n",
    "            if file.endswith(\".csv\"):\n",
    "                fpath = os.path.join(root, file)\n",
    "                try:\n",
    "                    df = pd.read_csv(fpath, sep=None, engine='python', on_bad_lines='skip', header=0)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                df = df.dropna().astype(object)\n",
    "                datasets[file] = DataItem.from_array(\"\", df.to_numpy().flatten())\n",
    "                for col in df.columns:\n",
    "                    # use the first row as label values for this column\n",
    "                    name = str(df[col].iloc[0])\n",
    "                    datasets[file] = DataItem.from_array(name, df[col].to_numpy().flatten())\n",
    "\n",
    "    return datasets\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 1: Create 8-shingles from data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_whole_file(item: DataItem) -> bool:\n",
    "    return item.name == \"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def step_1_kshingles_cols(data_string: DataItem, k=8):\n",
    "\n",
    "    return data_string.to_shingles(k)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def step_1_kshingles(data_strs: Dict, k=8):\n",
    "    shingle_sets = {}\n",
    "    for fname, string in datasets.items():\n",
    "        for shingle in kshingle_manual(string, k):\n",
    "            shingle_sets[fname].add(shingle)\n",
    "    return shingle_sets\n",
    "\n",
    "\n",
    "\n",
    "def minhash_wrapper(shingle_set: set):\n",
    "\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    for shingle in shingle_set:\n",
    "        try:\n",
    "            minhash.update(shingle.encode(\"utf8\"))\n",
    "        except Exception:\n",
    "            minhash.update(str(shingle).encode(\"utf8\"))\n",
    "    return minhash\n",
    "\n",
    "curr_shinglesets = step_1_kshingles(read_files(\"./lake33\"), 8)\n",
    "min_hashes = {k: minhash_wrapper(v) for k, v in curr_shinglesets.items()}\n",
    "print(min_hashes)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 2: Sort the document-shingle pairs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Python\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def build_minhash_from_shingles(shingle_set, num_perm=128):\n",
    "    mh = MinHash(num_perm=num_perm)\n",
    "    for sh in shingle_set:\n",
    "        mh.update(sh.encode(\"utf-8\"))\n",
    "    return mh\n",
    "\n",
    "# Build all shingles once\n",
    "\n",
    "# Build all MinHashes once with the same num_perm\n",
    "num_perm = 8\n",
    "mh_by_file = {fname: build_minhash_from_shingles(s, num_perm) for fname, s in curr_shinglesets.items()}\n",
    "\n",
    "# Estimate Jaccard between any two files\n",
    "def est_jaccard(f1, f2):\n",
    "    return mh_by_file[f1].jaccard(mh_by_file[f2])\n",
    "\n",
    "# Optional: LSH for fast candidate search (not for exact jaccard)\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=num_perm)\n",
    "for fname, mh in mh_by_file.items():\n",
    "    lsh.insert(fname, mh)\n",
    "\n",
    "# Query similar files to 'file_a'\n",
    "for fname in mh_by_file.keys():\n",
    "    candidates = [c for c in lsh.query(mh_by_file[fname]) if c != fname]\n",
    "    print(f\"Candidates for {fname}: {candidates}\")\n",
    "\n",
    "# Compute true set Jaccard if needed (slower but exact)\n",
    "def true_jaccard(f1, f2):\n",
    "    s1, s2 = curr_shinglesets[f1], curr_shinglesets[f2]\n",
    "    return len(s1 & s2) / len(s1 | s2) if (s1 or s2) else 0.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "Candidates for table_0.csv: []\n",
    "Candidates for table_1.csv: ['table_2.csv']\n",
    "Candidates for table_10.csv: ['table_11.csv']\n",
    "Candidates for table_11.csv: ['table_10.csv']\n",
    "Candidates for table_13.csv: []\n",
    "Candidates for table_14.csv: []\n",
    "Candidates for table_15.csv: []\n",
    "Candidates for table_16.csv: []\n",
    "Candidates for table_17.csv: []\n",
    "Candidates for table_2.csv: ['table_1.csv']\n",
    "Candidates for table_3.csv: []\n",
    "Candidates for table_5.csv: []\n",
    "Candidates for table_6.csv: []\n",
    "Candidates for table_7.csv: []\n",
    "```\n",
    "\n",
    "These values show up even when num_perm=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would have noticed that the data has some issues in them.\n",
    "So perhaps those issues have been troublesome to deal with.\n",
    "\n",
    "Please try to do some cleaning on the data.\n",
    "\n",
    "After performing cleaning see if the results of the data discovery has changed?\n",
    "\n",
    "Please try to explain this in your report, and try to match up the error with the observation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Cleaning data, scrubbing, washing, mopping\n",
    "\n",
    "def cleaningData(data):\n",
    "    \"\"\"Function should be able to clean the data\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of cleaned datasets\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "1)  Different aspects of the data can effect the data discovery process. Write a short report on your findings. Such as which data quality issues had the largest effect on data discovery. Which data quality problem was repairable and how you choose to do the repair.\n",
    "\n",
    "<!-- For the set of considerations that you have outlined for the choice of data discovery methods, choose one and identify under this new constraint, how would you identify and resolve this problem? -->\n",
    "\n",
    "Max 400 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
