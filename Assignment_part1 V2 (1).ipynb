{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3XaxKH6Kftw"
   },
   "source": [
    "# Introduction\n",
    "A very important aspect of supervised and semi-supervised machine learning is the quality of the labels produced by human labelers. Unfortunately, humans are not perfect and in some cases may even maliciously label things incorrectly. In this assignment, you will evaluate the impact of incorrect labels on a number of different classifiers.\n",
    "\n",
    "We have provided a number of code snippets you can use during this assignment. Feel free to modify them or replace them.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "The dataset you will be using is the [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). This dataset was created by Ronny Kohavi and Barry Becker and was used to predict whether a person's income is more/less than 50k USD based on census data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PQMNKE8Kftx"
   },
   "source": [
    "### Data preprocessing\n",
    "Start by loading and preprocessing the data. Remove NaN values, convert strings to categorical variables and encode the target variable (the string <=50K, >50K in column index 14)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uWQ1LajpKftx"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FPV_mO_OKfty"
   },
   "source": [
    "# This can be used to load the dataset\n",
    "data = pd.read_csv(\"adult.csv\", na_values='?')\n",
    "data.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW1yPBViKfty"
   },
   "source": [
    "##### Check the percentage of missing values in the columns. Rule of thumb: If the percentage of missing values is above 60%, remove the feature."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4dsqYhcHKfty"
   },
   "source": [
    "for column in data.columns:\n",
    "    nan_count = data[column].isna().sum()/len(data)*100\n",
    "    print(\"Percentage of NaN in column \" + column + \" is \" + str(nan_count) + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiXVdGrBKftz"
   },
   "source": [
    "Remove all rows that contain nan values, since the columns with missing values can't be imputed (no numerical values)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lQ6OhS3jKftz"
   },
   "source": [
    "data_before = len(data)\n",
    "data = data.dropna()\n",
    "data_after = len(data)\n",
    "print(\"Removed \" + str(data_before-data_after) + \" rows from the \" + str(data_before) + \" rows\")\n",
    "\n",
    "data = data.drop(columns=[\"education\", \"fnlwgt\"])\n",
    "print(data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0yLs2bGKftz"
   },
   "source": [
    "Turn string columns into categorical data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MoGd9OOkKftz"
   },
   "source": [
    "string_columns = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "for col in string_columns:\n",
    "    data[col] = pd.Categorical(data[col])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q7mubsGcKftz"
   },
   "source": [
    "print(data['salary'].unique())\n",
    "data['salary'] = data['salary'].str.strip().str.replace(r\"\\.$\", \"\", regex=True)\n",
    "data['salary'] = pd.Categorical(data['salary'],categories=[\"<=50K\", \">50K\"],ordered=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnEYOemKft0"
   },
   "source": [
    "### Data classification\n",
    "Choose at least 4 different classifiers and evaluate their performance in predicting the target variable.\n",
    "\n",
    "#### Preprocessing\n",
    "Think about how you are going to encode the categorical variables, normalization, whether you want to use all of the features, feature dimensionality reduction, etc. Justify your choices\n",
    "\n",
    "A good method to apply preprocessing steps is using a Pipeline. Read more about this [here](https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/) and [here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf).\n",
    "\n",
    "<!-- #### Data visualization\n",
    "Calculate the correlation between different features, including the target variable. Visualize the correlations in a heatmap. A good example of how to do this can be found [here](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec).\n",
    "\n",
    "Select a features you think will be an important predictor of the target variable and one which is not important. Explain your answers. -->\n",
    "\n",
    "#### Evaluation\n",
    "Use a validation technique from the previous lecture to evaluate the performance of the model. Explain and justify which metrics you used to compare the different models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R2oKVSSbKft0"
   },
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# determine categorical and numerical features\n",
    "numerical_ix = ['age','education-num','capital-gain','capital-loss','hours-per-week']\n",
    "categorical_ix = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "\n",
    "# Define your preprocessing steps here\n",
    "steps = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_ix), ('num', MinMaxScaler() , numerical_ix)]\n",
    "\n",
    "\n",
    "# Apply your model to feature array X and labels y\n",
    "def apply_model(model, ct, X, y, feature_reduction = False):\n",
    "    pipeline_pca = Pipeline(steps=[('t', ct), ('pca', PCA(n_components=60)), ('m', model)])\n",
    "    pipeline_nopca = Pipeline(steps=[('t', ct) , ('m', model)])\n",
    "\n",
    "    return evaluate_model(X, y, pipeline_nopca, pipeline_pca, feature_reduction)\n",
    "\n",
    "# Apply your validation techniques and calculate metrics\n",
    "def evaluate_model(X, y, pipeline_nopca, pipeline_pca, feature_reduction=False):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    scores_nopca = cross_val_score(pipeline_nopca, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    scores_pca = cross_val_score(pipeline_pca, X, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "    #print(\"Mean accuracy without PCA:\", scores_nopca.mean())\n",
    "    #print(\"Mean accuracy with PCA   :\", scores_pca.mean())\n",
    "\n",
    "    y_pred = cross_val_predict(pipeline_pca, X, y, cv=cv)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    return scores_nopca.mean(), scores_pca.mean()\n",
    "\n",
    "### DEPRECATED METHOD AS PCA IS USED INSTEAD OF FEATURE IMPORTANCE\n",
    "\n",
    "# def show_feature_importance(pipeline, X, y, top_n = 12):\n",
    "#     model = pipeline.named_steps[\"m\"]\n",
    "#     feature_names = pipeline.named_steps[\"t\"].get_feature_names_out()\n",
    "\n",
    "#     importance = None\n",
    "\n",
    "#     if hasattr(model, \"feature_importances_\"):\n",
    "#         importance = model.feature_importances_ * 100\n",
    "#     elif hasattr(model, \"coef_\"):\n",
    "#         importance = abs(model.coef_[0])\n",
    "#     else:\n",
    "#         print(\"Using permutation importance (slower)...\")\n",
    "#         r = permutation_importance(pipeline, X, y, n_repeats=10, random_state=42)\n",
    "#         importance = r.importances_mean\n",
    "\n",
    "#     df = pd.DataFrame({\"feature\": feature_names, \"importance\": importance})\n",
    "\n",
    "#     df[\"base_feature\"] = (\n",
    "#         df[\"feature\"]\n",
    "#         .str.replace(r\"^cat__|^num__\", \"\", regex=True)   # remove prefixes\n",
    "#         .str.split(\"_\").str[0]                          # keep original feature name\n",
    "#     )\n",
    "\n",
    "#     agg_df = df.groupby(\"base_feature\")[\"importance\"].sum().sort_values(ascending=False)\n",
    "\n",
    "#     print(\"\\nTop Features (aggregated):\")\n",
    "#     print(agg_df.head(top_n))\n",
    "\n",
    "#     red = agg_df.head(top_n).index.to_list()\n",
    "\n",
    "#     # --- Plot aggregated importance ---\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.barplot(x=agg_df.head(top_n), y=agg_df.head(top_n).index, palette=\"viridis\")\n",
    "#     plt.title(f\"Aggregated Feature Importance ({type(model).__name__})\")\n",
    "#     plt.xlabel(\"Importance\")\n",
    "#     plt.ylabel(\"Feature\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return red\n",
    "\n",
    "# DEPRECATED METHOD AS PCA IS USED INSTEAD OF FEATURE SELECTION\n",
    "\n",
    "# def compare_and_plot(models, ct, X, y):\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     results = {}\n",
    "\n",
    "#     for name, model in models.items():\n",
    "#         # Full features\n",
    "#         pipeline_full = Pipeline(steps=[('t', ct), ('m', model)])\n",
    "#         scores_full = cross_val_score(pipeline_full, X, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "#         # Reduced features\n",
    "#         reduced_features = apply_model(model, ct, X, y, feature_reduction=True)\n",
    "#         X_reduced = X[reduced_features]\n",
    "\n",
    "#         cat_selected = [c for c in reduced_features if c in categorical_ix]\n",
    "#         num_selected = [c for c in reduced_features if c in numerical_ix]\n",
    "\n",
    "#         ct_reduced = ColumnTransformer([\n",
    "#             ('cat', OneHotEncoder(handle_unknown='ignore'), cat_selected),\n",
    "#             ('num', MinMaxScaler(), num_selected)\n",
    "#         ])\n",
    "#         pipeline_reduced = Pipeline(steps=[('t', ct_reduced), ('m', model)])\n",
    "#         scores_reduced = cross_val_score(pipeline_reduced, X_reduced, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "#         # Store mean difference\n",
    "#         results[name] = scores_reduced.mean() - scores_full.mean()\n",
    "\n",
    "#     # Plot differences\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     plt.barh(list(results.keys()), list(results.values()), color=\"skyblue\")\n",
    "#     plt.axvline(0, color=\"red\", linestyle=\"--\")\n",
    "#     plt.xlabel(\"Accuracy Difference (Reduced - Full)\")\n",
    "#     plt.title(\"Effect of Feature Reduction on Model Accuracy\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return results\n",
    "\n",
    "ct = ColumnTransformer(steps)\n",
    "\n",
    "models = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=10000),\n",
    "    \"SGD\": SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10000),\n",
    "    \"DecisionTree\": tree.DecisionTreeClassifier(),\n",
    "    \"LinearSVC\": LinearSVC()\n",
    "}\n",
    "\n",
    "results_nopca = []\n",
    "results_pca = []\n",
    "\n",
    "for model in models.values():\n",
    "    y = data['salary']\n",
    "    X = data.drop('salary', axis=1)\n",
    "\n",
    "    npca, ypca = apply_model(model, ct, X, y)\n",
    "    results_nopca.append(npca)\n",
    "    results_pca.append(ypca)\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x - width/2, results_nopca, width, label=\"No PCA\", color=\"steelblue\")\n",
    "plt.bar(x + width/2, results_pca, width, label=\"With PCA\", color=\"orange\")\n",
    "plt.xticks(x, list(models.keys()))\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.title(\"Model Performance: With vs Without PCA\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OjDvFJuKft0"
   },
   "source": [
    "### Label perturbation\n",
    "To evaluate the impact of faulty labels in a dataset, we will introduce some errors in the labels of our data.\n",
    "\n",
    "\n",
    "#### Preparation\n",
    "Start by creating a method which alters a dataset by selecting a percentage of rows randomly and swaps labels from a 0->1 and 1->0.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jv1-kPq7Kft0"
   },
   "source": [
    "\"\"\"Given a label vector, create a new copy where a random fraction of the labels have been flipped.\"\"\"\n",
    "def pertubate(y: np.ndarray, fraction: float) -> np.ndarray:\n",
    "    copy = y.copy()\n",
    "    n = len(y)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    flip_idx = rng.choice(n, size=int(fraction*n), replace=False)\n",
    "\n",
    "    copy.iloc[flip_idx] = 1 - copy.iloc[flip_idx]\n",
    "\n",
    "    return copy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcWHSVYwKft0"
   },
   "source": [
    "#### Analysis\n",
    "Create a number of new datasets with perturbed labels, for fractions ranging from `0` to `0.5` in increments of `0.1`.\n",
    "\n",
    "Perform the same experiment you did before, which compared the performances of different models except with the new datasets. Repeat your experiment at least 5x for each model and perturbation level and calculate the mean and variance of the scores. Visualize the change in score for different perturbation levels for all of the models in a single plot.\n",
    "\n",
    "State your observations. Is there a change in the performance of the models? Are there some classifiers which are impacted more/less than other classifiers and why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FuWE4XuMKft0"
   },
   "source": [
    "og_data = pd.read_csv(\"adult.csv\", na_values='?')\n",
    "data = og_data.copy()\n",
    "\n",
    "for column in data.columns:\n",
    "    nan_count = data[column].isna().sum()/len(data)*100\n",
    "\n",
    "data_before = len(data)\n",
    "data = data.dropna()\n",
    "data_after = len(data)\n",
    "\n",
    "data = data.drop(columns=['education', 'fnlwgt'])\n",
    "string_columns = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "for col in string_columns:\n",
    "    data[col] = pd.Categorical(data[col])\n",
    "\n",
    "data['salary'] = data['salary'].str.strip().str.replace(r\"\\.$\", \"\", regex=True)\n",
    "data['salary'] = data['salary'].replace({\"<=50K\":0, \">50K\":1})\n",
    "\n",
    "og_data = data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D0G5vpjckewO"
   },
   "source": [
    "salary = og_data['salary']\n",
    "\n",
    "data_00 = og_data.copy()\n",
    "\n",
    "data_01 = og_data.copy()\n",
    "data_01['salary'] = pertubate(salary, 0.1)\n",
    "\n",
    "data_02 = og_data.copy()\n",
    "data_02['salary'] = pertubate(salary, 0.2)\n",
    "\n",
    "data_03 = og_data.copy()\n",
    "data_03['salary'] = pertubate(salary, 0.3)\n",
    "\n",
    "data_04 = og_data.copy()\n",
    "data_04['salary'] = pertubate(salary, 0.4)\n",
    "\n",
    "data_05 = og_data.copy()\n",
    "data_05['salary'] = pertubate(salary, 0.5)\n",
    "\n",
    "# 4 different models\n",
    "lr = LogisticRegression()\n",
    "sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10000)\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "svc = LinearSVC()\n",
    "\n",
    "data = [(\"data_00\", data_00), (\"data_01\", data_01), (\"data_02\", data_02), (\"data_03\" ,data_03), (\"data_04\", data_04), (\"data_05\", data_05)]\n",
    "models = [lr, sgd, dt, svc]\n",
    "\n",
    "results = {\n",
    "    f\"data_{i:02d}\": {\n",
    "        model: {\"mean\": None, \"variance\": None}\n",
    "        for model in models\n",
    "    }\n",
    "    for i, _ in enumerate(data)\n",
    "}\n",
    "\n",
    "numerical_ix = ['age' ,'education-num','capital-gain','capital-loss','hours-per-week']\n",
    "categorical_ix = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "\n",
    "steps = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_ix), ('num', MinMaxScaler() , numerical_ix)]\n",
    "\n",
    "ct = ColumnTransformer(steps)\n",
    "\n",
    "for m in models:\n",
    "    for name, df in data:\n",
    "        scores = []\n",
    "\n",
    "        for r in range(0,5):\n",
    "            y = df['salary']\n",
    "            X = df.drop('salary', axis=1)\n",
    "\n",
    "            _, score = apply_model(m, ct, X, y)\n",
    "            scores.append(score)\n",
    "\n",
    "        mean = np.mean(scores)\n",
    "        variance = np.var(scores)\n",
    "\n",
    "        results[name][m][\"mean\"] = mean\n",
    "        results[name][m][\"variance\"] = variance\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xI9ye9kSkewP"
   },
   "source": [
    "data_names = [\"data_00\", \"data_01\", \"data_02\", \"data_03\", \"data_04\", \"data_05\"]\n",
    "x = range(len(data_names))  # 0..5\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for model in models:\n",
    "    means = [results[dname][model][\"mean\"] for dname in data_names]\n",
    "    variances = [results[dname][model][\"variance\"] for dname in data_names]\n",
    "    std_devs = np.sqrt(variances)\n",
    "\n",
    "    plt.plot(x, means, marker='o', label=model)\n",
    "    plt.fill_between(x,\n",
    "                     np.array(means) - std_devs,\n",
    "                     np.array(means) + std_devs,\n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.xticks(x, data_names)\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.title(\"Model Performance Across Datasets\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "10IunJVEkewP"
   },
   "source": [
    "records = []\n",
    "for dataset, models_dict in results.items():\n",
    "    for model_name, stats in models_dict.items():\n",
    "        records.append({\n",
    "            \"Dataset\": dataset,\n",
    "            \"Model\": model_name,\n",
    "            \"Mean\": stats[\"mean\"],\n",
    "            \"Variance\": stats[\"variance\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cteyZajKft0"
   },
   "source": [
    "Observations + explanations: max. 400 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYEnN0iEKft0"
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "1)  Discuss how you could reduce the impact of wrongly labeled data or correct wrong labels. <br />\n",
    "    max. 400 words\n",
    "\n",
    "\n",
    "\n",
    "    Authors: Youri Arkesteijn, Tim van der Horst and Kevin Chong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3VOEXokKft0"
   },
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "From part 1, you will have gone through the entire machine learning workflow which are they following steps:\n",
    "\n",
    "1) Data Loading\n",
    "2) Data Pre-processing\n",
    "3) Machine Learning Model Training\n",
    "4) Machine Learning Model Testing\n",
    "\n",
    "You can see these tasks are very sequential, and need to be done in a serial fashion.\n",
    "\n",
    "As a small perturbation in the actions performed in each of the steps may have a detrimental knock-on effect in the task that comes afterwards.\n",
    "\n",
    "In the final part of Part 1, you will have experienced the effects of performing perturbations to the machine learning model training aspect and the reaction of the machine learning model testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1BWet5eb3s2"
   },
   "source": [
    "# Part 2: Data Discovery - RADU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part implements four methods to find similar columns across different tables in a dataset, two for each relation: join and union. The methods are:\n",
    "1. Set Containment method (JOIN): Similarity between column values of different tables. \\\n",
    "2. Column name method (2xUNION): Similarity between column names of different tables. Measured in two different ways:\n",
    "    - Levenshtein distance (Shows how many single-character edits are needed to change one string into another)\n",
    "    - Jaccard similarity on shingles.\n",
    "3. JOSIE method (JOIN): Similarity between column values of different tables, using JOSIE. Creates posting lists and prints a top k list of similar columns (k=3).\n",
    "\n",
    "Thresholds have been set to 0.8, to avoid too many false positives.\n",
    "First, we read the datasets, and store them in a list of dataframes."
   ],
   "metadata": {
    "collapsed": false,
    "id": "hWKrdM1Wb3s2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "# Load the dataset\n",
    "\n",
    "def read_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        return df\n",
    "    except Exception:\n",
    "        return pd.read_csv(file_path, delimiter='_', low_memory=False)\n",
    "\n",
    "tables = []\n",
    "for i in range(0, 19):\n",
    "    ct = read_csv(f'lake33/table_{i}.csv')\n",
    "    ct.name = f'table_{i}.csv'\n",
    "    tables.append(ct)"
   ],
   "metadata": {
    "id": "7nzWtDi2b3s2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first run the methods on uncleaned data, to identify first results."
   ],
   "metadata": {
    "collapsed": false,
    "id": "96yy540Pb3s2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def set_containment(set_a, set_b):\n",
    "    \"\"\" Returns the containment of setA in setB \"\"\"\n",
    "    if len(set_a) == 0:\n",
    "        return 0\n",
    "    return len(set_a.intersection(set_b)) / len(set_a)"
   ],
   "metadata": {
    "id": "fOzRP39yb3s2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Set Containment method:\n",
    "def set_containment_method(dataset):\n",
    "    threshold = 0.8\n",
    "\n",
    "    rows_list = []\n",
    "    # Iterate through pairs of tables\n",
    "    for i, df1 in enumerate(tqdm.tqdm(dataset)):\n",
    "        for j, df2 in enumerate(dataset):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            # Iterate through pairs of columns of both tables:\n",
    "            for colidx1, col1 in enumerate(df1.columns):\n",
    "                for colidx2, col2 in enumerate(df2.columns):\n",
    "                    vals1 = set(df1[col1].dropna())\n",
    "                    vals2 = set(df2[col2].dropna())\n",
    "\n",
    "                    sc1 = set_containment(vals1, vals2)\n",
    "                    if sc1 >= threshold:\n",
    "                        dict = {}\n",
    "                        dict.update({'Dataset 1': df1.name, 'Column 1': str(col1), 'Dataset 2': df2.name, 'Column 2': str(col2), 'Set Containment': sc1, 'Relation': 'JOIN'})\n",
    "                        rows_list.append(dict)\n",
    "\n",
    "    result = pd.DataFrame(rows_list, columns=['Dataset 1', 'Dataset 2', 'Relation', 'Column 1', 'Column 2', 'Set Containment'])\n",
    "    return result\n",
    "\n",
    "result_sc_unclean = set_containment_method(tables)\n",
    "result_sc_unclean.to_csv('outputs/set_containment_results_unclean.csv', encoding='utf-8', index=False, header=True)"
   ],
   "metadata": {
    "id": "eMcnDWZib3s2",
    "outputId": "da441256-33b2-4b80-cac7-63813cffd550"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results method 1: (Runtime ~ 10 minutes)\n",
    "\n",
    "We notice the set containment method manages to identify many similarities between tables. For instance, it correctly captures similarities between tables 1 and 2, which appear to have similar data, even when the column titles are mismatched. This is the case for many tables, where the column names are different but the values are similar.\n",
    "\n",
    "However, this method yields a large number of false positives as well. For example, some tables appear to contain some sort of sensor data, which may have a small range of values that can potentially overlap with some other columns that have no connection to sensors, but a wide range of values. This yields a set containment value that exceeds the threshold, but the columns are not actually similar in any meaningful way.\n",
    "\n",
    "One method to mitigate this issue would be to consider the set containment relation in both directions, however that would significantly increase the computation time."
   ],
   "metadata": {
    "collapsed": false,
    "id": "LIg2e2G2b3s3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Column name method\n",
    "# Two ideas: Levenshtein distance, or Jaccard similarity on shingles.\n",
    "def levenshtein(s1, s2):\n",
    "    m = len(s1)\n",
    "    n = len(s2)\n",
    "\n",
    "    prev_row = [i for i in range(n + 1)]\n",
    "    curr_row = [0] * (n + 1)\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        curr_row[0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                curr_row[j] = prev_row[j - 1]\n",
    "            else:\n",
    "                curr_row[j] = min(prev_row[j - 1], prev_row[j], curr_row[j - 1]) + 1\n",
    "        prev_row = curr_row.copy()\n",
    "    return curr_row[n]\n",
    "\n",
    "def jaccard_similarity(s1, s2, k=2):\n",
    "    def get_shingles(s, k):\n",
    "        return {s[i:i+k] for i in range(len(s) - k + 1)}\n",
    "\n",
    "    shingles1 = get_shingles(s1, k)\n",
    "    shingles2 = get_shingles(s2, k)\n",
    "\n",
    "    intersection = len(shingles1.intersection(shingles2))\n",
    "    union = len(shingles1.union(shingles2))\n",
    "\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "def column_names(dataset):\n",
    "    threshold = 0.8\n",
    "    rows_list = []\n",
    "    for i, df1 in enumerate(tqdm.tqdm(dataset)):\n",
    "        for j, df2 in enumerate(dataset):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            for colidx1, col1 in enumerate(df1.columns):\n",
    "                for colidx2, col2 in enumerate(df2.columns):\n",
    "                    c1 = col1.lower()\n",
    "                    c2 = col2.lower()\n",
    "                    has_result = False\n",
    "\n",
    "                    dict = {'Dataset 1': df1.name, 'Column 1': str(col1), 'Dataset 2': df2.name, 'Column 2': str(col2), 'Relation': 'UNION'}\n",
    "\n",
    "                    lev_ratio = (levenshtein(col1.lower(), col2.lower())) / max(len(c1), len(c2))\n",
    "                    if lev_ratio <= (1 - threshold):\n",
    "                        has_result = True\n",
    "                        dict.update({'Levenshtein Ratio': 1 - lev_ratio})\n",
    "\n",
    "                    for k in range(2,6):\n",
    "                        sim = jaccard_similarity(c1, c2, k)\n",
    "                        if sim >= threshold:\n",
    "                            has_result = True\n",
    "                            dict.update({f'Jaccard (k={k})': sim})\n",
    "                    if has_result:\n",
    "                        rows_list.append(dict)\n",
    "\n",
    "    result = pd.DataFrame(rows_list, columns=['Dataset 1', 'Dataset 2', 'Relation', 'Column 1', 'Column 2', 'Levenshtein Ratio', 'Jaccard (k=2)', 'Jaccard (k=3)', 'Jaccard (k=4)', 'Jaccard (k=5)'])\n",
    "    return result\n",
    "\n",
    "result_cn_unclean = column_names(tables)\n",
    "result_cn_unclean.to_csv('outputs/column_name_results_unclean.csv', encoding='utf-8', index=False, header=True)"
   ],
   "metadata": {
    "id": "gxtQ32Wob3s3",
    "outputId": "3bd2631d-451d-41c5-b2a6-e1b004e20847"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results method 2 (methods 2.1 and 2.2): (Runtime ~ 75 seconds)\n",
    "\n",
    "The column name method is faster, as we only analyse the column names instead of searching through the whole values. The Levenshtein distance method manages to capture similarities when the column names are very similar. The threshold has been set to 20% difference, which means that the strings can differ by at most 20% of the length of the longer string. This may exclude shorter strings that are similar, but works well for longer strings which may contain spelling mistakes or shortcuts.\n",
    "\n",
    "The Jaccard similarity method on shingles captures similarities when column names have similar substrings. This works well for column names that differ by many characters semantically, but share common words or abbreviations.\n",
    "\n",
    "On the other hand, if the names are completely different, both methods cannot capture any similarity between tables, leading to a large number of false negatives."
   ],
   "metadata": {
    "collapsed": false,
    "id": "0OH1xq-db3s3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# JOSIE method\n",
    "\n",
    "def josie(dataset):\n",
    "    # Create posting lists\n",
    "    postings = {}\n",
    "    for i, df in enumerate(tqdm.tqdm(dataset)):\n",
    "        for j, col in enumerate(df.columns):\n",
    "            vals = set(df[col].dropna())\n",
    "            for entry in vals:\n",
    "                entry = str(entry)\n",
    "                if entry not in postings:\n",
    "                    postings[entry] = set()\n",
    "                postings[entry].add((df.name, j))\n",
    "\n",
    "    k = 3\n",
    "    rows_list = []\n",
    "\n",
    "    for i, df1 in enumerate(tqdm.tqdm(dataset)):\n",
    "        for j, col1 in enumerate(df1.columns):\n",
    "            vals1 = set(df1[col1].dropna())\n",
    "            counter = {}\n",
    "            for val in vals1:\n",
    "                val = str(val)\n",
    "                if val in postings:\n",
    "                    for (table_id, col_id) in postings[val]:\n",
    "                        if table_id == df1.name: # Do not match within the same table\n",
    "                            continue\n",
    "                        if (table_id, col_id) not in counter:\n",
    "                            counter[(table_id, col_id)] = 0\n",
    "                        counter[(table_id, col_id)] += 1\n",
    "            sorted_counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "            has_result = False\n",
    "            dict = {'Dataset 1': df1.name, 'Column 1': str(col1)}\n",
    "            for ((table_id, col_id), count) in sorted_counter[:(min(len(counter), k))]:\n",
    "                has_result = True\n",
    "                dict.update({f'Similar Column {len(dict)-1}': f'Table {table_id}, Column {col_id}, Overlap: {count}'})\n",
    "            if has_result:\n",
    "                rows_list.append(dict)\n",
    "\n",
    "    col_names = ['Dataset 1', 'Column 1']\n",
    "    for i in range(k):\n",
    "        col_names.append(f'Similar Column {i+1}')\n",
    "\n",
    "    result = pd.DataFrame(rows_list, columns=col_names)\n",
    "    return result\n",
    "\n",
    "result_josie_unclean = josie(tables)\n",
    "result_josie_unclean.to_csv('outputs/josie_results_unclean.csv', encoding='utf-8', index=False, header=True)"
   ],
   "metadata": {
    "id": "3TOfrPi_b3s3",
    "outputId": "76f96f67-ee31-4c3d-eb67-ae7c1ad13bf1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results method 3: (Runtime ~ 8 seconds)\n",
    "\n",
    "JOSIE does great at creating posting lists quickly, and uses them to quickly find similar columns. It is very fast and also creates a top k list of similar columns, which is useful for considering more potential matches.\n",
    "\n",
    "However, this method only uses overlap as a metric, which is not giving very insightful information on its own. This method works best if combined with other previous approaches, which could give a better idea if two columns are actually similar or not."
   ],
   "metadata": {
    "collapsed": false,
    "id": "dJFsqdw2b3s3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- Rest of assignment follows --"
   ],
   "metadata": {
    "collapsed": false,
    "id": "FBxm5R0Vb3s3"
   }
  },
  {
   "metadata": {
    "id": "wWUjEmzkb3tD"
   },
   "cell_type": "markdown",
   "source": [
    "# Part 2: Data Discovery MinHash - Berken Tekin"
   ]
  },
  {
   "metadata": {
    "id": "OQCPWEmgb3tE"
   },
   "cell_type": "markdown",
   "source": [
    "Discovery algorithm:\n",
    "1. Scan each database with read_csv.\n",
    "2. Flatten each database, convert it to one string\n",
    "3. Shingle with k=6\n",
    "4. The MinHash and CMinHash values are calculated for each set of shingles\n",
    "5. LSH returns pairs of columns with similarities above a threshold.\n"
   ]
  },
  {
   "metadata": {
    "id": "Vs0wb3VJb3tE"
   },
   "cell_type": "code",
   "source": [
    "import numba\n",
    "@numba.jit(nopython=True)\n",
    "def kshingle_manual(s, k=6):\n",
    "   sh = set()\n",
    "   for i in range(len(s) - k + 1):\n",
    "       sh.add(s[i:i + k])\n",
    "   return list(sh)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ihGvUOSdb3tE"
   },
   "cell_type": "markdown",
   "source": [
    "We run the experiment twice for dirty and clean datasets."
   ]
  },
  {
   "metadata": {
    "id": "CJk7R8YCb3tE"
   },
   "cell_type": "code",
   "source": "csv_path = \"./lake33\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2Yqop9fKb3tE",
    "outputId": "452b1f61-a7a6-4201-ec5b-55243db60934"
   },
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "ColStruct = namedtuple(\"ColStruct\", [\"index\", \"col_name\", \"value\"])\n",
    "\n",
    "df_dict = {}\n",
    "df_col_dict = defaultdict(list)\n",
    "\n",
    "for root, dirs, files in os.walk(csv_path):\n",
    "    for csv_file in files:\n",
    "        if os.path.splitext(csv_file)[1] == \".csv\":\n",
    "            file_path = os.path.join(root, csv_file)\n",
    "            try:\n",
    "                # Naive read_csv() as mentionmed ion the report\n",
    "                df = pd.read_csv(file_path, sep=None, engine='python', on_bad_lines='skip', header=0)\n",
    "                df = df.dropna().astype(object)\n",
    "                # print(df)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            # Serialize whole file (flattened)\n",
    "            df_str = df.to_numpy().flatten()\n",
    "            df_str = \" \".join(str(x) for x in df_str).replace(\"\\n\", \" \")\n",
    "            df_str = re.sub(r\"\\s+\", \" \", df_str).strip()\n",
    "            df_dict[csv_file] = df_str\n",
    "\n",
    "            # Serialize each column for this file\n",
    "            for col_idx, col in enumerate(df.columns):\n",
    "                col_arr = df[col].to_numpy().flatten()\n",
    "                col_str = \" \".join(str(x) for x in col_arr).replace(\"\\n\", \" \")\n",
    "                col_str = re.sub(r\"\\s+\", \" \", col_str).strip()\n",
    "                df_col_dict[csv_file].append(ColStruct(index=col_idx, col_name=str(col), value=col_str))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "fecO4A2db3tE"
   },
   "cell_type": "code",
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "from typing import Callable, Generator, Iterable, List, Optional, Tuple\n",
    "from datasketch.hashfunc import sha1_hash32\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "T8ZvfhU8b3tF"
   },
   "cell_type": "markdown",
   "source": [
    "The datasketch.MinHash module is a good starting point to build the CMinHash algorithm."
   ]
  },
  {
   "metadata": {
    "id": "FOAVvt07b3tF"
   },
   "cell_type": "markdown",
   "source": [
    "For MinHash, at the beginning I did not really understand what the algorithm does.\n",
    "Working on extending a module instead of starting from scratch helped me understand the algorithm better. I also learned about batch processing from datasketch.MinHash."
   ]
  },
  {
   "metadata": {
    "id": "iiUq6HK0b3tF"
   },
   "cell_type": "code",
   "source": [
    "minhash_files = {}\n",
    "cminhash_files = {}\n",
    "cminhash_files_pi = {}\n",
    "cminhash_cols = defaultdict(list)\n",
    "cminhash_cols_pi = defaultdict(list)\n",
    "minhash_cols = defaultdict(list)\n",
    "lookup = {}\n",
    "mh_file_ind = 0\n",
    "hashvalue_byte_size = 4\n",
    "# The size of a hash value in number of bytes\n",
    "\n",
    "\n",
    "class CMinHashTest(MinHash):\n",
    "\n",
    "    _large_prime = np.uint64((1 << 31) - 1) # A large prime\n",
    "    _hash_range = 1 << 31\n",
    "\n",
    "    def __init__(self, num_perm: int = 128, seed: int = 1234, hashfunc: Callable = sha1_hash32,\n",
    "                 hashobj: Optional[object] = None, hashvalues: Optional[Iterable] = None,\n",
    "                 permutations: Optional[Tuple[Iterable, Iterable]] = None, c_minhash_rotations: Optional[int] = None) -> None:\n",
    "        super().__init__(num_perm, seed, hashfunc, hashobj, hashvalues, permutations)\n",
    "        if c_minhash_rotations is not None: # Hard code\n",
    "            self.c_minhash_rotations = c_minhash_rotations\n",
    "        self.pi_permutations = self._init_pi_permutations()\n",
    "        self.sigma_permutations = self._init_sigma_permutations()\n",
    "        self.shift_right_precomputed = np.array([k for k in range(1, self.c_minhash_rotations + 1)], dtype=np.uint64).T\n",
    "\n",
    "\n",
    "    def _init_sigma_permutations(self):\n",
    "        gen = np.random.RandomState(self.seed + 1) # So we get different values for sigma permutation\n",
    "        return np.array([\n",
    "            gen.randint(1, self._large_prime, dtype=np.uint64),\n",
    "            gen.randint(0, self._large_prime, dtype=np.uint64),\n",
    "        ], dtype=np.uint64).T\n",
    "\n",
    "    def _init_pi_permutations(self):\n",
    "        # Create parameters for a random bijective permutation function\n",
    "        # that maps a 32-bit hash value to another 32-bit hash value.\n",
    "        # http://en.wikipedia.org/wiki/Universal_hashing\n",
    "        gen = np.random.RandomState(self.seed)\n",
    "        return np.array(\n",
    "            [\n",
    "                (\n",
    "                    gen.randint(1, self._large_prime, dtype=np.uint64),\n",
    "                    gen.randint(0, self._large_prime, dtype=np.uint64),\n",
    "                )\n",
    "            ],\n",
    "            dtype=np.uint64,\n",
    "        ).T\n",
    "\n",
    "\n",
    "    def sf(self, hv):\n",
    "        a, b = self.sigma_permutations\n",
    "        return (hv * a + b) % self._large_prime\n",
    "\n",
    "    def pf(self, hv) -> np.ndarray:\n",
    "        a, b = self.pi_permutations\n",
    "        return (hv * a + b) % self._large_prime\n",
    "\n",
    "    # Another way of shifting the permutations.\n",
    "    # The k offset is introduced at the end of the operation, instead of the beginning.\n",
    "    # Here, \"pia\" holds (a * k + b % p) for all k.\n",
    "    # https://github.com/beowolx/rensa/blob/95d80780f52f3105df4433132b279acd8c2762a0/src/cminhash.rs\n",
    "    def pf2(self, hv, pia) -> np.ndarray:\n",
    "        a, b = self.pi_permutations\n",
    "        return (hv * a + pia) % self._large_prime\n",
    "\n",
    "    def pf3(self, hv, off) -> np.ndarray:\n",
    "        a, b = self.pi_permutations\n",
    "        return (hv * (a - off) + b) % self._large_prime\n",
    "\n",
    "\n",
    "\n",
    "    def update_batch_cminhash_pi_pi(self, b: Iterable) -> None:\n",
    "        \"\"\"Update this MinHash with new values.\n",
    "        The values will be hashed using the hash function specified by\n",
    "        the `hashfunc` argument in the constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.c_minhash_rotations is None:\n",
    "            raise ValueError(\"You need to specify c_minhash_rotations\")\n",
    "\n",
    "        hv = np.array([self.hashfunc(_b) for _b in b], dtype=np.uint64, ndmin=2).T\n",
    "        phv = self.pf(hv)\n",
    "        phv = self.pf3(phv, self.shift_right_precomputed)\n",
    "\n",
    "        self.hashvalues = np.vstack([phv, self.hashvalues]).min(axis=0)\n",
    "\n",
    "\n",
    "    def update_batch_cminhash_sigma_pi(self, b: Iterable) -> None:\n",
    "        \"\"\"Update this MinHash with new values.\n",
    "        The values will be hashed using the hash function specified by\n",
    "        the `hashfunc` argument in the constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.c_minhash_rotations is None:\n",
    "            raise ValueError(\"You need to specify c_minhash_rotations\")\n",
    "\n",
    "        hv = np.array([self.hashfunc(_b) for _b in b], dtype=np.uint64, ndmin=2).T\n",
    "        phv = self.sf(hv)\n",
    "        phv = self.pf3(phv, self.shift_right_precomputed)\n",
    "\n",
    "        self.hashvalues = np.vstack([phv, self.hashvalues]).min(axis=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "UTUqHwjpb3tF",
    "outputId": "9f39aebd-2c24-48f7-c632-64ed4495db5c"
   },
   "cell_type": "code",
   "source": [
    "\n",
    "text_shinglesets = {}\n",
    "col_shinglesets = defaultdict(set)\n",
    "\n",
    "for fname, text in tqdm(df_dict.items()):\n",
    "    cmh = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "    cmhp = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "    mh = MinHash(num_perm=256)\n",
    "\n",
    "    ks = kshingle_manual(text)\n",
    "    text_shinglesets[fname] = tuple(ks)\n",
    "    max_buf=128\n",
    "    for i in range(0, len(ks), max_buf):\n",
    "        bufsize = min(len(ks)-i, max_buf)\n",
    "        cmh.update_batch_cminhash_sigma_pi([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "        cmhp.update_batch_cminhash_pi_pi([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "    for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "        cmh_col = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "        cmhp_col = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "\n",
    "        ks = kshingle_manual(col_value)\n",
    "        cmh_col.update_batch_cminhash_sigma_pi([e.encode(\"utf8\") for e in ks])\n",
    "        cmhp_col.update_batch_cminhash_pi_pi([e.encode(\"utf8\") for e in ks])\n",
    "        cminhash_cols[fname].append(ColStruct((fname, col_idx), col_name ,cmh_col))\n",
    "        cminhash_cols_pi[fname].append(ColStruct((fname, col_idx), col_name ,cmhp_col))\n",
    "    cminhash_files[fname] = cmh\n",
    "    cminhash_files_pi[fname] = cmhp\n",
    "\n",
    "\n",
    "    for i in range(0, len(ks), max_buf):\n",
    "        bufsize = min(len(ks)-i, max_buf)\n",
    "        mh.update_batch([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "    for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "        mh_col = MinHash(num_perm=256)\n",
    "        ks = kshingle_manual(col_value)\n",
    "        mh_col.update_batch([e.encode(\"utf8\") for e in ks])\n",
    "        minhash_cols[fname].append(ColStruct((fname, col_idx), col_name ,mh_col))\n",
    "\n",
    "        col_shinglesets[fname].add(ColStruct((fname, col_idx), col_name, tuple(ks)))\n",
    "\n",
    "\n",
    "    minhash_files[fname] = mh\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8-pYsKAIb3tF"
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def jaccard_manual(set1, set2):\n",
    "    set1 = set(set1)\n",
    "    set2 = set(set2)\n",
    "\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    if len(intersection) == 0:\n",
    "        return 0.0\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "hash_scores = {}\n",
    "chash_scores = {}\n",
    "chash_scores_pi = {}\n",
    "plain_scores = {}\n",
    "for fname, content in minhash_files.items():\n",
    "    hash_scores[fname] = {f: minhash_files[f].jaccard(content) for f in minhash_files.keys()}\n",
    "for fname, content in cminhash_files.items():\n",
    "    chash_scores[fname] = {f: cminhash_files[f].jaccard(content) for f in cminhash_files.keys()}\n",
    "\n",
    "for fname, content in cminhash_files_pi.items():\n",
    "    chash_scores_pi[fname] = {f: cminhash_files_pi[f].jaccard(content) for f in cminhash_files_pi.keys()}\n",
    "for fname, content in text_shinglesets.items():\n",
    "    plain_scores[fname] = {f: jaccard_manual(text_shinglesets[f], content) for f in text_shinglesets.keys()}\n",
    "\n",
    "df = pd.DataFrame(hash_scores)\n",
    "cdf = pd.DataFrame(chash_scores)\n",
    "pdf = pd.DataFrame(plain_scores)\n",
    "cpdf = pd.DataFrame(chash_scores_pi)\n",
    "\n",
    "df.to_csv(\"outputs/hash_scores_dirty.csv\", index=False)\n",
    "cdf.to_csv(\"outputs/chash_scores_dirty.csv\", index=False)\n",
    "pdf.to_csv(\"outputs/plain_scores_dirty.csv\", index=False)\n",
    "cpdf.to_csv(\"outputs/chash_scores_pi_dirty.csv\", index=False)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "rX5NdCNgb3tG",
    "outputId": "d61975c5-fe89-4232-a66c-e91e1c82e675"
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = sorted(pdf.index)\n",
    "n = len(files)\n",
    "cols = 3\n",
    "rows = (n + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, max(6, rows * 3)), sharey=True)\n",
    "axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "bar_width = 0.2\n",
    "offsets = (-1.5*bar_width, -0.5*bar_width, 0.5*bar_width, 1.5*bar_width)\n",
    "\n",
    "for idx, anchor in enumerate(files):\n",
    "    ax = axes[idx]\n",
    "    others = [f for f in files if f != anchor]\n",
    "    x = np.arange(len(others))\n",
    "\n",
    "    y_plain = [plain_scores[anchor][o] for o in others]\n",
    "    y_mh   = [hash_scores[anchor][o] for o in others]\n",
    "    y_cmh  = [chash_scores[anchor][o] for o in others]\n",
    "    y_cmhp = [chash_scores_pi[anchor][o] for o in others]\n",
    "\n",
    "    ax.bar(x + offsets[0], y_plain, width=bar_width, label='Plain')\n",
    "    ax.bar(x + offsets[1], y_mh,   width=bar_width, label='MinHash')\n",
    "    ax.bar(x + offsets[2], y_cmh,  width=bar_width, label='CMinHash')\n",
    "    ax.bar(x + offsets[3], y_cmhp, width=bar_width, label='CMinHash-pi-pi')\n",
    "\n",
    "    ax.set_title(anchor)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(others, rotation=90)\n",
    "    ax.set_ylim(0, 1)\n",
    "    if idx % cols == 0:\n",
    "        ax.set_ylabel('Similarity')\n",
    "\n",
    "\n",
    "# One legend for all subplots (same as before)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=4)\n",
    "\n",
    "fig.suptitle('Per-File Pairwise Similarities (Plain vs MinHash vs CMinHash vs Fourth)', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "qlhrG4OHb3tG"
   },
   "cell_type": "markdown",
   "source": [
    "Task: For each table, find the top-5 most similar columns across all tables using Jaccard similarity of 8-shingles computed from each column’s flattened string. Return a tidy DataFrame with: source_file, source_col, match_file, match_col, jaccard, ranked by jaccard descending per source column."
   ]
  },
  {
   "metadata": {
    "id": "MI_En_Kqb3tG"
   },
   "cell_type": "markdown",
   "source": [
    "Use already computed minhash_files and cminhash_files to build tidy DataFrames of pairwise Jaccard similarities (files × files) and a long-form table. Do not recompute signatures."
   ]
  },
  {
   "metadata": {
    "id": "y44uX_Tpb3tG"
   },
   "cell_type": "markdown",
   "source": [
    "Code for LSH"
   ]
  },
  {
   "metadata": {
    "id": "-LP2e1CGb3tH"
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datasketch.hashfunc import sha1_hash32\n",
    "\n",
    "@dataclass\n",
    "class LSH:\n",
    "    threshold: float = None\n",
    "    b: int = 4\n",
    "    r: int = 64\n",
    "    num_perm: int = 256\n",
    "    seed: int = 1\n",
    "    hashfunc = sha1_hash32\n",
    "    metadata = []\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.init_hashvalues()\n",
    "\n",
    "    def init_hashvalues(self):\n",
    "        self.hashvalues = np.empty(shape = (0, self.b * self.r), dtype=np.uint64)\n",
    "\n",
    "    def add(self, minhash_list: List[ColStruct]):\n",
    "        self.threshold = (1/self.b)**(1/self.r)\n",
    "        print(f\"approximate threshhold: {self.threshold}\")\n",
    "        minhash_digests = np.array([minhash.value.digest() for minhash in minhash_list], dtype=np.uint64)\n",
    "        minhash_metadata = [(minhash.index, minhash.col_name) for minhash in minhash_list]\n",
    "        self.metadata.extend(minhash_metadata)\n",
    "\n",
    "        if (self.b * self.r != len(minhash_digests[0])):\n",
    "            raise Exception(\"Invalid shape\")\n",
    "\n",
    "        self.hashvalues = np.append(self.hashvalues, minhash_digests, axis=0)\n",
    "\n",
    "    def query(self, col: ColStruct) -> List[MinHash]:\n",
    "        minhash_digests = np.array([col.value.digest()], dtype=np.uint64)\n",
    "        if (self.b * self.r != len(minhash_digests[0])):\n",
    "            raise Exception(\"Invalid shape\")\n",
    "\n",
    "        # VERY IMPORTANT WE SPLIT BY r\n",
    "        hashvalues_split = np.split(self.hashvalues, self.b, axis=1)\n",
    "        col_split = np.split(minhash_digests, self.b, axis=1)\n",
    "\n",
    "        candidates = set()\n",
    "        for i in range(self.b):\n",
    "            w = np.where(np.all(hashvalues_split[i] == col_split[i], axis=1))\n",
    "            if w is None or w is []:\n",
    "                continue\n",
    "            carr = w[0]\n",
    "            candidates.add(tuple([self.metadata[j] for j in carr]))\n",
    "        return candidates\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "35lJXyhXb3tH",
    "outputId": "01ec4d4e-1284-4d6e-b9b9-250e58be8e7a"
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "lsh = LSH()\n",
    "\n",
    "cmh_ci_exp = pd.DataFrame(cminhash_cols.values())\n",
    "cmh_ci_exp = np.array(cmh_ci_exp).reshape(-1)\n",
    "cmh_ci_exp = [c for c in cmh_ci_exp if c is not None]\n",
    "\n",
    "lsh.add(cmh_ci_exp)\n",
    "\n",
    "outs = {}\n",
    "\n",
    "# For each table, print out all columns that are at least 97% similar\n",
    "for i in range(len(cmh_ci_exp)):\n",
    "    qr = lsh.query(cmh_ci_exp[i])\n",
    "    if len(qr) > 1:\n",
    "        print(cmh_ci_exp[i])\n",
    "        outs[str(cmh_ci_exp[i])] = str(qr)\n",
    "        print(qr)\n",
    "        print()\n",
    "\n",
    "json.dump(outs, open(\"./outputs/CMinHash_LSH_similar_rows_copy.json\", \"w\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3: Cleaning Data - Ocean"
   ],
   "metadata": {
    "id": "gs-0VuWKfA3B"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By looking at the CSV files manually, we observe the following:\n",
    "\n",
    "### Table 0\n",
    "**Separator:** Underscore  \n",
    "**Issues Found:** Multilingual column headers with translations separated by newlines\n",
    "**Cleaning Applied:**\n",
    "- `has_multilingual`: Extracts only English portion of column names by splitting on newline characters\n",
    "\n",
    "---\n",
    "\n",
    "### Table 1\n",
    "**Separator:** Underscore  \n",
    "**Issues Found:** Similar multilingual headers as Table 0, emoji presence in data  \n",
    "**Cleaning Applied:**\n",
    "- `has_multilingual`: Standardizes to English headers\n",
    "- `has_emojis`: Strips emojis from both headers and data values\n",
    "\n",
    "---\n",
    "\n",
    "### Table 2\n",
    "**Separator:** Comma\n",
    "**Issues Found:** Multilingual and emojis\n",
    "**Cleaning Applied:**\n",
    "- `has_multilingual`: Ensures English-only headers\n",
    "- `has_emojis`: Removes emojis from text fields\n",
    "\n",
    "---\n",
    "\n",
    "### Table 3\n",
    "**Separator:** Underscore\n",
    "**Issues Found:** BOM character corruption, excessive quotation marks  \n",
    "**Cleaning Applied:**\n",
    "- `has_bom`: Removes UTF-8 BOM characters that corrupt first column name\n",
    "- `has_quotes`: Strips excessive quotes from values and headers\n",
    "\n",
    "---\n",
    "\n",
    "### Table 4\n",
    "**Separator:** Comma\n",
    "**Issues Found:** BOM character issues, emoji contamination  \n",
    "**Cleaning Applied:**\n",
    "- `has_bom`: Fixes first column corruption\n",
    "- `has_emojis`: Cleans emoji characters\n",
    "\n",
    "---\n",
    "\n",
    "### Table 5\n",
    "**Separator:** Comma\n",
    "**Issues Found:** BOM character, malformed first column name  \n",
    "**Cleaning Applied:**\n",
    "- `has_bom`: Removes BOM character\n",
    "- `fix_first_col='REF_DATE'`: Explicitly corrects the first column name\n",
    "\n",
    "---\n",
    "\n",
    "### Table 6\n",
    "**Separator:** Comma\n",
    "**Issues Found:** Missing headers entirely, there are tables that seem to be related but cannot find any with corresponding headers that we could use.  \n",
    "**Cleaning Applied:**\n",
    "- `no_headers`: Loads without headers (assigns numeric column names)\n",
    "\n",
    "---\n",
    "\n",
    "### Table 7\n",
    "**Separator:** Comma   \n",
    "**Issues Found:** Scrambled/corrupted headers but data intact  \n",
    "**Cleaning Applied:** None, don't know how to unscramble headers besides brute forcing each header\n",
    "\n",
    "---\n",
    "\n",
    "### Table 8\n",
    "**Separator:** Underscore\n",
    "**Issues Found:** Duplicate header names repeated within single cells, emojis  \n",
    "**Cleaning Applied:**\n",
    "- `has_duplicate_headers`: Detects repeated words in headers (e.g., \"Name Name Name\" → \"Name\")\n",
    "- `has_emojis`: Removes emoji characters\n",
    "\n",
    "---\n",
    "\n",
    "### Table 9\n",
    "**Separator:** Underscore\n",
    "**Issues Found:** Excessive quotes, emoji contamination  \n",
    "**Cleaning Applied:**\n",
    "- `has_quotes`: Removes quote artifacts\n",
    "- `has_emojis`: Strips emojis\n",
    "\n",
    "---\n",
    "\n",
    "### Table 10\n",
    "**Separator:** Comma\n",
    "**Issues Found:** No headers, same structure as 12 and 11\n",
    "**Cleaning Applied:**\n",
    "- `use_headers_from='table_12.csv'`: Borrows headers from Table 12\n",
    "\n",
    "---\n",
    "\n",
    "### Table 11\n",
    "**Separator:** Comma\n",
    "**Issues Found:** No headers, but structure matches Table 12  \n",
    "**Cleaning Applied:**\n",
    "- `no_headers`: Loads without headers initially\n",
    "- `has_emojis`: Removes emojis from data\n",
    "- `use_headers_from='table_12.csv'`: Borrows headers from Table 12\n",
    "\n",
    "---\n",
    "\n",
    "### Table 12\n",
    "**Separator:** Underscore\n",
    "**Issues Found:** Non-standard header format requiring custom parsing, emojis  \n",
    "**Cleaning Applied:**\n",
    "- `special_header`: Uses custom extraction logic for malformed header line\n",
    "- `has_emojis`: Cleans emoji characters\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Table 13\n",
    "**Separator:** Comma  \n",
    "**Issues Found:** None\n",
    "\n",
    "---\n",
    "\n",
    "### Table 14\n",
    "**Separator:** Tab (\\t)  \n",
    "**Issues Found:** Uses tab separator instead of standard CSV  \n",
    "**Cleaning Applied:** None, just separator specification\n",
    "\n",
    "---\n",
    "\n",
    "### Table 15\n",
    "**Separator:** Underscore  \n",
    "**Issues Found:** Quote artifacts, emoji contamination  \n",
    "**Cleaning Applied:**\n",
    "- `has_quotes`: Removes excessive quoting\n",
    "- `has_emojis`: Strips emoji characters\n",
    "\n",
    "---\n",
    "\n",
    "### Table 16\n",
    "**Separator:** Underscore  \n",
    "**Issues Found:** Similar to Table 15 - quotes and emojis  \n",
    "**Cleaning Applied:**\n",
    "- `has_quotes`: Quote removal\n",
    "- `has_emojis`: Emoji stripping\n",
    "\n",
    "---\n",
    "\n",
    "### Table 17\n",
    "**Separator:** Underscore   \n",
    "**Issues Found:** Duplicate headers, emoji presence  \n",
    "**Cleaning Applied:**\n",
    "- `has_duplicate_headers`: Fixes repeated header names\n",
    "- `has_emojis`: Removes emojis\n",
    "\n",
    "---\n",
    "\n",
    "### Table 18\n",
    "**Separator:** Underscore\n",
    "**Issues Found:** Has data but malformed headers, structure matches Table 17  \n",
    "**Cleaning Applied:**\n",
    "- `skip_header`: Skips the corrupted header row\n",
    "- `has_emojis`: Cleans emojis\n",
    "- `use_headers_from='table_17.csv'`: Borrows headers from Table 17\n",
    "\n",
    "---\n",
    "\n",
    "### Processing order\n",
    "\n",
    "The order for cleaning tables is important because some tables are almost the same, and in case there are missing headers, we can inherit them from another similar table that does have the headers.\n",
    "\n",
    "#### 1. Independent Tables First (Tables 0-9, 12-17)\n",
    "- These tables have their own headers (even if corrupted)\n",
    "- Must be processed first to establish clean header sets\n",
    "- Table 12 specifically must be processed before Table 11\n",
    "- Table 17 must be processed before Table 18\n",
    "\n",
    "#### 2. Dependent Tables Second (Tables 11, 18)\n",
    "- **Table 11** depends on **Table 12**: Borrows the cleaned headers after Table 12's special header extraction\n",
    "- **Table 18** depends on **Table 17**: Uses Table 17's deduplicated headers after skipping its own corrupted header row\n",
    "\n",
    "### Dependency Rationale\n",
    "\n",
    "**Table 12 → Table 11 Dependency**\n",
    "- Table 11 has no headers but identical structure to Table 12\n",
    "- Likely these are split parts of the same dataset or parallel data collections\n",
    "- Table 12's custom header parsing must complete first\n",
    "\n",
    "**Table 17 → Table 18 Dependency**\n",
    "- Table 18's headers are scrambled\n",
    "- Column count and data types match Table 17 exactly\n",
    "- Indication these are related, possibly temporal splits or data segments\n",
    "\n",
    "---\n",
    "\n",
    "## Common data issues in lake33\n",
    "\n",
    "- **BOM characters** (Tables 3, 4, 5)\n",
    "- **Emojis** (11 tables)\n",
    "- **Multilingual content** (Tables 0, 1, 2)\n",
    "- **Missing headers** (Tables 6, 11)\n",
    "- **Duplicate headers** (Tables 8, 17)\n",
    "- **Special formats** (Table 12)\n",
    "- Several use underscore while comma is the standard\n",
    "- One uses tab (Table 14)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "T1jkfeVqfKNt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class datacleaner:\n",
    "    \"\"\"Cleaning code for all lake33 tables\"\"\"\n",
    "    def __init__(self, base_dir: str = './lake33', output_dir: str = './cleaned'):\n",
    "        self.base_dir = base_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.cleaned_datasets = {}\n",
    "\n",
    "        # Compact table configurations using sets for boolean flags\n",
    "        self.configs = {\n",
    "            'table_0.csv': ('_', {'has_multilingual', 'has_emojis'}),\n",
    "            'table_1.csv': ('_', {'has_multilingual', 'has_emojis'}),\n",
    "            'table_2.csv': (',', {'has_multilingual', 'has_emojis'}),\n",
    "            'table_3.csv': ('_', {'has_bom', 'has_quotes'}),\n",
    "            'table_4.csv': (',', {'has_bom', 'has_emojis'}),\n",
    "            'table_5.csv': (',', {'has_bom'}, 'REF_DATE'),\n",
    "            'table_6.csv': (',', {'no_headers'}),\n",
    "            'table_7.csv': (',', set()),\n",
    "            'table_8.csv': ('_', {'has_duplicate_headers', 'has_emojis'}),\n",
    "            'table_9.csv': ('_', {'has_quotes', 'has_emojis'}),\n",
    "            'table_11.csv': (',', {'no_headers', 'has_emojis'}, None, 'table_12.csv'),\n",
    "            'table_12.csv': ('_', {'special_header', 'has_emojis'}),\n",
    "            'table_13.csv': (',', set()),\n",
    "            'table_14.csv': ('\\t', set()),\n",
    "            'table_15.csv': ('_', {'has_quotes', 'has_emojis'}),\n",
    "            'table_16.csv': ('_', {'has_quotes', 'has_emojis'}),\n",
    "            'table_17.csv': ('_', {'has_duplicate_headers', 'has_emojis'}),\n",
    "            'table_18.csv': ('_', {'skip_header', 'has_emojis'}, None, 'table_17.csv'),\n",
    "            'table_10.csv': (',', set(), None, 'table_12.csv'),}\n",
    "\n",
    "    def clean_text(self, text, rm_emoji: bool = False) -> str:\n",
    "        #Clean text values: remove emojis, quotes, and normalize whitespace\n",
    "        if pd.isna(text) or isinstance(text, (int, float)):\n",
    "            return text\n",
    "\n",
    "        text = str(text).strip('[]')\n",
    "\n",
    "        if rm_emoji:\n",
    "            text = ''.join(c for c in text if ord(c) < 128)\n",
    "            text = re.sub(r'[\\[\\]\\'\"]', '', text)\n",
    "\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        parts = text.split()\n",
    "\n",
    "        if len(parts) > 1 and len(set(parts)) == 1:\n",
    "            return parts[0]\n",
    "        return 'MSNG' if not text and 'MSNG' in str(text) else text or str(text)\n",
    "\n",
    "    def clean_col(self, col: str, flags: set) -> str:\n",
    "        #Clean column name based on flags\n",
    "        col = str(col)\n",
    "\n",
    "        if 'has_bom' in flags:\n",
    "            col = col.replace('\\ufeff', '').replace('ï»¿', '')\n",
    "            col = ''.join(c for c in col if ord(c) >= 32 and ord(c) != 8203)\n",
    "\n",
    "        if 'has_quotes' in flags or 'has_emojis' in flags:\n",
    "            col = re.sub(r'[\\[\\]\"\\']', '', col)\n",
    "\n",
    "        if 'has_multilingual' in flags:\n",
    "            for d in ['\\\\\\\\n', '\\\\n', '\\n']:\n",
    "                if d in col:\n",
    "                    col = col.split(d)[0]\n",
    "                    break\n",
    "\n",
    "        if 'has_emojis' in flags:\n",
    "            col = ''.join(c for c in col if ord(c) < 128)\n",
    "\n",
    "        col = re.sub(r'\\s+', ' ', col).strip()\n",
    "\n",
    "        if 'has_duplicate_headers' in flags:\n",
    "            parts = col.split()\n",
    "            if len(parts) >= 2 and all(parts[0] == p for p in parts[1:]):\n",
    "                col = parts[0]\n",
    "        return col or 'unnamed_column'\n",
    "\n",
    "    def get_t12_headers(self, path: str) -> List[str]:\n",
    "        \"\"\"Special header extraction for table 12\"\"\"\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            line = f.readline().strip()\n",
    "        return [p.replace('\"', '').replace('_', ' ').strip()\n",
    "                for p in line.split('_\"') if p.replace('\"', '').replace('_', ' ').strip()]\n",
    "\n",
    "    def load_table(self, name: str) -> Optional[pd.DataFrame]:\n",
    "        path = os.path.join(self.base_dir, name)\n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "\n",
    "        cfg = self.configs.get(name, (',', set()))\n",
    "        sep = cfg[0]\n",
    "        flags = cfg[1] if len(cfg) > 1 else set()\n",
    "        try:\n",
    "            kw = {'low_memory': False, 'on_bad_lines': 'skip'}\n",
    "\n",
    "            if 'no_headers' in flags:\n",
    "                return pd.read_csv(path, header=None, **kw)\n",
    "            elif 'special_header' in flags and name == 'table_12.csv':\n",
    "                headers = self.get_t12_headers(path)\n",
    "                df = pd.read_csv(path, sep=sep, skiprows=1, header=None, **kw)\n",
    "                if len(df.columns) == len(headers):\n",
    "                    df.columns = headers\n",
    "                return df\n",
    "            elif 'skip_header' in flags:\n",
    "                return pd.read_csv(path, sep=sep, skiprows=1, header=None, **kw)\n",
    "            else:\n",
    "                return pd.read_csv(path, sep=sep, **kw)\n",
    "        except:\n",
    "            try:\n",
    "                return pd.read_csv(path, **kw)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def process(self, name: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Process a single table with all necessary cleaning\"\"\"\n",
    "        print(f\"Processing {name}...\", end=' ')\n",
    "        df = self.load_table(name)\n",
    "        if df is None:\n",
    "            print(\"Failed\")\n",
    "            return None\n",
    "        cfg = self.configs.get(name, (',', set()))\n",
    "        flags = cfg[1] if len(cfg) > 1 else set()\n",
    "\n",
    "        #handle headers from other tables\n",
    "        if len(cfg) > 3 and cfg[3]:\n",
    "            src = cfg[3]\n",
    "            if src in self.cleaned_datasets:\n",
    "                src_cols = self.cleaned_datasets[src].columns\n",
    "                if len(df.columns) == len(src_cols):\n",
    "                    df.columns = src_cols\n",
    "        else:\n",
    "            df.columns = [self.clean_col(c, flags) for c in df.columns]\n",
    "\n",
    "        #fix first column if specified\n",
    "        if len(cfg) > 2 and cfg[2]:\n",
    "            cols = df.columns.tolist()\n",
    "            cols[0] = cfg[2]\n",
    "            df.columns = cols\n",
    "\n",
    "        if 'has_emojis' in flags or 'has_quotes' in flags:\n",
    "            for i, col in enumerate(df.columns):\n",
    "                if df.iloc[:, i].dtype == 'object':\n",
    "                    df.iloc[:, i] = df.iloc[:, i].apply(\n",
    "                        lambda x: self.clean_text(x, 'has_emojis' in flags))\n",
    "\n",
    "        df = df.drop_duplicates()\n",
    "        print(f\"{df.shape[0]} rows, {df.shape[1]} cols\")\n",
    "        return df\n",
    "\n",
    "    def clean_all(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Main method to clean all tables\"\"\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        tables = [f for f in os.listdir(self.base_dir)\n",
    "                 if f.endswith('.csv')]\n",
    "\n",
    "        #process independent tables first, then dependent ones\n",
    "        indep = [t for t in tables if len(self.configs.get(t, ())) <= 3 or not self.configs.get(t)[3]]\n",
    "        dep = [t for t in tables if len(self.configs.get(t, ())) > 3 and self.configs.get(t)[3]]\n",
    "\n",
    "        print(f\"Cleaning {len(tables)} tables\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for table in indep + dep:\n",
    "            df = self.process(table)\n",
    "            if df is not None:\n",
    "                self.cleaned_datasets[table] = df\n",
    "                df.to_csv(os.path.join(self.output_dir, f\"cleaned_{table}\"), index=False)\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Completed: {len(self.cleaned_datasets)} tables cleaned\\n\")\n",
    "        return self.cleaned_datasets\n",
    "\n",
    "    def report(self) -> dict:\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_tables_processed': len(self.cleaned_datasets),\n",
    "                'output_directory': self.output_dir\n",
    "            },\n",
    "            'table_details': {}}\n",
    "\n",
    "        for name, df in self.cleaned_datasets.items():\n",
    "            cfg = self.configs.get(name, (',', set()))\n",
    "            flags = list(cfg[1]) if len(cfg) > 1 else []\n",
    "\n",
    "            details = {\n",
    "                'shape': df.shape,\n",
    "                'separator': cfg[0],\n",
    "                'cleaning_applied': flags.copy(),\n",
    "                'columns': df.columns.tolist()[:10]}\n",
    "\n",
    "            if len(cfg) > 2 and cfg[2]:\n",
    "                details['cleaning_applied'].append(f'fix_first_col={cfg[2]}')\n",
    "            if len(cfg) > 3 and cfg[3]:\n",
    "                details['cleaning_applied'].append(f'use_headers_from={cfg[3]}')\n",
    "\n",
    "            report['table_details'][name] = details\n",
    "\n",
    "        with open(os.path.join(self.output_dir, 'cleaning_report.json'), 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "        return report\n",
    "\n",
    "def clean_lake33_tables(base_dir='./lake33', output_dir='./cleaned'):\n",
    "    cleaner = datacleaner(base_dir, output_dir)\n",
    "    cleaned_data = cleaner.clean_all()\n",
    "    report = cleaner.report()\n",
    "    return cleaned_data, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_datasets, cleaning_report = clean_lake33_tables()"
   ],
   "metadata": {
    "id": "4zW-nvKufNO5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 4: running discovery methods on clean data\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "iIkVlTX7b3tD"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o4liJ4rBb3tD",
    "outputId": "f446f666-9767-49fe-feba-656eb054b6fa"
   },
   "source": [
    "# Reading data from the cleaned tables. Since tables 6,7 and 10 have been removed, we no longer attempt to read them.\n",
    "tables_clean = []\n",
    "for i in range(0, 19):\n",
    "    if i in [6,7,10]:\n",
    "        continue\n",
    "    ct = read_csv(f'lake33c/cleaned_table_{i}.csv')\n",
    "    ct.name = f'cleaned_table_{i}.csv'\n",
    "    tables_clean.append(ct)\n",
    "\n",
    "result_sc_clean = set_containment_method(tables_clean)\n",
    "result_sc_clean.to_csv('outputs/set_containment_results_clean.csv', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "result_cn_clean = column_names(tables_clean)\n",
    "result_cn_clean.to_csv('outputs/column_name_results_clean.csv', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "result_josie_clean = josie(tables_clean)\n",
    "result_josie_clean.to_csv('outputs/josie_results_clean.csv', encoding='utf-8', index=False, header=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 4: Clean Data Discovery MinHash - Berken Tekin"
  },
  {
   "metadata": {
    "id": "CJk7R8YCb3tE"
   },
   "cell_type": "code",
   "source": "csv_path = \"./lake33c\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2Yqop9fKb3tE",
    "outputId": "452b1f61-a7a6-4201-ec5b-55243db60934"
   },
   "cell_type": "code",
   "source": [
    "df_dict = {}\n",
    "df_col_dict = defaultdict(list)\n",
    "\n",
    "error_count_file = 0\n",
    "for root, dirs, files in os.walk(csv_path):\n",
    "    for csv_file in files:\n",
    "        if os.path.splitext(csv_file)[1] == \".csv\":\n",
    "            file_path = os.path.join(root, csv_file)\n",
    "            try:\n",
    "                # Naive read_csv() as mentionmed ion the report\n",
    "                df = pd.read_csv(file_path, sep=None, engine='python', on_bad_lines='skip', header=0)\n",
    "                df = df.dropna().astype(object)\n",
    "                # print(df)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            # Serialize whole file (flattened)\n",
    "            df_str = df.to_numpy().flatten()\n",
    "            df_str = \" \".join(str(x) for x in df_str).replace(\"\\n\", \" \")\n",
    "            df_str = re.sub(r\"\\s+\", \" \", df_str).strip()\n",
    "            df_dict[csv_file] = df_str\n",
    "\n",
    "            # Serialize each column for this file\n",
    "            for col_idx, col in enumerate(df.columns):\n",
    "                col_arr = df[col].to_numpy().flatten()\n",
    "                col_str = \" \".join(str(x) for x in col_arr).replace(\"\\n\", \" \")\n",
    "                col_str = re.sub(r\"\\s+\", \" \", col_str).strip()\n",
    "                df_col_dict[csv_file].append(ColStruct(index=col_idx, col_name=str(col), value=col_str))\n",
    "\n",
    "print(error_count_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Recalculate MinHash for clean datasets"
  },
  {
   "metadata": {
    "id": "UTUqHwjpb3tF",
    "outputId": "9f39aebd-2c24-48f7-c632-64ed4495db5c"
   },
   "cell_type": "code",
   "source": [
    "\n",
    "text_shinglesets = {}\n",
    "col_shinglesets = defaultdict(set)\n",
    "\n",
    "for fname, text in tqdm(df_dict.items()):\n",
    "    cmh = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "    cmhp = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "    mh = MinHash(num_perm=256)\n",
    "\n",
    "    ks = kshingle_manual(text)\n",
    "    text_shinglesets[fname] = tuple(ks)\n",
    "    max_buf=128\n",
    "    for i in range(0, len(ks), max_buf):\n",
    "        bufsize = min(len(ks)-i, max_buf)\n",
    "        cmh.update_batch_cminhash_sigma_pi([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "        cmhp.update_batch_cminhash_pi_pi([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "    for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "        cmh_col = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "        cmhp_col = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "\n",
    "        ks = kshingle_manual(col_value)\n",
    "        cmh_col.update_batch_cminhash_sigma_pi([e.encode(\"utf8\") for e in ks])\n",
    "        cmhp_col.update_batch_cminhash_pi_pi([e.encode(\"utf8\") for e in ks])\n",
    "        cminhash_cols[fname].append(ColStruct((fname, col_idx), col_name ,cmh_col))\n",
    "        cminhash_cols_pi[fname].append(ColStruct((fname, col_idx), col_name ,cmhp_col))\n",
    "    cminhash_files[fname] = cmh\n",
    "    cminhash_files_pi[fname] = cmhp\n",
    "\n",
    "\n",
    "    for i in range(0, len(ks), max_buf):\n",
    "        bufsize = min(len(ks)-i, max_buf)\n",
    "        mh.update_batch([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "    for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "        mh_col = MinHash(num_perm=256)\n",
    "        ks = kshingle_manual(col_value)\n",
    "        mh_col.update_batch([e.encode(\"utf8\") for e in ks])\n",
    "        minhash_cols[fname].append(ColStruct((fname, col_idx), col_name ,mh_col))\n",
    "\n",
    "        col_shinglesets[fname].add(ColStruct((fname, col_idx), col_name, tuple(ks)))\n",
    "\n",
    "\n",
    "    minhash_files[fname] = mh\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8-pYsKAIb3tF"
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def jaccard_manual(set1, set2):\n",
    "    set1 = set(set1)\n",
    "    set2 = set(set2)\n",
    "\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    if len(intersection) == 0:\n",
    "        return 0.0\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "hash_scores = {}\n",
    "chash_scores = {}\n",
    "chash_scores_pi = {}\n",
    "plain_scores = {}\n",
    "for fname, content in minhash_files.items():\n",
    "    hash_scores[fname] = {f: minhash_files[f].jaccard(content) for f in minhash_files.keys()}\n",
    "for fname, content in cminhash_files.items():\n",
    "    chash_scores[fname] = {f: cminhash_files[f].jaccard(content) for f in cminhash_files.keys()}\n",
    "\n",
    "for fname, content in cminhash_files_pi.items():\n",
    "    chash_scores_pi[fname] = {f: cminhash_files_pi[f].jaccard(content) for f in cminhash_files_pi.keys()}\n",
    "for fname, content in text_shinglesets.items():\n",
    "    plain_scores[fname] = {f: jaccard_manual(text_shinglesets[f], content) for f in text_shinglesets.keys()}\n",
    "\n",
    "df = pd.DataFrame(hash_scores)\n",
    "cdf = pd.DataFrame(chash_scores)\n",
    "pdf = pd.DataFrame(plain_scores)\n",
    "cpdf = pd.DataFrame(chash_scores_pi)\n",
    "\n",
    "df.to_csv(\"outputs/hash_scores_clean.csv\", index=False)\n",
    "cdf.to_csv(\"outputs/chash_scores_clean.csv\", index=False)\n",
    "pdf.to_csv(\"outputs/plain_scores_clean.csv\", index=False)\n",
    "cpdf.to_csv(\"outputs/chash_scores_pi_clean.csv\", index=False)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "rX5NdCNgb3tG",
    "outputId": "d61975c5-fe89-4232-a66c-e91e1c82e675"
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = sorted(pdf.index)\n",
    "n = len(files)\n",
    "cols = 3\n",
    "rows = (n + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, max(6, rows * 3)), sharey=True)\n",
    "axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "bar_width = 0.2\n",
    "offsets = (-1.5*bar_width, -0.5*bar_width, 0.5*bar_width, 1.5*bar_width)\n",
    "\n",
    "for idx, anchor in enumerate(files):\n",
    "    ax = axes[idx]\n",
    "    others = [f for f in files if f != anchor]\n",
    "    x = np.arange(len(others))\n",
    "\n",
    "    y_plain = [plain_scores[anchor][o] for o in others]\n",
    "    y_mh   = [hash_scores[anchor][o] for o in others]\n",
    "    y_cmh  = [chash_scores[anchor][o] for o in others]\n",
    "    y_cmhp = [chash_scores_pi[anchor][o] for o in others]\n",
    "\n",
    "    ax.bar(x + offsets[0], y_plain, width=bar_width, label='Plain')\n",
    "    ax.bar(x + offsets[1], y_mh,   width=bar_width, label='MinHash')\n",
    "    ax.bar(x + offsets[2], y_cmh,  width=bar_width, label='CMinHash')\n",
    "    ax.bar(x + offsets[3], y_cmhp, width=bar_width, label='CMinHash-pi-pi')\n",
    "\n",
    "    ax.set_title(anchor)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(others, rotation=90)\n",
    "    ax.set_ylim(0, 1)\n",
    "    if idx % cols == 0:\n",
    "        ax.set_ylabel('Similarity')\n",
    "\n",
    "\n",
    "# One legend for all subplots (same as before)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=4)\n",
    "\n",
    "fig.suptitle('Per-File Pairwise Similarities (Plain vs MinHash vs CMinHash vs Fourth)', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Task: For each table, find the top-5 most similar columns across all tables using Jaccard similarity of 8-shingles computed from each column’s flattened string. Return a tidy DataFrame with: source_file, source_col, match_file, match_col, jaccard, ranked by jaccard descending per source column."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use already computed minhash_files and cminhash_files to build tidy DataFrames of pairwise Jaccard similarities (files × files) and a long-form table. Do not recompute signatures."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code for LSH"
  },
  {
   "metadata": {
    "id": "-LP2e1CGb3tH"
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datasketch.hashfunc import sha1_hash32\n",
    "\n",
    "@dataclass\n",
    "class LSH:\n",
    "    threshold: float = None\n",
    "    b: int = 4\n",
    "    r: int = 64\n",
    "    num_perm: int = 256\n",
    "    seed: int = 1\n",
    "    hashfunc = sha1_hash32\n",
    "    metadata = []\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.init_hashvalues()\n",
    "\n",
    "    def init_hashvalues(self):\n",
    "        self.hashvalues = np.empty(shape = (0, self.b * self.r), dtype=np.uint64)\n",
    "\n",
    "    def add(self, minhash_list: List[ColStruct]):\n",
    "        self.threshold = (1/self.b)**(1/self.r)\n",
    "        print(f\"approximate threshhold: {self.threshold}\")\n",
    "        minhash_digests = np.array([minhash.value.digest() for minhash in minhash_list], dtype=np.uint64)\n",
    "        minhash_metadata = [(minhash.index, minhash.col_name) for minhash in minhash_list]\n",
    "        self.metadata.extend(minhash_metadata)\n",
    "\n",
    "        if (self.b * self.r != len(minhash_digests[0])):\n",
    "            raise Exception(\"Invalid shape\")\n",
    "\n",
    "        self.hashvalues = np.append(self.hashvalues, minhash_digests, axis=0)\n",
    "\n",
    "    def query(self, col: ColStruct) -> List[MinHash]:\n",
    "        minhash_digests = np.array([col.value.digest()], dtype=np.uint64)\n",
    "        if (self.b * self.r != len(minhash_digests[0])):\n",
    "            raise Exception(\"Invalid shape\")\n",
    "\n",
    "        # VERY IMPORTANT WE SPLIT BY r\n",
    "        hashvalues_split = np.split(self.hashvalues, self.b, axis=1)\n",
    "        col_split = np.split(minhash_digests, self.b, axis=1)\n",
    "\n",
    "        candidates = set()\n",
    "        for i in range(self.b):\n",
    "            w = np.where(np.all(hashvalues_split[i] == col_split[i], axis=1))\n",
    "            if w is None or w is []:\n",
    "                continue\n",
    "            carr = w[0]\n",
    "            candidates.add(tuple([self.metadata[j] for j in carr]))\n",
    "        return candidates\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "35lJXyhXb3tH",
    "outputId": "01ec4d4e-1284-4d6e-b9b9-250e58be8e7a"
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "lsh = LSH()\n",
    "\n",
    "cmh_ci_exp = pd.DataFrame(cminhash_cols.values())\n",
    "cmh_ci_exp = np.array(cmh_ci_exp).reshape(-1)\n",
    "cmh_ci_exp = [c for c in cmh_ci_exp if c is not None]\n",
    "\n",
    "lsh.add(cmh_ci_exp)\n",
    "\n",
    "outs = {}\n",
    "\n",
    "# For each table, print out all columns that are at least 97% similar\n",
    "for i in range(len(cmh_ci_exp)):\n",
    "    qr = lsh.query(cmh_ci_exp[i])\n",
    "    if len(qr) > 1:\n",
    "        print(cmh_ci_exp[i])\n",
    "        outs[str(cmh_ci_exp[i])] = str(qr)\n",
    "        print(qr)\n",
    "        print()\n",
    "\n",
    "json.dump(outs, open(\"./outputs/CMinHash_LSH_similar_rows_copy_clean.json\", \"w\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "-1BWet5eb3s2",
    "gs-0VuWKfA3B",
    "iIkVlTX7b3tD"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
