{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A very important aspect of supervised and semi-supervised machine learning is the quality of the labels produced by human labelers. Unfortunately, humans are not perfect and in some cases may even maliciously label things incorrectly. In this assignment, you will evaluate the impact of incorrect labels on a number of different classifiers.\n",
    "\n",
    "We have provided a number of code snippets you can use during this assignment. Feel free to modify them or replace them.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "The dataset you will be using is the [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). This dataset was created by Ronny Kohavi and Barry Becker and was used to predict whether a person's income is more/less than 50k USD based on census data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Start by loading and preprocessing the data. Remove NaN values, convert strings to categorical variables and encode the target variable (the string <=50K, >50K in column index 14)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba.core.types import uint64\n",
    "from pandas.core.interchange.dataframe_protocol import Column, DataFrame\n",
    "from pandas.core.util.hashing import hash_pandas_object\n",
    "from setuptools.config.pyprojecttoml import load_file\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#import kshingle as ks\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple, NamedTuple\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from typing import Callable, Generator, Iterable, List, Optional, Tuple\n",
    "import numpy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasketch.hashfunc import sha1_hash32\n",
    "from tqdm import tqdm\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# This can be used to load the dataset\n",
    "data = pd.read_csv(\"adult.csv\", header=0, na_values='?')\n",
    "data = data.dropna()\n",
    "\n",
    "data = data.convert_dtypes()\n",
    "\n",
    "numericals = data.select_dtypes(include=[np.number]).columns\n",
    "categoricals = data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "data[categoricals] = data[categoricals].astype('category')\n",
    "\n",
    "encoder = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categoricals)], remainder='passthrough')\n",
    "\n",
    "#dt = encoder.fit_transform(data)\n",
    "\n",
    "#for c in data.columns:\n",
    "#    if data[c].dtype == 'object':\n",
    "#        data[c] = pd.Categorical(data[c])\n",
    "\n",
    "#print(dt)\n",
    "data.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(data.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data classification\n",
    "Choose at least 4 different classifiers and evaluate their performance in predicting the target variable. \n",
    "\n",
    "#### Preprocessing\n",
    "Think about how you are going to encode the categorical variables, normalization, whether you want to use all of the features, feature dimensionality reduction, etc. Justify your choices \n",
    "\n",
    "A good method to apply preprocessing steps is using a Pipeline. Read more about this [here](https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/) and [here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf). \n",
    "\n",
    "<!-- #### Data visualization\n",
    "Calculate the correlation between different features, including the target variable. Visualize the correlations in a heatmap. A good example of how to do this can be found [here](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec). \n",
    "\n",
    "Select a features you think will be an important predictor of the target variable and one which is not important. Explain your answers. -->\n",
    "\n",
    "#### Evaluation\n",
    "Use a validation technique from the previous lecture to evaluate the performance of the model. Explain and justify which metrics you used to compare the different models. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define your preprocessing steps here\n",
    "steps = []\n",
    "\n",
    "# Combine steps into a ColumnTransformer\n",
    "ct = ColumnTransformer(steps)\n",
    "\n",
    "# show the correlation between different features including target variable\n",
    "def visualize(data, ct):\n",
    "    pass\n",
    "\n",
    "# Apply your model to feature array X and labels y\n",
    "def apply_model(model, X, y):    \n",
    "    # Wrap the model and steps into a Pipeline\n",
    "    pipeline = Pipeline(steps=[('t', ct), ('m', model)])\n",
    "    \n",
    "    # Evaluate the model and store results\n",
    "    return evaluate_model(X, y, pipeline)\n",
    "\n",
    "# Apply your validation techniques and calculate metrics\n",
    "def evaluate_model(X, y, pipeline):\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label perturbation\n",
    "To evaluate the impact of faulty labels in a dataset, we will introduce some errors in the labels of our data.\n",
    "\n",
    "\n",
    "#### Preparation\n",
    "Start by creating a method which alters a dataset by selecting a percentage of rows randomly and swaps labels from a 0->1 and 1->0. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"Given a label vector, create a new copy where a random fraction of the labels have been flipped.\"\"\"\n",
    "def pertubate(y: np.ndarray, fraction: float) -> np.ndarray:\n",
    "    copy = data.copy()\n",
    "    # Flip fraction*len(data) of the labels in copy\n",
    "    return copy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "Create a number of new datasets with perturbed labels, for fractions ranging from `0` to `0.5` in increments of `0.1`.\n",
    "\n",
    "Perform the same experiment you did before, which compared the performances of different models except with the new datasets. Repeat your experiment at least 5x for each model and perturbation level and calculate the mean and variance of the scores. Visualize the change in score for different perturbation levels for all of the models in a single plot. \n",
    "\n",
    "State your observations. Is there a change in the performance of the models? Are there some classifiers which are impacted more/less than other classifiers and why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Code"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations + explanations: max. 400 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "1)  Discuss how you could reduce the impact of wrongly labeled data or correct wrong labels. <br />\n",
    "    max. 400 words\n",
    "\n",
    "\n",
    "\n",
    "    Authors: Youri Arkesteijn, Tim van der Horst and Kevin Chong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "From part 1, you will have gone through the entire machine learning workflow which are they following steps:\n",
    "\n",
    "1) Data Loading\n",
    "2) Data Pre-processing\n",
    "3) Machine Learning Model Training\n",
    "4) Machine Learning Model Testing\n",
    "\n",
    "You can see these tasks are very sequential, and need to be done in a serial fashion. \n",
    "\n",
    "As a small perturbation in the actions performed in each of the steps may have a detrimental knock-on effect in the task that comes afterwards.\n",
    "\n",
    "In the final part of Part 1, you will have experienced the effects of performing perturbations to the machine learning model training aspect and the reaction of the machine learning model testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Data Discovery\n",
    "\n",
    "You will be given a set of datasets and you are tasked to perform data discovery on the data sets.\n",
    "\n",
    "<b>The datasets are provided in the group lockers on brightspace. Let me know if you are having trouble accessing the datasets</b>\n",
    "\n",
    "The process is to have the goal of finding datasets that are related to each other, finding relationships between the datasets.\n",
    "\n",
    "The relationships that we are primarily working with are Join and Union relationships.\n",
    "\n",
    "So please implement two methods for allowing us to find those pesky Join and Union relationships.\n",
    "\n",
    "Try to do this with the datasets as is and no processing.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Discovery algorithm:\n",
    "1. Scan each database with read_csv.\n",
    "2. Flatten each database, convert it to one string\n",
    "3. Shingle with k=6\n",
    "4. The MinHash and CMinHash values are calculated for each set of shingles\n",
    "5. LSH returns pairs of columns with similarities above a threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@numba.jit(nopython=True)\n",
    "def kshingle_manual(s, k=6):\n",
    "   sh = set()\n",
    "   for i in range(len(s) - k + 1):\n",
    "       sh.add(s[i:i + k])\n",
    "   return list(sh)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We run the experiment twice for dirty and clean datasets."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "csv_path = \"./lake33\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "ColStruct = namedtuple(\"ColStruct\", [\"index\", \"col_name\", \"value\"])\n",
    "\n",
    "df_dict = {}\n",
    "df_col_dict = defaultdict(list)\n",
    "dirty = False\n",
    "\n",
    "error_count_file = 0\n",
    "for root, dirs, files in os.walk(csv_path):\n",
    "    for csv_file in files:\n",
    "        if os.path.splitext(csv_file)[1] == \".csv\":\n",
    "            file_path = os.path.join(root, csv_file)\n",
    "            try:\n",
    "                # Naive read_csv() as mentionmed ion the report\n",
    "                df = pd.read_csv(file_path, sep=None, engine='python', on_bad_lines='skip', header=0)\n",
    "                df = df.dropna().astype(object)\n",
    "                # print(df)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            # Serialize whole file (flattened)\n",
    "            df_str = df.to_numpy().flatten()\n",
    "            if not dirty:\n",
    "                df_str = \" \".join(str(x) for x in df_str).replace(\"\\n\", \" \")\n",
    "                df_str = re.sub(r\"\\s+\", \" \", df_str).strip()\n",
    "            df_dict[csv_file] = df_str\n",
    "\n",
    "            # Serialize each column for this file\n",
    "            for col_idx, col in enumerate(df.columns):\n",
    "                col_arr = df[col].to_numpy().flatten()\n",
    "                if not dirty:\n",
    "                    col_str = \" \".join(str(x) for x in col_arr).replace(\"\\n\", \" \")\n",
    "                    col_str = re.sub(r\"\\s+\", \" \", col_str).strip()\n",
    "                else:\n",
    "                    col_str = str(col_arr)\n",
    "                df_col_dict[csv_file].append(ColStruct(index=col_idx, col_name=str(col), value=col_str))\n",
    "\n",
    "print(error_count_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "22s to read files"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "from typing import Callable, Generator, Iterable, List, Optional, Tuple\n",
    "import numpy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasketch.hashfunc import sha1_hash32\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numba as nb\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The datasketch.MinHash module is a good starting point to build the CMinHash algorithm."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For MinHash, at the beginning I did not really understand what the algorithm does.\n",
    "Working on extending a module instead of starting from scratch helped me understand the algorithm better. I also learned about batch processing from datasketch.MinHash."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "minhash_files = {}\n",
    "cminhash_files = {}\n",
    "cminhash_files_pi = {}\n",
    "cminhash_cols = defaultdict(list)\n",
    "cminhash_cols_pi = defaultdict(list)\n",
    "minhash_cols = defaultdict(list)\n",
    "lookup = {}\n",
    "mh_file_ind = 0\n",
    "hashvalue_byte_size = 4\n",
    "# The size of a hash value in number of bytes\n",
    "\n",
    "\n",
    "class CMinHashTest(MinHash):\n",
    "\n",
    "    _large_prime = np.uint64((1 << 31) - 1) # A large prime\n",
    "    _hash_range = 1 << 31\n",
    "\n",
    "    def __init__(self, num_perm: int = 128, seed: int = 1234, hashfunc: Callable = sha1_hash32,\n",
    "                 hashobj: Optional[object] = None, hashvalues: Optional[Iterable] = None,\n",
    "                 permutations: Optional[Tuple[Iterable, Iterable]] = None, c_minhash_rotations: Optional[int] = None) -> None:\n",
    "        super().__init__(num_perm, seed, hashfunc, hashobj, hashvalues, permutations)\n",
    "        if c_minhash_rotations is not None: # Hard code\n",
    "            self.c_minhash_rotations = c_minhash_rotations\n",
    "        self.pi_permutations = self._init_pi_permutations()\n",
    "        self.sigma_permutations = self._init_sigma_permutations()\n",
    "        self.shift_right_precomputed = np.array([k for k in range(1, self.c_minhash_rotations + 1)], dtype=np.uint64).T\n",
    "\n",
    "\n",
    "    def _init_sigma_permutations(self):\n",
    "        gen = np.random.RandomState(self.seed + 1) # So we get different values for sigma permutation\n",
    "        return np.array([\n",
    "            gen.randint(1, self._large_prime, dtype=np.uint64),\n",
    "            gen.randint(0, self._large_prime, dtype=np.uint64),\n",
    "        ], dtype=np.uint64).T\n",
    "\n",
    "    def _init_pi_permutations(self):\n",
    "        # Create parameters for a random bijective permutation function\n",
    "        # that maps a 32-bit hash value to another 32-bit hash value.\n",
    "        # http://en.wikipedia.org/wiki/Universal_hashing\n",
    "        gen = np.random.RandomState(self.seed)\n",
    "        return np.array(\n",
    "            [\n",
    "                (\n",
    "                    gen.randint(1, self._large_prime, dtype=np.uint64),\n",
    "                    gen.randint(0, self._large_prime, dtype=np.uint64),\n",
    "                )\n",
    "            ],\n",
    "            dtype=np.uint64,\n",
    "        ).T\n",
    "\n",
    "\n",
    "    def sf(self, hv):\n",
    "        a, b = self.sigma_permutations\n",
    "        return (hv * a + b) % self._large_prime\n",
    "\n",
    "    def pf(self, hv) -> np.ndarray:\n",
    "        a, b = self.pi_permutations\n",
    "        return (hv * a + b) % self._large_prime\n",
    "\n",
    "    # Another way of shifting the permutations.\n",
    "    # The k offset is introduced at the end of the operation, instead of the beginning.\n",
    "    # Here, \"pia\" holds (a * k + b % p) for all k.\n",
    "    # https://github.com/beowolx/rensa/blob/95d80780f52f3105df4433132b279acd8c2762a0/src/cminhash.rs\n",
    "    def pf2(self, hv, pia) -> np.ndarray:\n",
    "        a, b = self.pi_permutations\n",
    "        return (hv * a + pia) % self._large_prime\n",
    "\n",
    "    def pf3(self, hv, off) -> np.ndarray:\n",
    "        a, b = self.pi_permutations\n",
    "        return (hv * (a - off) + b) % self._large_prime\n",
    "\n",
    "\n",
    "\n",
    "    def update_batch_cminhash_pi_pi(self, b: Iterable) -> None:\n",
    "        \"\"\"Update this MinHash with new values.\n",
    "        The values will be hashed using the hash function specified by\n",
    "        the `hashfunc` argument in the constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.c_minhash_rotations is None:\n",
    "            raise ValueError(\"You need to specify c_minhash_rotations\")\n",
    "\n",
    "        hv = np.array([self.hashfunc(_b) for _b in b], dtype=np.uint64, ndmin=2).T\n",
    "        phv = self.pf(hv)\n",
    "        phv = self.pf3(phv, self.shift_right_precomputed)\n",
    "\n",
    "        self.hashvalues = np.vstack([phv, self.hashvalues]).min(axis=0)\n",
    "\n",
    "\n",
    "    def update_batch_cminhash_sigma_pi(self, b: Iterable) -> None:\n",
    "        \"\"\"Update this MinHash with new values.\n",
    "        The values will be hashed using the hash function specified by\n",
    "        the `hashfunc` argument in the constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.c_minhash_rotations is None:\n",
    "            raise ValueError(\"You need to specify c_minhash_rotations\")\n",
    "\n",
    "        hv = np.array([self.hashfunc(_b) for _b in b], dtype=np.uint64, ndmin=2).T\n",
    "        phv = self.sf(hv)\n",
    "        phv = self.pf3(phv, self.shift_right_precomputed)\n",
    "\n",
    "        self.hashvalues = np.vstack([phv, self.hashvalues]).min(axis=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "text_shinglesets = {}\n",
    "col_shinglesets = defaultdict(set)\n",
    "\n",
    "for fname, text in tqdm(df_dict.items()):\n",
    "    cmh = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "    cmhp = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "    mh = MinHash(num_perm=256)\n",
    "\n",
    "    ks = kshingle_manual(text)\n",
    "    text_shinglesets[fname] = tuple(ks)\n",
    "    max_buf=128\n",
    "    for i in range(0, len(ks), max_buf):\n",
    "        bufsize = min(len(ks)-i, max_buf)\n",
    "        cmh.update_batch_cminhash_sigma_pi([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "        cmhp.update_batch_cminhash_pi_pi([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "    for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "        cmh_col = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "        cmhp_col = CMinHashTest(num_perm=256, c_minhash_rotations=256)\n",
    "\n",
    "        ks = kshingle_manual(col_value)\n",
    "        cmh_col.update_batch_cminhash_sigma_pi([e.encode(\"utf8\") for e in ks])\n",
    "        cmhp_col.update_batch_cminhash_pi_pi([e.encode(\"utf8\") for e in ks])\n",
    "        cminhash_cols[fname].append(ColStruct((fname, col_idx), col_name ,cmh_col))\n",
    "        cminhash_cols_pi[fname].append(ColStruct((fname, col_idx), col_name ,cmhp_col))\n",
    "    cminhash_files[fname] = cmh\n",
    "    cminhash_files_pi[fname] = cmhp\n",
    "\n",
    "\n",
    "    for i in range(0, len(ks), max_buf):\n",
    "        bufsize = min(len(ks)-i, max_buf)\n",
    "        mh.update_batch([e.encode(\"utf8\") for e in ks[i:i+bufsize]])\n",
    "    for (col_idx, col_name, col_value) in df_col_dict[fname]:\n",
    "        mh_col = MinHash(num_perm=256)\n",
    "        ks = kshingle_manual(col_value)\n",
    "        mh_col.update_batch([e.encode(\"utf8\") for e in ks])\n",
    "        minhash_cols[fname].append(ColStruct((fname, col_idx), col_name ,mh_col))\n",
    "\n",
    "        col_shinglesets[fname].add(ColStruct((fname, col_idx), col_name, tuple(ks)))\n",
    "\n",
    "\n",
    "    minhash_files[fname] = mh\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def jaccard_manual(set1, set2):\n",
    "    set1 = set(set1)\n",
    "    set2 = set(set2)\n",
    "\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    if len(intersection) == 0:\n",
    "        return 0.0\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "hash_scores = {}\n",
    "chash_scores = {}\n",
    "chash_scores_pi = {}\n",
    "plain_scores = {}\n",
    "for fname, content in minhash_files.items():\n",
    "    hash_scores[fname] = {f: minhash_files[f].jaccard(content) for f in minhash_files.keys()}\n",
    "for fname, content in cminhash_files.items():\n",
    "    chash_scores[fname] = {f: cminhash_files[f].jaccard(content) for f in cminhash_files.keys()}\n",
    "\n",
    "for fname, content in cminhash_files_pi.items():\n",
    "    chash_scores_pi[fname] = {f: cminhash_files_pi[f].jaccard(content) for f in cminhash_files_pi.keys()}\n",
    "for fname, content in text_shinglesets.items():\n",
    "    plain_scores[fname] = {f: jaccard_manual(text_shinglesets[f], content) for f in text_shinglesets.keys()}\n",
    "\n",
    "df = pd.DataFrame(hash_scores)\n",
    "cdf = pd.DataFrame(chash_scores)\n",
    "pdf = pd.DataFrame(plain_scores)\n",
    "cpdf = pd.DataFrame(chash_scores_pi)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = sorted(pdf.index)\n",
    "n = len(files)\n",
    "cols = 3\n",
    "rows = (n + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, max(6, rows * 3)), sharey=True)\n",
    "axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "bar_width = 0.2\n",
    "offsets = (-1.5*bar_width, -0.5*bar_width, 0.5*bar_width, 1.5*bar_width)\n",
    "\n",
    "for idx, anchor in enumerate(files):\n",
    "    ax = axes[idx]\n",
    "    others = [f for f in files if f != anchor]\n",
    "    x = np.arange(len(others))\n",
    "\n",
    "    y_plain = [plain_scores[anchor][o] for o in others]\n",
    "    y_mh   = [hash_scores[anchor][o] for o in others]\n",
    "    y_cmh  = [chash_scores[anchor][o] for o in others]\n",
    "    y_cmhp = [chash_scores_pi[anchor][o] for o in others]\n",
    "\n",
    "    ax.bar(x + offsets[0], y_plain, width=bar_width, label='Plain')\n",
    "    ax.bar(x + offsets[1], y_mh,   width=bar_width, label='MinHash')\n",
    "    ax.bar(x + offsets[2], y_cmh,  width=bar_width, label='CMinHash')\n",
    "    ax.bar(x + offsets[3], y_cmhp, width=bar_width, label='CMinHash-pi-pi')\n",
    "\n",
    "    ax.set_title(anchor)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(others, rotation=90)\n",
    "    ax.set_ylim(0, 1)\n",
    "    if idx % cols == 0:\n",
    "        ax.set_ylabel('Similarity')\n",
    "\n",
    "\n",
    "# One legend for all subplots (same as before)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=4)\n",
    "\n",
    "fig.suptitle('Per-File Pairwise Similarities (Plain vs MinHash vs CMinHash vs Fourth)', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Task: For each table, find the top-5 most similar columns across all tables using Jaccard similarity of 8-shingles computed from each column’s flattened string. Return a tidy DataFrame with: source_file, source_col, match_file, match_col, jaccard, ranked by jaccard descending per source column."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use already computed minhash_files and cminhash_files to build tidy DataFrames of pairwise Jaccard similarities (files × files) and a long-form table. Do not recompute signatures."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "files_sorted = sorted(minhash_files.keys())\n",
    "\n",
    "mh_pairs = []\n",
    "for i, a in enumerate(files_sorted):\n",
    "    for b in files_sorted[i:]:\n",
    "        jmh = float(minhash_files[a].jaccard(minhash_files[b]))\n",
    "        mh_pairs.append((a, b, jmh))\n",
    "        if a != b:\n",
    "            mh_pairs.append((b, a, jmh))\n",
    "\n",
    "mh_jaccard_df = pd.DataFrame(mh_pairs, columns=[\"file_a\", \"file_b\", \"jaccard_minhash\"]).sort_values(\n",
    "    [\"file_a\", \"file_b\"])\n",
    "mh_jaccard_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "files_sorted = sorted(cminhash_files.keys())\n",
    "\n",
    "cmh_pairs = []\n",
    "for i, a in enumerate(files_sorted):\n",
    "    for b in files_sorted[i:]:\n",
    "        jcmh = float(cminhash_files[a].jaccard(cminhash_files[b]))\n",
    "        cmh_pairs.append((a, b, jcmh))\n",
    "        if a != b:\n",
    "            cmh_pairs.append((b, a, jcmh))\n",
    "\n",
    "cmh_jaccard_df = pd.DataFrame(cmh_pairs, columns=[\"file_a\", \"file_b\", \"jaccard_cminhash\"]).sort_values(\n",
    "    [\"file_a\", \"file_b\"])\n",
    "cmh_jaccard_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code for LSH"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datasketch.hashfunc import sha1_hash32\n",
    "\n",
    "@dataclass\n",
    "class LSH:\n",
    "    threshold: float = None\n",
    "    b: int = 4\n",
    "    r: int = 64\n",
    "    num_perm: int = 256\n",
    "    seed: int = 1\n",
    "    hashfunc = sha1_hash32\n",
    "    metadata = []\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.init_hashvalues()\n",
    "\n",
    "    def init_hashvalues(self):\n",
    "        self.hashvalues = np.empty(shape = (0, self.b * self.r), dtype=np.uint64)\n",
    "\n",
    "    def add(self, minhash_list: List[ColStruct]):\n",
    "        self.threshold = (1/self.b)**(1/self.r)\n",
    "        print(f\"approximate threshhold: {self.threshold}\")\n",
    "        minhash_digests = np.array([minhash.value.digest() for minhash in minhash_list], dtype=np.uint64)\n",
    "        minhash_metadata = [(minhash.index, minhash.col_name) for minhash in minhash_list]\n",
    "        self.metadata.extend(minhash_metadata)\n",
    "\n",
    "        if (self.b * self.r != len(minhash_digests[0])):\n",
    "            raise Exception(\"Invalid shape\")\n",
    "\n",
    "        self.hashvalues = np.append(self.hashvalues, minhash_digests, axis=0)\n",
    "\n",
    "    def query(self, col: ColStruct) -> List[MinHash]:\n",
    "        minhash_digests = np.array([col.value.digest()], dtype=np.uint64)\n",
    "        if (self.b * self.r != len(minhash_digests[0])):\n",
    "            raise Exception(\"Invalid shape\")\n",
    "\n",
    "        # VERY IMPORTANT WE SPLIT BY B\n",
    "        hashvalues_split = np.split(self.hashvalues, self.b, axis=1)\n",
    "        col_split = np.split(minhash_digests, self.b, axis=1)\n",
    "\n",
    "        candidates = set()\n",
    "        for i in range(self.b):\n",
    "            w = np.where(np.all(hashvalues_split[i] == col_split[i], axis=1))\n",
    "            if w is None or w is []:\n",
    "                continue\n",
    "            carr = w[0]\n",
    "            candidates.add(tuple([self.metadata[j] for j in carr]))\n",
    "        return candidates\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lsh = LSH()\n",
    "\n",
    "cmh_ci_exp = pd.DataFrame(cminhash_cols.values())\n",
    "cmh_ci_exp = np.array(cmh_ci_exp).reshape(-1)\n",
    "cmh_ci_exp = [c for c in cmh_ci_exp if c is not None]\n",
    "\n",
    "lsh.add(cmh_ci_exp)\n",
    "\n",
    "# FOr each table, print out all columns that are at least 93% similar\n",
    "for i in range(len(cmh_ci_exp)):\n",
    "    qr = lsh.query(cmh_ci_exp[i])\n",
    "    if len(qr) > 1:\n",
    "        print(cmh_ci_exp[i])\n",
    "        print(qr)\n",
    "        print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You would have noticed that the data has some issues in them.\n",
    "So perhaps those issues have been troublesome to deal with.\n",
    "\n",
    "Please try to do some cleaning on the data.\n",
    "\n",
    "After performing cleaning see if the results of the data discovery has changed?\n",
    "\n",
    "Please try to explain this in your report, and try to match up the error with the observation."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Cleaning data, scrubbing, washing, mopping\n",
    "\n",
    "def cleaningData(data):\n",
    "    \"\"\"Function should be able to clean the data\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of cleaned datasets\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Discussions\n",
    "\n",
    "1)  Different aspects of the data can effect the data discovery process. Write a short report on your findings. Such as which data quality issues had the largest effect on data discovery. Which data quality problem was repairable and how you choose to do the repair.\n",
    "\n",
    "<!-- For the set of considerations that you have outlined for the choice of data discovery methods, choose one and identify under this new constraint, how would you identify and resolve this problem? -->\n",
    "\n",
    "Max 400 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would have noticed that the data has some issues in them.\n",
    "So perhaps those issues have been troublesome to deal with.\n",
    "\n",
    "Please try to do some cleaning on the data.\n",
    "\n",
    "After performing cleaning see if the results of the data discovery has changed?\n",
    "\n",
    "Please try to explain this in your report, and try to match up the error with the observation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Cleaning data, scrubbing, washing, mopping\n",
    "\n",
    "def cleaningData(data):\n",
    "    \"\"\"Function should be able to clean the data\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of cleaned datasets\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "1)  Different aspects of the data can effect the data discovery process. Write a short report on your findings. Such as which data quality issues had the largest effect on data discovery. Which data quality problem was repairable and how you choose to do the repair.\n",
    "\n",
    "<!-- For the set of considerations that you have outlined for the choice of data discovery methods, choose one and identify under this new constraint, how would you identify and resolve this problem? -->\n",
    "\n",
    "Max 400 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
