{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3XaxKH6Kftw"
   },
   "source": [
    "# Introduction\n",
    "A very important aspect of supervised and semi-supervised machine learning is the quality of the labels produced by human labelers. Unfortunately, humans are not perfect and in some cases may even maliciously label things incorrectly. In this assignment, you will evaluate the impact of incorrect labels on a number of different classifiers.\n",
    "\n",
    "We have provided a number of code snippets you can use during this assignment. Feel free to modify them or replace them.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "The dataset you will be using is the [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). This dataset was created by Ronny Kohavi and Barry Becker and was used to predict whether a person's income is more/less than 50k USD based on census data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PQMNKE8Kftx"
   },
   "source": [
    "### Data preprocessing\n",
    "Start by loading and preprocessing the data. Remove NaN values, convert strings to categorical variables and encode the target variable (the string <=50K, >50K in column index 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWQ1LajpKftx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPV_mO_OKfty"
   },
   "outputs": [],
   "source": [
    "# This can be used to load the dataset\n",
    "data = pd.read_csv(\"adult.csv\", na_values='?')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW1yPBViKfty"
   },
   "source": [
    "##### Check the percentage of missing values in the columns. Rule of thumb: If the percentage of missing values is above 60%, remove the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dsqYhcHKfty"
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    nan_count = data[column].isna().sum()/len(data)*100\n",
    "    print(\"Percentage of NaN in column \" + column + \" is \" + str(nan_count) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiXVdGrBKftz"
   },
   "source": [
    "Remove all rows that contain nan values, since the columns with missing values can't be imputed (no numerical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQ6OhS3jKftz"
   },
   "outputs": [],
   "source": [
    "data_before = len(data)\n",
    "data = data.dropna()\n",
    "data_after = len(data)\n",
    "print(\"Removed \" + str(data_before-data_after) + \" rows from the \" + str(data_before) + \" rows\")\n",
    "\n",
    "data = data.drop(columns=[\"education\", \"fnlwgt\"])\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0yLs2bGKftz"
   },
   "source": [
    "Turn string columns into categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoGd9OOkKftz"
   },
   "outputs": [],
   "source": [
    "string_columns = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "for col in string_columns:\n",
    "    data[col] = pd.Categorical(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7mubsGcKftz"
   },
   "outputs": [],
   "source": [
    "print(data['salary'].unique())\n",
    "data['salary'] = data['salary'].str.strip().str.replace(r\"\\.$\", \"\", regex=True)\n",
    "data['salary'] = pd.Categorical(data['salary'],categories=[\"<=50K\", \">50K\"],ordered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnEYOemKft0"
   },
   "source": [
    "### Data classification\n",
    "Choose at least 4 different classifiers and evaluate their performance in predicting the target variable.\n",
    "\n",
    "#### Preprocessing\n",
    "Think about how you are going to encode the categorical variables, normalization, whether you want to use all of the features, feature dimensionality reduction, etc. Justify your choices\n",
    "\n",
    "A good method to apply preprocessing steps is using a Pipeline. Read more about this [here](https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/) and [here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf).\n",
    "\n",
    "<!-- #### Data visualization\n",
    "Calculate the correlation between different features, including the target variable. Visualize the correlations in a heatmap. A good example of how to do this can be found [here](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec).\n",
    "\n",
    "Select a features you think will be an important predictor of the target variable and one which is not important. Explain your answers. -->\n",
    "\n",
    "#### Evaluation\n",
    "Use a validation technique from the previous lecture to evaluate the performance of the model. Explain and justify which metrics you used to compare the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2oKVSSbKft0"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# determine categorical and numerical features\n",
    "numerical_ix = ['age','education-num','capital-gain','capital-loss','hours-per-week']\n",
    "categorical_ix = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "\n",
    "# Define your preprocessing steps here\n",
    "steps = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_ix), ('num', MinMaxScaler() , numerical_ix)]\n",
    "\n",
    "\n",
    "# Apply your model to feature array X and labels y\n",
    "def apply_model(model, ct, X, y, feature_reduction = False):\n",
    "    pipeline_pca = Pipeline(steps=[('t', ct), ('pca', PCA(n_components=60)), ('m', model)])\n",
    "    pipeline_nopca = Pipeline(steps=[('t', ct) , ('m', model)])\n",
    "\n",
    "    return evaluate_model(X, y, pipeline_nopca, pipeline_pca, feature_reduction)\n",
    "\n",
    "# Apply your validation techniques and calculate metrics\n",
    "def evaluate_model(X, y, pipeline_nopca, pipeline_pca, feature_reduction=False):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    scores_nopca = cross_val_score(pipeline_nopca, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    scores_pca = cross_val_score(pipeline_pca, X, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "    #print(\"Mean accuracy without PCA:\", scores_nopca.mean())\n",
    "    #print(\"Mean accuracy with PCA   :\", scores_pca.mean())\n",
    "\n",
    "    y_pred = cross_val_predict(pipeline_pca, X, y, cv=cv)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    return scores_nopca.mean(), scores_pca.mean()\n",
    "\n",
    "### DEPRECATED METHOD AS PCA IS USED INSTEAD OF FEATURE IMPORTANCE\n",
    "\n",
    "# def show_feature_importance(pipeline, X, y, top_n = 12):\n",
    "#     model = pipeline.named_steps[\"m\"]\n",
    "#     feature_names = pipeline.named_steps[\"t\"].get_feature_names_out()\n",
    "\n",
    "#     importance = None\n",
    "\n",
    "#     if hasattr(model, \"feature_importances_\"):\n",
    "#         importance = model.feature_importances_ * 100\n",
    "#     elif hasattr(model, \"coef_\"):\n",
    "#         importance = abs(model.coef_[0])\n",
    "#     else:\n",
    "#         print(\"Using permutation importance (slower)...\")\n",
    "#         r = permutation_importance(pipeline, X, y, n_repeats=10, random_state=42)\n",
    "#         importance = r.importances_mean\n",
    "\n",
    "#     df = pd.DataFrame({\"feature\": feature_names, \"importance\": importance})\n",
    "\n",
    "#     df[\"base_feature\"] = (\n",
    "#         df[\"feature\"]\n",
    "#         .str.replace(r\"^cat__|^num__\", \"\", regex=True)   # remove prefixes\n",
    "#         .str.split(\"_\").str[0]                          # keep original feature name\n",
    "#     )\n",
    "\n",
    "#     agg_df = df.groupby(\"base_feature\")[\"importance\"].sum().sort_values(ascending=False)\n",
    "\n",
    "#     print(\"\\nTop Features (aggregated):\")\n",
    "#     print(agg_df.head(top_n))\n",
    "\n",
    "#     red = agg_df.head(top_n).index.to_list()\n",
    "\n",
    "#     # --- Plot aggregated importance ---\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.barplot(x=agg_df.head(top_n), y=agg_df.head(top_n).index, palette=\"viridis\")\n",
    "#     plt.title(f\"Aggregated Feature Importance ({type(model).__name__})\")\n",
    "#     plt.xlabel(\"Importance\")\n",
    "#     plt.ylabel(\"Feature\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return red\n",
    "\n",
    "# DEPRECATED METHOD AS PCA IS USED INSTEAD OF FEATURE SELECTION\n",
    "\n",
    "# def compare_and_plot(models, ct, X, y):\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     results = {}\n",
    "\n",
    "#     for name, model in models.items():\n",
    "#         # Full features\n",
    "#         pipeline_full = Pipeline(steps=[('t', ct), ('m', model)])\n",
    "#         scores_full = cross_val_score(pipeline_full, X, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "#         # Reduced features\n",
    "#         reduced_features = apply_model(model, ct, X, y, feature_reduction=True)\n",
    "#         X_reduced = X[reduced_features]\n",
    "\n",
    "#         cat_selected = [c for c in reduced_features if c in categorical_ix]\n",
    "#         num_selected = [c for c in reduced_features if c in numerical_ix]\n",
    "\n",
    "#         ct_reduced = ColumnTransformer([\n",
    "#             ('cat', OneHotEncoder(handle_unknown='ignore'), cat_selected),\n",
    "#             ('num', MinMaxScaler(), num_selected)\n",
    "#         ])\n",
    "#         pipeline_reduced = Pipeline(steps=[('t', ct_reduced), ('m', model)])\n",
    "#         scores_reduced = cross_val_score(pipeline_reduced, X_reduced, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "#         # Store mean difference\n",
    "#         results[name] = scores_reduced.mean() - scores_full.mean()\n",
    "\n",
    "#     # Plot differences\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     plt.barh(list(results.keys()), list(results.values()), color=\"skyblue\")\n",
    "#     plt.axvline(0, color=\"red\", linestyle=\"--\")\n",
    "#     plt.xlabel(\"Accuracy Difference (Reduced - Full)\")\n",
    "#     plt.title(\"Effect of Feature Reduction on Model Accuracy\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return results\n",
    "\n",
    "ct = ColumnTransformer(steps)\n",
    "\n",
    "models = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=10000),\n",
    "    \"SGD\": SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10000),\n",
    "    \"DecisionTree\": tree.DecisionTreeClassifier(),\n",
    "    \"LinearSVC\": LinearSVC()\n",
    "}\n",
    "\n",
    "results_nopca = []\n",
    "results_pca = []\n",
    "\n",
    "for model in models.values():\n",
    "    y = data['salary']\n",
    "    X = data.drop('salary', axis=1)\n",
    "\n",
    "    npca, ypca = apply_model(model, ct, X, y)\n",
    "    results_nopca.append(npca)\n",
    "    results_pca.append(ypca)\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x - width/2, results_nopca, width, label=\"No PCA\", color=\"steelblue\")\n",
    "plt.bar(x + width/2, results_pca, width, label=\"With PCA\", color=\"orange\")\n",
    "plt.xticks(x, list(models.keys()))\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.title(\"Model Performance: With vs Without PCA\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OjDvFJuKft0"
   },
   "source": [
    "### Label perturbation\n",
    "To evaluate the impact of faulty labels in a dataset, we will introduce some errors in the labels of our data.\n",
    "\n",
    "\n",
    "#### Preparation\n",
    "Start by creating a method which alters a dataset by selecting a percentage of rows randomly and swaps labels from a 0->1 and 1->0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv1-kPq7Kft0"
   },
   "outputs": [],
   "source": [
    "\"\"\"Given a label vector, create a new copy where a random fraction of the labels have been flipped.\"\"\"\n",
    "def pertubate(y: np.ndarray, fraction: float) -> np.ndarray:\n",
    "    copy = y.copy()\n",
    "    n = len(y)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    flip_idx = rng.choice(n, size=int(fraction*n), replace=False)\n",
    "\n",
    "    copy.iloc[flip_idx] = 1 - copy.iloc[flip_idx]\n",
    "\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcWHSVYwKft0"
   },
   "source": [
    "#### Analysis\n",
    "Create a number of new datasets with perturbed labels, for fractions ranging from `0` to `0.5` in increments of `0.1`.\n",
    "\n",
    "Perform the same experiment you did before, which compared the performances of different models except with the new datasets. Repeat your experiment at least 5x for each model and perturbation level and calculate the mean and variance of the scores. Visualize the change in score for different perturbation levels for all of the models in a single plot.\n",
    "\n",
    "State your observations. Is there a change in the performance of the models? Are there some classifiers which are impacted more/less than other classifiers and why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuWE4XuMKft0"
   },
   "outputs": [],
   "source": [
    "og_data = pd.read_csv(\"adult.csv\", na_values='?')\n",
    "data = og_data.copy()\n",
    "\n",
    "for column in data.columns:\n",
    "    nan_count = data[column].isna().sum()/len(data)*100\n",
    "\n",
    "data_before = len(data)\n",
    "data = data.dropna()\n",
    "data_after = len(data)\n",
    "\n",
    "data = data.drop(columns=['education', 'fnlwgt'])\n",
    "string_columns = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "for col in string_columns:\n",
    "    data[col] = pd.Categorical(data[col])\n",
    "\n",
    "data['salary'] = data['salary'].str.strip().str.replace(r\"\\.$\", \"\", regex=True)\n",
    "data['salary'] = data['salary'].replace({\"<=50K\":0, \">50K\":1})\n",
    "\n",
    "og_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0G5vpjckewO"
   },
   "outputs": [],
   "source": [
    "salary = og_data['salary']\n",
    "\n",
    "data_00 = og_data.copy()\n",
    "\n",
    "data_01 = og_data.copy()\n",
    "data_01['salary'] = pertubate(salary, 0.1)\n",
    "\n",
    "data_02 = og_data.copy()\n",
    "data_02['salary'] = pertubate(salary, 0.2)\n",
    "\n",
    "data_03 = og_data.copy()\n",
    "data_03['salary'] = pertubate(salary, 0.3)\n",
    "\n",
    "data_04 = og_data.copy()\n",
    "data_04['salary'] = pertubate(salary, 0.4)\n",
    "\n",
    "data_05 = og_data.copy()\n",
    "data_05['salary'] = pertubate(salary, 0.5)\n",
    "\n",
    "# 4 different models\n",
    "lr = LogisticRegression()\n",
    "sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10000)\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "svc = LinearSVC()\n",
    "\n",
    "data = [(\"data_00\", data_00), (\"data_01\", data_01), (\"data_02\", data_02), (\"data_03\" ,data_03), (\"data_04\", data_04), (\"data_05\", data_05)]\n",
    "models = [lr, sgd, dt, svc]\n",
    "\n",
    "results = {\n",
    "    f\"data_{i:02d}\": {\n",
    "        model: {\"mean\": None, \"variance\": None}\n",
    "        for model in models\n",
    "    }\n",
    "    for i, _ in enumerate(data)\n",
    "}\n",
    "\n",
    "numerical_ix = ['age' ,'education-num','capital-gain','capital-loss','hours-per-week']\n",
    "categorical_ix = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "\n",
    "steps = [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_ix), ('num', MinMaxScaler() , numerical_ix)]\n",
    "\n",
    "ct = ColumnTransformer(steps)\n",
    "\n",
    "for m in models:\n",
    "    for name, df in data:\n",
    "        scores = []\n",
    "\n",
    "        for r in range(0,5):\n",
    "            y = df['salary']\n",
    "            X = df.drop('salary', axis=1)\n",
    "\n",
    "            _, score = apply_model(m, ct, X, y)\n",
    "            scores.append(score)\n",
    "\n",
    "        mean = np.mean(scores)\n",
    "        variance = np.var(scores)\n",
    "\n",
    "        results[name][m][\"mean\"] = mean\n",
    "        results[name][m][\"variance\"] = variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xI9ye9kSkewP"
   },
   "outputs": [],
   "source": [
    "data_names = [\"data_00\", \"data_01\", \"data_02\", \"data_03\", \"data_04\", \"data_05\"]\n",
    "x = range(len(data_names))  # 0..5\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for model in models:\n",
    "    means = [results[dname][model][\"mean\"] for dname in data_names]\n",
    "    variances = [results[dname][model][\"variance\"] for dname in data_names]\n",
    "    std_devs = np.sqrt(variances)\n",
    "\n",
    "    plt.plot(x, means, marker='o', label=model)\n",
    "    plt.fill_between(x,\n",
    "                     np.array(means) - std_devs,\n",
    "                     np.array(means) + std_devs,\n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.xticks(x, data_names)\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.title(\"Model Performance Across Datasets\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10IunJVEkewP"
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for dataset, models_dict in results.items():\n",
    "    for model_name, stats in models_dict.items():\n",
    "        records.append({\n",
    "            \"Dataset\": dataset,\n",
    "            \"Model\": model_name,\n",
    "            \"Mean\": stats[\"mean\"],\n",
    "            \"Variance\": stats[\"variance\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cteyZajKft0"
   },
   "source": [
    "Observations + explanations: max. 400 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYEnN0iEKft0"
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "1)  Discuss how you could reduce the impact of wrongly labeled data or correct wrong labels. <br />\n",
    "    max. 400 words\n",
    "\n",
    "\n",
    "\n",
    "    Authors: Youri Arkesteijn, Tim van der Horst and Kevin Chong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3VOEXokKft0"
   },
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "From part 1, you will have gone through the entire machine learning workflow which are they following steps:\n",
    "\n",
    "1) Data Loading\n",
    "2) Data Pre-processing\n",
    "3) Machine Learning Model Training\n",
    "4) Machine Learning Model Testing\n",
    "\n",
    "You can see these tasks are very sequential, and need to be done in a serial fashion.\n",
    "\n",
    "As a small perturbation in the actions performed in each of the steps may have a detrimental knock-on effect in the task that comes afterwards.\n",
    "\n",
    "In the final part of Part 1, you will have experienced the effects of performing perturbations to the machine learning model training aspect and the reaction of the machine learning model testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Data Discovery\n",
    "\n",
    "You will be given a set of datasets and you are tasked to perform data discovery on the data sets.\n",
    "\n",
    "<b>The datasets are provided in the group lockers on brightspace. Let me know if you are having trouble accessing the datasets</b>\n",
    "\n",
    "The process is to have the goal of finding datasets that are related to each other, finding relationships between the datasets.\n",
    "\n",
    "The relationships that we are primarily working with are Join and Union relationships.\n",
    "\n",
    "So please implement two methods for allowing us to find those pesky Join and Union relationships.\n",
    "\n",
    "Try to do this with the datasets as is and no processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part implements four methods to find similar columns across different tables in a dataset, two for each relation: join and union. The methods are:\n",
    "1. Set Containment method (JOIN): Similarity between column values of different tables. \\\n",
    "2. Column name method (2xUNION): Similarity between column names of different tables. Measured in two different ways: \n",
    "    - Levenshtein distance (Shows how many single-character edits are needed to change one string into another) \n",
    "    - Jaccard similarity on shingles.\n",
    "3. JOSIE method (JOIN): Similarity between column values of different tables, using JOSIE. Creates posting lists and prints a top k list of similar columns (k=3).\n",
    "\n",
    "Thresholds have been set to 0.8, to avoid too many false positives.\n",
    "First, we read the datasets, and store them in a list of dataframes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import tqdm\n",
    "# Load the dataset\n",
    "\n",
    "def read_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        return df\n",
    "    except Exception:\n",
    "        return pd.read_csv(file_path, delimiter='_', low_memory=False)\n",
    "\n",
    "tables = []\n",
    "for i in range(0, 19):\n",
    "    ct = read_csv(f'lake33/table_{i}.csv')\n",
    "    ct.name = f'table_{i}.csv'\n",
    "    tables.append(ct)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-01T14:30:30.838254600Z",
     "start_time": "2025-10-01T14:30:26.746460Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first run the methods on uncleaned data, to identify first results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def set_containment(set_a, set_b):\n",
    "    \"\"\" Returns the containment of setA in setB \"\"\"\n",
    "    if len(set_a) == 0:\n",
    "        return 0\n",
    "    return len(set_a.intersection(set_b)) / len(set_a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-01T14:30:33.923838700Z",
     "start_time": "2025-10-01T14:30:33.915320400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [10:08<00:00, 32.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# Set Containment method:\n",
    "def set_containment_method(dataset):\n",
    "    threshold = 0.8\n",
    "    \n",
    "    rows_list = []\n",
    "    # Iterate through pairs of tables\n",
    "    for i, df1 in enumerate(tqdm.tqdm(dataset)):\n",
    "        for j, df2 in enumerate(dataset):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            # Iterate through pairs of columns of both tables:\n",
    "            for colidx1, col1 in enumerate(df1.columns):\n",
    "                for colidx2, col2 in enumerate(df2.columns):\n",
    "                    vals1 = set(df1[col1].dropna())\n",
    "                    vals2 = set(df2[col2].dropna())\n",
    "                    \n",
    "                    sc1 = set_containment(vals1, vals2)\n",
    "                    if sc1 >= threshold:\n",
    "                        dict = {}\n",
    "                        dict.update({'Dataset 1': df1.name, 'Column 1': str(col1), 'Dataset 2': df2.name, 'Column 2': str(col2), 'Set Containment': sc1, 'Relation': 'JOIN'})\n",
    "                        rows_list.append(dict)\n",
    "                    \n",
    "    result = pd.DataFrame(rows_list, columns=['Dataset 1', 'Dataset 2', 'Relation', 'Column 1', 'Column 2', 'Set Containment'])\n",
    "    return result\n",
    "\n",
    "result_sc_unclean = set_containment_method(tables)\n",
    "result_sc_unclean.to_csv('outputs/set_containment_results_unclean.csv', encoding='utf-8', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-01T14:40:44.143223600Z",
     "start_time": "2025-10-01T14:30:35.847039800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results method 1: (Runtime ~ 10 minutes)\n",
    "\n",
    "We notice the set containment method manages to identify many similarities between tables. For instance, it correctly captures similarities between tables 1 and 2, which appear to have similar data, even when the column titles are mismatched. This is the case for many tables, where the column names are different but the values are similar. \n",
    "\n",
    "However, this method yields a large number of false positives as well. For example, some tables appear to contain some sort of sensor data, which may have a small range of values that can potentially overlap with some other columns that have no connection to sensors, but a wide range of values. This yields a set containment value that exceeds the threshold, but the columns are not actually similar in any meaningful way.\n",
    "\n",
    "One method to mitigate this issue would be to consider the set containment relation in both directions, however that would significantly increase the computation time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [01:15<00:00,  3.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Column name method\n",
    "# Two ideas: Levenshtein distance, or Jaccard similarity on shingles.\n",
    "def levenshtein(s1, s2):\n",
    "    m = len(s1)\n",
    "    n = len(s2)\n",
    "    \n",
    "    prev_row = [i for i in range(n + 1)]\n",
    "    curr_row = [0] * (n + 1)\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        curr_row[0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                curr_row[j] = prev_row[j - 1]\n",
    "            else:\n",
    "                curr_row[j] = min(prev_row[j - 1], prev_row[j], curr_row[j - 1]) + 1\n",
    "        prev_row = curr_row.copy()\n",
    "    return curr_row[n]\n",
    "\n",
    "def jaccard_similarity(s1, s2, k=2):\n",
    "    def get_shingles(s, k):\n",
    "        return {s[i:i+k] for i in range(len(s) - k + 1)}\n",
    "    \n",
    "    shingles1 = get_shingles(s1, k)\n",
    "    shingles2 = get_shingles(s2, k)\n",
    "    \n",
    "    intersection = len(shingles1.intersection(shingles2))\n",
    "    union = len(shingles1.union(shingles2))\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "def column_names(dataset):\n",
    "    threshold = 0.8\n",
    "    rows_list = []\n",
    "    for i, df1 in enumerate(tqdm.tqdm(dataset)):\n",
    "        for j, df2 in enumerate(dataset):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            for colidx1, col1 in enumerate(df1.columns):\n",
    "                for colidx2, col2 in enumerate(df2.columns):\n",
    "                    c1 = col1.lower()\n",
    "                    c2 = col2.lower()\n",
    "                    has_result = False\n",
    "                    \n",
    "                    dict = {'Dataset 1': df1.name, 'Column 1': str(col1), 'Dataset 2': df2.name, 'Column 2': str(col2), 'Relation': 'UNION'}\n",
    "                    \n",
    "                    lev_ratio = (levenshtein(col1.lower(), col2.lower())) / max(len(c1), len(c2))\n",
    "                    if lev_ratio <= (1 - threshold):\n",
    "                        has_result = True\n",
    "                        dict.update({'Levenshtein Ratio': 1 - lev_ratio})\n",
    "                    \n",
    "                    for k in range(2,6):\n",
    "                        sim = jaccard_similarity(c1, c2, k)\n",
    "                        if sim >= threshold:\n",
    "                            has_result = True\n",
    "                            dict.update({f'Jaccard (k={k})': sim})\n",
    "                    if has_result:\n",
    "                        rows_list.append(dict)\n",
    "                  \n",
    "    result = pd.DataFrame(rows_list, columns=['Dataset 1', 'Dataset 2', 'Relation', 'Column 1', 'Column 2', 'Levenshtein Ratio', 'Jaccard (k=2)', 'Jaccard (k=3)', 'Jaccard (k=4)', 'Jaccard (k=5)'])\n",
    "    return result\n",
    "\n",
    "result_cn_unclean = column_names(tables)\n",
    "result_cn_unclean.to_csv('outputs/column_name_results_unclean.csv', encoding='utf-8', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-01T14:41:59.638758600Z",
     "start_time": "2025-10-01T14:40:44.139225200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results method 2 (methods 2.1 and 2.2): (Runtime ~ 75 seconds)\n",
    "\n",
    "The column name method is faster, as we only analyse the column names instead of searching through the whole values. The Levenshtein distance method manages to capture similarities when the column names are very similar. The threshold has been set to 20% difference, which means that the strings can differ by at most 20% of the length of the longer string. This may exclude shorter strings that are similar, but works well for longer strings which may contain spelling mistakes or shortcuts.\n",
    "\n",
    "The Jaccard similarity method on shingles captures similarities when column names have similar substrings. This works well for column names that differ by many characters semantically, but share common words or abbreviations.\n",
    "\n",
    "On the other hand, if the names are completely different, both methods cannot capture any similarity between tables, leading to a large number of false negatives."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  7.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:05<00:00,  3.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# JOSIE method\n",
    "\n",
    "def josie(dataset):\n",
    "    # Create posting lists\n",
    "    postings = {}\n",
    "    for i, df in enumerate(tqdm.tqdm(dataset)):\n",
    "        for j, col in enumerate(df.columns):\n",
    "            vals = set(df[col].dropna())\n",
    "            for entry in vals:\n",
    "                entry = str(entry)\n",
    "                if entry not in postings:\n",
    "                    postings[entry] = set()\n",
    "                postings[entry].add((df.name, j))\n",
    "                \n",
    "    k = 3\n",
    "    rows_list = []\n",
    "    \n",
    "    for i, df1 in enumerate(tqdm.tqdm(dataset)):\n",
    "        for j, col1 in enumerate(df1.columns):\n",
    "            vals1 = set(df1[col1].dropna())\n",
    "            counter = {}\n",
    "            for val in vals1:\n",
    "                val = str(val)\n",
    "                if val in postings:\n",
    "                    for (table_id, col_id) in postings[val]:\n",
    "                        if table_id == df1.name: # Do not match within the same table\n",
    "                            continue\n",
    "                        if (table_id, col_id) not in counter:\n",
    "                            counter[(table_id, col_id)] = 0\n",
    "                        counter[(table_id, col_id)] += 1\n",
    "            sorted_counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "            has_result = False\n",
    "            dict = {'Dataset 1': df1.name, 'Column 1': str(col1)}\n",
    "            for ((table_id, col_id), count) in sorted_counter[:(min(len(counter), k))]:\n",
    "                has_result = True\n",
    "                dict.update({f'Similar Column {len(dict)-1}': f'Table {table_id}, Column {col_id}, Overlap: {count}'})\n",
    "            if has_result:\n",
    "                rows_list.append(dict)\n",
    "                \n",
    "    col_names = ['Dataset 1', 'Column 1']\n",
    "    for i in range(k):\n",
    "        col_names.append(f'Similar Column {i+1}')\n",
    "    \n",
    "    result = pd.DataFrame(rows_list, columns=col_names)\n",
    "    return result\n",
    "\n",
    "result_josie_unclean = josie(tables)\n",
    "result_josie_unclean.to_csv('outputs/josie_results_unclean.csv', encoding='utf-8', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-01T14:42:07.818211500Z",
     "start_time": "2025-10-01T14:41:59.642744400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results method 3: (Runtime ~ 8 seconds)\n",
    "\n",
    "JOSIE does great at creating posting lists quickly, and uses them to quickly find similar columns. It is very fast and also creates a top k list of similar columns, which is useful for considering more potential matches.\n",
    "\n",
    "However, this method only uses overlap as a metric, which is not giving very insightful information on its own. This method works best if combined with other previous approaches, which could give a better idea if two columns are actually similar or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- Rest of assignment follows --"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T20:02:39.244597Z",
     "start_time": "2025-09-17T20:02:39.242544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet kshingle\n",
    "%pip install --quiet datasketch\n",
    "\n",
    "import kshingle as ks\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovery algorithm:\n",
    "1. I scan each database with read_csv.\n",
    "2. I flatten each database, convert it to one string\n",
    "3. I shingle with k=8\n",
    "4. I calculate the Jaccard similarity between all pairs of shingles\n",
    "5. I return the list of relatedness between databases\n",
    "\n",
    "Comments:\n",
    "1. File sizes are manageable enough to directly calculate Jaccard similarity without relying on MinHash\n",
    "2. Tables 10 and 11 are the same\n",
    "3. For k=8, most similar tables are table_13.csv <> table_7.csv | plain_8=0.1207 | plain_containment_8 = 0.2582 | minhash_8=0.1328\n",
    "4. \n",
    "table_6.csv <> table_5.csv | plain_8=0.0958 | plain_containment_8 = 0.2043 | minhash_8=0.0703\n",
    "table_6.csv <> table_2.csv | plain_8=0.0858 | plain_containment_8 = 0.2544 | minhash_8=0.0859\n",
    "Significant Jaccard containment scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T21:05:28.742618Z",
     "start_time": "2025-09-17T21:01:17.476764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_11.csv <> table_13.csv | plain_8=0.0017 | plain_containment_8 = 0.0082 | minhash_8=0.0000\n",
      "table_11.csv <> table_14.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_11.csv <> table_2.csv | plain_8=0.0008 | plain_containment_8 = 0.0037 | minhash_8=0.0078\n",
      "table_11.csv <> table_4.csv | plain_8=0.0002 | plain_containment_8 = 0.0004 | minhash_8=0.0000\n",
      "table_11.csv <> table_5.csv | plain_8=0.0001 | plain_containment_8 = 0.0002 | minhash_8=0.0000\n",
      "table_11.csv <> table_6.csv | plain_8=0.0003 | plain_containment_8 = 0.0006 | minhash_8=0.0000\n",
      "table_11.csv <> table_7.csv | plain_8=0.0006 | plain_containment_8 = 0.0042 | minhash_8=0.0000\n",
      "table_11.csv <> table_8.csv | plain_8=0.0000 | plain_containment_8 = 0.0003 | minhash_8=0.0000\n",
      "table_11.csv <> table_9.csv | plain_8=0.0442 | plain_containment_8 = 0.0871 | minhash_8=0.0469\n",
      "table_13.csv <> table_14.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_13.csv <> table_2.csv | plain_8=0.0033 | plain_containment_8 = 0.0068 | minhash_8=0.0000\n",
      "table_13.csv <> table_4.csv | plain_8=0.0031 | plain_containment_8 = 0.0163 | minhash_8=0.0000\n",
      "table_13.csv <> table_5.csv | plain_8=0.0003 | plain_containment_8 = 0.0021 | minhash_8=0.0000\n",
      "table_13.csv <> table_6.csv | plain_8=0.0007 | plain_containment_8 = 0.0027 | minhash_8=0.0000\n",
      "table_13.csv <> table_7.csv | plain_8=0.0548 | plain_containment_8 = 0.1294 | minhash_8=0.0625\n",
      "table_13.csv <> table_8.csv | plain_8=0.0184 | plain_containment_8 = 0.0406 | minhash_8=0.0156\n",
      "table_13.csv <> table_9.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_14.csv <> table_2.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_14.csv <> table_4.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_14.csv <> table_5.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_14.csv <> table_6.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_14.csv <> table_7.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_14.csv <> table_8.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_14.csv <> table_9.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_2.csv <> table_4.csv | plain_8=0.0086 | plain_containment_8 = 0.0467 | minhash_8=0.0000\n",
      "table_2.csv <> table_5.csv | plain_8=0.0103 | plain_containment_8 = 0.0624 | minhash_8=0.0078\n",
      "table_2.csv <> table_6.csv | plain_8=0.0594 | plain_containment_8 = 0.2216 | minhash_8=0.0938\n",
      "table_2.csv <> table_7.csv | plain_8=0.0024 | plain_containment_8 = 0.0059 | minhash_8=0.0000\n",
      "table_2.csv <> table_8.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_2.csv <> table_9.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_4.csv <> table_5.csv | plain_8=0.0034 | plain_containment_8 = 0.0073 | minhash_8=0.0078\n",
      "table_4.csv <> table_6.csv | plain_8=0.0022 | plain_containment_8 = 0.0056 | minhash_8=0.0000\n",
      "table_4.csv <> table_7.csv | plain_8=0.0007 | plain_containment_8 = 0.0051 | minhash_8=0.0078\n",
      "table_4.csv <> table_8.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_4.csv <> table_9.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_5.csv <> table_6.csv | plain_8=0.0285 | plain_containment_8 = 0.0758 | minhash_8=0.0312\n",
      "table_5.csv <> table_7.csv | plain_8=0.0000 | plain_containment_8 = 0.0001 | minhash_8=0.0000\n",
      "table_5.csv <> table_8.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_5.csv <> table_9.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_6.csv <> table_7.csv | plain_8=0.0001 | plain_containment_8 = 0.0003 | minhash_8=0.0000\n",
      "table_6.csv <> table_8.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_6.csv <> table_9.csv | plain_8=0.0000 | plain_containment_8 = 0.0000 | minhash_8=0.0000\n",
      "table_7.csv <> table_8.csv | plain_8=0.0330 | plain_containment_8 = 0.0703 | minhash_8=0.0391\n",
      "table_7.csv <> table_9.csv | plain_8=0.0001 | plain_containment_8 = 0.0004 | minhash_8=0.0000\n",
      "table_8.csv <> table_9.csv | plain_8=0.0009 | plain_containment_8 = 0.0047 | minhash_8=0.0000\n"
     ]
    }
   ],
   "source": [
    "def discovery_algorithm():\n",
    "    \"\"\"Function should be able to perform data discovery to find related datasets\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of pairs of related datasets\n",
    "    \"\"\"\n",
    "\n",
    "    base_dir = \"./lake33\"\n",
    "    datasets = {}\n",
    "    ds = {}\n",
    "\n",
    "    if not os.path.isdir(base_dir):\n",
    "        return datasets  # return empty if folder not found\n",
    "    for fname in os.listdir(base_dir):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(base_dir, fname)\n",
    "            try:\n",
    "                df = pd.read_csv(fpath, low_memory=False)\n",
    "            except Exception:\n",
    "                continue\n",
    "            df = df.dropna().astype(object)\n",
    "            datasets[fname] = df.to_numpy().flatten()\n",
    "            string = \" \".join(str(x) for x in datasets[fname]).replace(\"\\n\", \" \")\n",
    "            # merge consecutive whitespaces (book advice)\n",
    "            string = re.sub(' +', ' ', string)\n",
    "            ds[fname] = string\n",
    "\n",
    "    def jaccard_set(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        return intersection / union\n",
    "\n",
    "    def jaccard_containment(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        def jacc_c1(set1, set2):\n",
    "            if len(set1) == 0:\n",
    "                return 0.0\n",
    "            return intersection / len(set1)\n",
    "        def jacc_c2(set1, set2):\n",
    "            if len(set2) == 0:\n",
    "                return 0.0\n",
    "            return intersection / len(set2)\n",
    "        return max(jacc_c1(set1, set2), jacc_c2(set1, set2))\n",
    "\n",
    "\n",
    "    # Cache shingles and MinHash per file per k to avoid recomputing\n",
    "    #    ks_range = range(8, 9)  # log20 suggests 7.92 (chapter 3 rule of thumb)\n",
    "    shingles_k = {fname: {} for fname in ds}\n",
    "    minhash_k = {fname: {} for fname in ds}\n",
    "\n",
    "    for fname, text in ds.items():\n",
    "        for k in range(8,9):\n",
    "            kshingle = set(ks.shingleset_range(text, 8, 8)) #log20\n",
    "            shingles_k[fname][k] = kshingle\n",
    "            mh = MinHash(num_perm=128)\n",
    "            for sh in kshingle:\n",
    "                try:\n",
    "                    mh.update(sh.encode(\"utf8\"))\n",
    "                except Exception:\n",
    "                    mh.update(str(sh).encode(\"utf8\"))\n",
    "            minhash_k[fname][k] = mh\n",
    "\n",
    "    # Print plain Jaccard (set-based) and MinHash estimated Jaccard side by side for each k\n",
    "    filenames = list(ds.keys())\n",
    "    for i in range(len(filenames)):\n",
    "        for j in range(i + 1, len(filenames)):\n",
    "            f1 = filenames[i]\n",
    "            f2 = filenames[j]\n",
    "            parts = [f\"{f1} <> {f2}\"]\n",
    "            for k in range(8, 9):\n",
    "                s1 = shingles_k[f1][k]\n",
    "                s2 = shingles_k[f2][k]\n",
    "                plain_j = jaccard_set(s1, s2)\n",
    "                try:\n",
    "                    est_j = minhash_k[f1][k].jaccard(minhash_k[f2][k])\n",
    "                except Exception:\n",
    "                    est_j = None\n",
    "                parts.append(f\"plain_{k}={plain_j:.4f}\")\n",
    "                parts.append(f\"plain_containment_{k} = {jaccard_containment(s1, s2):.4f}\")\n",
    "                parts.append(f\"minhash_{k}={est_j if est_j is None else f'{est_j:.4f}'}\")\n",
    "            print(\" | \".join(parts))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "discovery_algorithm()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 10 and 11 appear to be the same. We delete table 10.\n",
    "(table_10.csv <> table_11.csv | plain_8=1.0000 | plain_containment_8 = 1.0000 | minhash_8=1.0000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY SUMMARY\n",
      "----------------------------------------\n",
      "Total files: 18\n",
      "Successfully loaded: 17\n",
      "Failed to load: 1\n",
      "Failed files: ['table_18.csv']\n",
      "Files with quality issues: 10\n",
      "Files with critical parsing issues: 7\n",
      "\n",
      "Detailed report saved to: data_quality_report.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def robust_csv_loader(base_dir=\"./lake33\"):\n",
    "    \"\"\"Load CSV files with various fallback strategies for malformed data\"\"\"\n",
    "    \n",
    "    datasets = {}\n",
    "    failed_files = []\n",
    "    \n",
    "    if not os.path.isdir(base_dir):\n",
    "        return datasets, failed_files\n",
    "        \n",
    "    for fname in os.listdir(base_dir):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(base_dir, fname)\n",
    "            \n",
    "            df = None\n",
    "            load_method = None\n",
    "            \n",
    "            # Try multiple loading strategies\n",
    "            strategies = [\n",
    "                ('standard', lambda: pd.read_csv(fpath, low_memory=False)),\n",
    "                ('skip_bad_lines', lambda: pd.read_csv(fpath, low_memory=False, on_bad_lines='skip')),\n",
    "                ('semicolon_sep', lambda: pd.read_csv(fpath, sep=';', low_memory=False)),\n",
    "                ('tab_sep', lambda: pd.read_csv(fpath, sep='\\t', low_memory=False)),\n",
    "                ('pipe_sep', lambda: pd.read_csv(fpath, sep='|', low_memory=False)),\n",
    "                ('raw_lines', lambda: pd.read_csv(fpath, sep='\\n', header=None, names=['raw_data']))\n",
    "            ]\n",
    "            \n",
    "            for method_name, loader in strategies:\n",
    "                try:\n",
    "                    df = loader()\n",
    "                    load_method = method_name\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if df is not None:\n",
    "                datasets[fname] = {'dataframe': df, 'load_method': load_method}\n",
    "            else:\n",
    "                failed_files.append(fname)\n",
    "    \n",
    "    return datasets, failed_files\n",
    "\n",
    "def safe_json_convert(obj):\n",
    "    \"\"\"Convert objects to JSON-serializable format\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    elif hasattr(obj, 'dtype'):\n",
    "        return str(obj)\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "def generate_comprehensive_report(datasets, failed_files, output_file=\"data_quality_report.json\"):\n",
    "    \"\"\"Generate comprehensive report and save to file\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'summary': {\n",
    "            'total_files_attempted': len(datasets) + len(failed_files),\n",
    "            'successfully_loaded': len(datasets),\n",
    "            'failed_to_load': len(failed_files),\n",
    "            'failed_files': failed_files\n",
    "        },\n",
    "        'file_details': {}\n",
    "    }\n",
    "    \n",
    "    for filename, data_info in datasets.items():\n",
    "        df = data_info['dataframe']\n",
    "        method = data_info['load_method']\n",
    "        \n",
    "        file_report = {\n",
    "            'load_method': method,\n",
    "            'shape': [int(df.shape[0]), int(df.shape[1])],\n",
    "            'issues': []\n",
    "        }\n",
    "        \n",
    "        if method in ['standard', 'semicolon_sep', 'tab_sep', 'pipe_sep', 'skip_bad_lines']:\n",
    "            # Normal DataFrame analysis\n",
    "            total_nulls = df.isnull().sum().sum()\n",
    "            null_percentage = (total_nulls / df.size * 100) if df.size > 0 else 0\n",
    "            \n",
    "            # Convert data types to JSON-serializable format\n",
    "            data_types = {}\n",
    "            for dtype, count in df.dtypes.value_counts().items():\n",
    "                data_types[str(dtype)] = int(count)\n",
    "            \n",
    "            file_report.update({\n",
    "                'columns': [str(col) for col in df.columns],\n",
    "                'data_types': data_types,\n",
    "                'total_nulls': int(total_nulls),\n",
    "                'null_percentage': float(round(null_percentage, 2)),\n",
    "                'duplicate_rows': int(df.duplicated().sum()),\n",
    "                'potential_id_columns': []\n",
    "            })\n",
    "            \n",
    "            # Find potential ID columns\n",
    "            for col in df.columns:\n",
    "                if len(df) > 0:\n",
    "                    try:\n",
    "                        uniqueness = df[col].nunique() / len(df)\n",
    "                        if uniqueness > 0.95:\n",
    "                            file_report['potential_id_columns'].append(str(col))\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Check for suspicious patterns\n",
    "            suspicious_patterns = []\n",
    "            for col in df.select_dtypes(include=['object']).columns:\n",
    "                try:\n",
    "                    suspicious_count = df[col].astype(str).str.contains(\n",
    "                        r'error|null|#N/A|test|dummy|placeholder|nan', \n",
    "                        case=False, na=False\n",
    "                    ).sum()\n",
    "                    if suspicious_count > df.shape[0] * 0.01:\n",
    "                        suspicious_patterns.append({\n",
    "                            'column': str(col),\n",
    "                            'count': int(suspicious_count),\n",
    "                            'percentage': float(round((suspicious_count / len(df)) * 100, 2))\n",
    "                        })\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            file_report['suspicious_patterns'] = suspicious_patterns\n",
    "            \n",
    "            # Add issues based on findings\n",
    "            if method == 'skip_bad_lines':\n",
    "                file_report['issues'].append('Some lines were skipped due to parsing errors')\n",
    "            if null_percentage > 50:\n",
    "                file_report['issues'].append(f'High missing data: {null_percentage:.1f}%')\n",
    "            if df.duplicated().sum() > df.shape[0] * 0.1:\n",
    "                file_report['issues'].append('High duplicate rate')\n",
    "            if suspicious_patterns:\n",
    "                file_report['issues'].append(f'Suspicious patterns found in {len(suspicious_patterns)} columns')\n",
    "                \n",
    "        elif method == 'raw_lines':\n",
    "            file_report['issues'].append('File loaded as single column - needs manual parsing')\n",
    "            # Get sample content safely\n",
    "            sample_content = []\n",
    "            for val in df.head(3)['raw_data']:\n",
    "                sample_content.append(str(val)[:100])  # Truncate long lines\n",
    "            file_report['sample_content'] = sample_content\n",
    "        \n",
    "        report['file_details'][filename] = file_report\n",
    "    \n",
    "    # Save to file with safe JSON conversion\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=safe_json_convert)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n",
    "        # Fallback: save as text\n",
    "        with open(output_file.replace('.json', '.txt'), 'w') as f:\n",
    "            f.write(str(report))\n",
    "        print(f\"Report saved as text file instead: {output_file.replace('.json', '.txt')}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "def print_summary(report):\n",
    "    \"\"\"Print only key summary information\"\"\"\n",
    "    \n",
    "    print(\"DATA QUALITY SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total files: {report['summary']['total_files_attempted']}\")\n",
    "    print(f\"Successfully loaded: {report['summary']['successfully_loaded']}\")\n",
    "    print(f\"Failed to load: {report['summary']['failed_to_load']}\")\n",
    "    \n",
    "    if report['summary']['failed_files']:\n",
    "        print(f\"Failed files: {report['summary']['failed_files']}\")\n",
    "    \n",
    "    # Count files with issues\n",
    "    files_with_issues = 0\n",
    "    critical_issues = 0\n",
    "    \n",
    "    for filename, details in report['file_details'].items():\n",
    "        if details['issues']:\n",
    "            files_with_issues += 1\n",
    "        if details['load_method'] in ['raw_lines', 'skip_bad_lines']:\n",
    "            critical_issues += 1\n",
    "    \n",
    "    print(f\"Files with quality issues: {files_with_issues}\")\n",
    "    print(f\"Files with critical parsing issues: {critical_issues}\")\n",
    "    print(f\"\\nDetailed report saved to: data_quality_report.json\")\n",
    "\n",
    "# Usage:\n",
    "datasets, failed_files = robust_csv_loader()\n",
    "report = generate_comprehensive_report(datasets, failed_files)\n",
    "print_summary(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the CSV files manually:\n",
    "\n",
    "Table 0 and 1 seem to have weird formatting, and 1 containing emojis. Using underscore as seperator.\\\n",
    "Table 2 column names have emojis and weird formatting.\\\n",
    "Table 3 Seems to have BOM characters in the header and weird formatting as well.\\\n",
    "Table 4 has BOM characters and emojis in header.\\\n",
    "Table 5 has encoding issues BOM characters.\\\n",
    "Table 6 misses headers.\\\n",
    "Table 7 the column names look scrambled.\\\n",
    "Table 8 has duplicate headers  with emoji pollution, and column entries split by underscore.\\\n",
    "Table 9 has heavy emoji polution, and entries seperated with underscore.\\\n",
    "Table 11 missing headers.\\\n",
    "Table 12 looks like some entries are in the wrong place. Entries seperated with underscore. Heavy emoji polution.\\\n",
    "Table 13 looks intact.\\\n",
    "Table 14 looks intact, but no valid seperator. The empty ' ' is used as seperator.\\\n",
    "Table 15 has heavy emoji polution, seems like entries are out of place, and seperated by underscore. Duplicated rows.\\\n",
    "Table 16 seems to be the same as 15 but less duplicate rows?\\\n",
    "Table 17 has duplicate headers with emojis.\\\n",
    "Table 18 seems to be the same as 17 but has scrambled headers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean table 2, which looks like the same as table 1, but different formats. Columns of table 1 seems to be split with '_', and table 0 with ','. Different emojis as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Tables 1 and 2:\n",
      "==================================================\n",
      "\n",
      "table_1.csv:\n",
      "  Load method: skip_bad_lines\n",
      "  Shape: [23569, 1]\n",
      "  Columns (first char): ['Bottom Quartile Advertising and promotion\\nQuart inférieur Publicité et promotion✔️'\n",
      " 'Bottom Quar...\n",
      "  Issues: ['Some lines were skipped due to parsing errors']\n",
      "\n",
      "table_2.csv:\n",
      "  Load method: standard\n",
      "  Shape: [23518, 64]\n",
      "  Columns (first char): ['Bottom Quartile Advertising and promotion\\nQuart inférieur Publicité et promotion✨'\n",
      " 'Bottom Quart...\n",
      "  Issues: []\n"
     ]
    }
   ],
   "source": [
    "# Compare tables 1 and 2 from the report\n",
    "with open('data_quality_report.json', 'r') as f:\n",
    "    report = json.load(f)\n",
    "\n",
    "print(\"Comparing Tables 1 and 2:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for table in ['table_1.csv', 'table_2.csv']:\n",
    "    details = report['file_details'][table]\n",
    "    print(f\"\\n{table}:\")\n",
    "    print(f\"  Load method: {details['load_method']}\")\n",
    "    print(f\"  Shape: {details['shape']}\")\n",
    "    print(f\"  Columns (first char): {details['columns'][0][:100]}...\")\n",
    "    print(f\"  Issues: {details.get('issues', [])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2 structure (successfully loaded):\n",
      "Shape: (23518, 64)\n",
      "\n",
      "Number of columns: 64\n",
      "\n",
      "Column names (with emoji pollution):\n",
      "1. ['Bottom Quartile Advertising and promotion\\nQuart inférieur Publicité et promot...\n",
      "2. ['Quality Indicator\\nIndicateurs de qualité.10👀'\n",
      " 'Quality Indicator\\nIndicateur...\n",
      "3. ['Lower Middle Amortization and depletion\\n2e quart Amortissement et tarissement...\n",
      "4. [\"Bottom Quartile Operating Expenses (indirect)\\nQuart inférieurs Dépenses d'exp...\n",
      "5. ['Top Quartile Purchases, materials and sub-contracts\\nquart supérieurs Achats, ...\n",
      "\n",
      "First row of data:\n",
      "['Bottom Quartile Advertising and promotion\\nQuart inférieur Publicité et promotion✨'\\n 'Bottom Quartile Advertising and promotion\\nQuart inférieur Publicité et promotion❤️']                                                     99999.9\n",
      "['Quality Indicator\\nIndicateurs de qualité.10👀'\\n 'Quality Indicator\\nIndicateurs de qualité.10😬']                                                                                                                                      E\n",
      "['Lower Middle Amortization and depletion\\n2e quart Amortissement et tarissement🤯'\\n 'Lower Middle Amortization and depletion\\n2e quart Amortissement et tarissement✔️']                                                           99999.9\n",
      "[\"Bottom Quartile Operating Expenses (indirect)\\nQuart inférieurs Dépenses d'exploitation (frais indirects)🤔\"\\n \"Bottom Quartile Operating Expenses (indirect)\\nQuart inférieurs Dépenses d'exploitation (frais indirects)🙌\"]      99999.9\n",
      "['Top Quartile Purchases, materials and sub-contracts\\nquart supérieurs Achats, matériaux et sous-traitances😇'\\n 'Top Quartile Purchases, materials and sub-contracts\\nquart supérieurs Achats, matériaux et sous-traitances🙌']    99999.9\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Examine Table 2's structure\n",
    "df2 = clean_datasets['table_2.csv']\n",
    "\n",
    "print(\"Table 2 structure (successfully loaded):\")\n",
    "print(f\"Shape: {df2.shape}\")\n",
    "print(f\"\\nNumber of columns: {len(df2.columns)}\")\n",
    "print(\"\\nColumn names (with emoji pollution):\")\n",
    "for i, col in enumerate(df2.columns[:5]):\n",
    "    print(f\"{i+1}. {col[:80]}...\")\n",
    "\n",
    "print(f\"\\nFirst row of data:\")\n",
    "print(df2.iloc[0].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the column names are in English with French translation next to it, and emoji as well? Lets remove the emojis and only keep the english column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2 with properly cleaned columns:\n",
      "1. Bottom Quartile Advertising and promotion\n",
      "2. Quality Indicator\n",
      "3. Lower Middle Amortization and depletion\n",
      "4. Bottom Quartile Operating Expenses (indirect)\n",
      "5. Top Quartile Purchases, materials and sub-contracts\n",
      "6. Quality Indicator\n",
      "7. Lower Middle Utilities and telephone/telecommunication\n",
      "8. Top Quartile Net Profit/Loss\n",
      "9. Upper Middle Amortization and depletion\n",
      "10. % of Businesses Reporting\n",
      "\n",
      "Shape: (23518, 64)\n",
      "   Bottom Quartile Advertising and promotion  Quality Indicator  \\\n",
      "0                                    99999.9                  E   \n",
      "1                                        0.8                  E   \n",
      "2                                        0.4                  B   \n",
      "3                                    99999.9                  A   \n",
      "4                                        1.7                  B   \n",
      "\n",
      "   Lower Middle Amortization and depletion  \\\n",
      "0                                  99999.9   \n",
      "1                                      2.3   \n",
      "2                                      1.3   \n",
      "3                                  99999.9   \n",
      "4                                     10.6   \n",
      "\n",
      "   Bottom Quartile Operating Expenses (indirect)  \\\n",
      "0                                        99999.9   \n",
      "1                                          123.6   \n",
      "2                                           11.2   \n",
      "3                                        99999.9   \n",
      "4                                           77.4   \n",
      "\n",
      "   Top Quartile Purchases, materials and sub-contracts  Quality Indicator  \\\n",
      "0                                            99999.9                    E   \n",
      "1                                                0.0                    E   \n",
      "2                                               97.0                    B   \n",
      "3                                            99999.9                    A   \n",
      "4                                             1625.4                    B   \n",
      "\n",
      "   Lower Middle Utilities and telephone/telecommunication  \\\n",
      "0                                            99999.9        \n",
      "1                                                8.9        \n",
      "2                                                1.7        \n",
      "3                                            99999.9        \n",
      "4                                                5.4        \n",
      "\n",
      "   Top Quartile Net Profit/Loss  Upper Middle Amortization and depletion  \\\n",
      "0                       99999.9                                  99999.9   \n",
      "1                          41.2                                      6.3   \n",
      "2                          78.9                                      3.1   \n",
      "3                       99999.9                                  99999.9   \n",
      "4                          -2.8                                     20.6   \n",
      "\n",
      "  % of Businesses Reporting  ...  Quality Indicator % of Businesses Reporting  \\\n",
      "0                      38.9  ...                  E                     100.0   \n",
      "1                      71.4  ...                  E                     100.0   \n",
      "2                      77.6  ...                  E                      94.5   \n",
      "3                      80.0  ...                  E                     100.0   \n",
      "4                      83.1  ...                  C                     100.0   \n",
      "\n",
      "  % of Businesses Reporting % of Businesses Reporting  \\\n",
      "0                       5.6                     100.0   \n",
      "1                       5.7                     100.0   \n",
      "2                      10.9                     100.0   \n",
      "3                      40.0                     100.0   \n",
      "4                      40.4                     100.0   \n",
      "\n",
      "   Bottom Quartile Professional and business fees  \\\n",
      "0                                         99999.9   \n",
      "1                                            10.2   \n",
      "2                                             0.8   \n",
      "3                                         99999.9   \n",
      "4                                             3.3   \n",
      "\n",
      "   Upper Middle Delivery, shipping and warehouse expense  \\\n",
      "0                                            99999.9       \n",
      "1                                                0.4       \n",
      "2                                                0.2       \n",
      "3                                            99999.9       \n",
      "4                                               31.7       \n",
      "\n",
      "   Lower Middle Labour and Commissions  \\\n",
      "0                              99999.9   \n",
      "1                                220.3   \n",
      "2                                  6.6   \n",
      "3                              99999.9   \n",
      "4                                 87.1   \n",
      "\n",
      "   Upper Middle Utilities and telephone/telecommunication  Quality Indicator  \\\n",
      "0                                            99999.9                       E   \n",
      "1                                               13.6                       E   \n",
      "2                                                2.4                       C   \n",
      "3                                            99999.9                       A   \n",
      "4                                               13.1                       E   \n",
      "\n",
      "   Top Quartile Insurance  \n",
      "0                 99999.9  \n",
      "1                     1.8  \n",
      "2                     3.2  \n",
      "3                 99999.9  \n",
      "4                    10.9  \n",
      "\n",
      "[5 rows x 64 columns]\n",
      "\n",
      "Table 2 cleaned and saved!\n"
     ]
    }
   ],
   "source": [
    "# Clean Table 2 column names - remove newlines and French text\n",
    "df2_clean = clean_datasets['table_2.csv'].copy()\n",
    "\n",
    "cleaned_column_names = []\n",
    "for col in df2_clean.columns:\n",
    "    col_str = str(col)\n",
    "    # Split on newline and take only the English part (before \\n)\n",
    "    if '\\\\n' in col_str:\n",
    "        english_part = col_str.split('\\\\n')[0]\n",
    "    else:\n",
    "        english_part = col_str\n",
    "    \n",
    "    # Remove list brackets, quotes, and extra characters\n",
    "    english_part = english_part.replace(\"['\", \"\").replace(\"']\", \"\").replace('[', '').replace('\"', '')\n",
    "    # Remove emojis and special characters\n",
    "    english_part = ''.join(char for char in english_part if ord(char) < 128)\n",
    "    # Clean up extra spaces\n",
    "    english_part = ' '.join(english_part.split())\n",
    "    \n",
    "    cleaned_column_names.append(english_part.strip())\n",
    "\n",
    "df2_clean.columns = cleaned_column_names\n",
    "\n",
    "print(\"Table 2 with properly cleaned columns:\")\n",
    "for i, col in enumerate(df2_clean.columns[:10]):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "print(f\"\\nShape: {df2_clean.shape}\")\n",
    "print(df2_clean.head())\n",
    "\n",
    "# Save it\n",
    "df2_clean.to_csv('./cleaned_table_2.csv', index=False)\n",
    "print(\"\\nTable 2 cleaned and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our discovery algorithm to show that table 1-2 are the same, we need to clean 1 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 cleaned:\n",
      "Shape: (23518, 64)\n",
      "Columns (first 10): ['Bottom Quartile Advertising and promotion', 'Quality Indicator', 'Lower Middle Amortization and depletion', 'Bottom Quartile Operating Expenses (indirect)', 'Top Quartile Purchases, materials and sub-contracts', 'Quality Indicator', 'Lower Middle Utilities and telephone/telecommunication', 'Top Quartile Net Profit/Loss', 'Upper Middle Amortization and depletion', '% of Businesses Reporting']\n",
      "   Bottom Quartile Advertising and promotion  Quality Indicator  \\\n",
      "0                                    99999.9                  E   \n",
      "1                                        0.8                  E   \n",
      "\n",
      "   Lower Middle Amortization and depletion  \\\n",
      "0                                  99999.9   \n",
      "1                                      2.3   \n",
      "\n",
      "   Bottom Quartile Operating Expenses (indirect)  \\\n",
      "0                                        99999.9   \n",
      "1                                          123.6   \n",
      "\n",
      "   Top Quartile Purchases, materials and sub-contracts  Quality Indicator  \\\n",
      "0                                            99999.9                    E   \n",
      "1                                                0.0                    E   \n",
      "\n",
      "   Lower Middle Utilities and telephone/telecommunication  \\\n",
      "0                                            99999.9        \n",
      "1                                                8.9        \n",
      "\n",
      "   Top Quartile Net Profit/Loss  Upper Middle Amortization and depletion  \\\n",
      "0                       99999.9                                  99999.9   \n",
      "1                          41.2                                      6.3   \n",
      "\n",
      "  % of Businesses Reporting  ...  Quality Indicator % of Businesses Reporting  \\\n",
      "0                      38.9  ...                  E                     100.0   \n",
      "1                      71.4  ...                  E                     100.0   \n",
      "\n",
      "  % of Businesses Reporting % of Businesses Reporting  \\\n",
      "0                       5.6                     100.0   \n",
      "1                       5.7                     100.0   \n",
      "\n",
      "   Bottom Quartile Professional and business fees  \\\n",
      "0                                         99999.9   \n",
      "1                                            10.2   \n",
      "\n",
      "   Upper Middle Delivery, shipping and warehouse expense  \\\n",
      "0                                            99999.9       \n",
      "1                                                0.4       \n",
      "\n",
      "   Lower Middle Labour and Commissions  \\\n",
      "0                              99999.9   \n",
      "1                                220.3   \n",
      "\n",
      "   Upper Middle Utilities and telephone/telecommunication  Quality Indicator  \\\n",
      "0                                            99999.9                       E   \n",
      "1                                               13.6                       E   \n",
      "\n",
      "   Top Quartile Insurance  \n",
      "0                 99999.9  \n",
      "1                     1.8  \n",
      "\n",
      "[2 rows x 64 columns]\n",
      "\n",
      "Table 1 cleaned and saved!\n"
     ]
    }
   ],
   "source": [
    "# Clean Table 1 (same process as Table 0)\n",
    "df1_clean = pd.read_csv('./lake33/table_1.csv', sep='_', on_bad_lines='skip')\n",
    "\n",
    "# Clean column names\n",
    "cleaned_column_names = []\n",
    "for col in df1_clean.columns:\n",
    "    col_str = str(col)\n",
    "    if '\\\\n' in col_str:\n",
    "        english_part = col_str.split('\\\\n')[0]\n",
    "    else:\n",
    "        english_part = col_str\n",
    "    \n",
    "    english_part = english_part.replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '')\n",
    "    english_part = ''.join(char for char in english_part if ord(char) < 128)\n",
    "    english_part = ' '.join(english_part.split())\n",
    "    \n",
    "    cleaned_column_names.append(english_part.strip())\n",
    "\n",
    "df1_clean.columns = cleaned_column_names\n",
    "\n",
    "print(\"Table 1 cleaned:\")\n",
    "print(f\"Shape: {df1_clean.shape}\")\n",
    "print(f\"Columns (first 10): {df1_clean.columns.tolist()[:10]}\")\n",
    "print(df1_clean.head(2))\n",
    "\n",
    "# Save it\n",
    "df1_clean.to_csv('./cleaned_table_1.csv', index=False)\n",
    "print(\"\\nTable 1 cleaned and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets clean table 0, which has French header polution as well. The French and english part are seperated by \\n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 0 raw column names:\n",
      "1. 'Bottom Quartile Professional and business fees\\nQuart inférieur Intérêts et frais bancaires'\n",
      "2. 'Quality Indicator\\nIndicateurs de qualité.8'\n",
      "3. '% of Businesses Reporting\\n% des entreprises déclarantes.13'\n",
      "4. 'Purchases, materials and sub-contracts\\nAchats, matériaux et sous-traitances'\n",
      "5. 'Lower Middle Total expenses\\n2e quart Dépenses totales'\n",
      "\n",
      "First row of data:\n",
      "Bottom Quartile Professional and business fees\\nQuart inférieur Intérêts et frais bancaires      0.7\n",
      "Quality Indicator\\nIndicateurs de qualité.8                                                        B\n",
      "% of Businesses Reporting\\n% des entreprises déclarantes.13                                     97.0\n",
      "Purchases, materials and sub-contracts\\nAchats, matériaux et sous-traitances                   369.4\n",
      "Lower Middle Total expenses\\n2e quart Dépenses totales                                         228.1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Examine Table 0's raw structure\n",
    "df0 = pd.read_csv('./lake33/table_0.csv', sep='_', on_bad_lines='skip')\n",
    "\n",
    "print(\"Table 0 raw column names:\")\n",
    "for i, col in enumerate(df0.columns[:5]):\n",
    "    print(f\"{i+1}. {repr(col)}\")\n",
    "\n",
    "print(f\"\\nFirst row of data:\")\n",
    "print(df0.iloc[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 0 with English-only columns:\n",
      "['Bottom Quartile Professional and business fees', 'Quality Indicator', '% of Businesses Reporting', 'Purchases, materials and sub-contracts', 'Lower Middle Total expenses', 'Bottom Quartile Net Profit/Loss', 'Top Quartile Net Profit/Loss', 'Lower Middle Net Profit/Loss', 'Top Quartile Operating Expenses (indirect)', 'Upper Middle Other expenses']\n",
      "\n",
      "Shape: (22267, 52)\n",
      "   Bottom Quartile Professional and business fees Quality Indicator  \\\n",
      "0                                             0.7                 B   \n",
      "1                                         99999.9                 A   \n",
      "\n",
      "   % of Businesses Reporting  Purchases, materials and sub-contracts  \\\n",
      "0                       97.0                                   369.4   \n",
      "1                      100.0                                 99999.9   \n",
      "\n",
      "   Lower Middle Total expenses  Bottom Quartile Net Profit/Loss  \\\n",
      "0                        228.1                             11.7   \n",
      "1                      99999.9                          99999.9   \n",
      "\n",
      "   Top Quartile Net Profit/Loss  Lower Middle Net Profit/Loss  \\\n",
      "0                         147.9                          22.1   \n",
      "1                       99999.9                       99999.9   \n",
      "\n",
      "   Top Quartile Operating Expenses (indirect)  Upper Middle Other expenses  \\\n",
      "0                                       578.4                        108.9   \n",
      "1                                     99999.9                      99999.9   \n",
      "\n",
      "   ...  Top Quartile Amortization and depletion  % of Businesses Reporting  \\\n",
      "0  ...                                     13.9                       81.8   \n",
      "1  ...                                  99999.9                      100.0   \n",
      "\n",
      "   Lower Middle Cost of Sales  Other expenses  \\\n",
      "0                       104.9            65.7   \n",
      "1                     99999.9         99999.9   \n",
      "\n",
      "   Top Quartile Delivery, shipping and warehouse expense  \\\n",
      "0                                               24.1       \n",
      "1                                            99999.9       \n",
      "\n",
      "   Upper Middle Utilities and telephone/telecommunication  \\\n",
      "0                                                8.7        \n",
      "1                                            99999.9        \n",
      "\n",
      "   Upper Middle cost of Sales Top Quartile Insurance  \\\n",
      "0                       306.7                   14.1   \n",
      "1                     99999.9                99999.9   \n",
      "\n",
      "   Lower Middle Wages and Benefits     Rent  \n",
      "0                              6.5     22.7  \n",
      "1                          99999.9  99999.9  \n",
      "\n",
      "[2 rows x 52 columns]\n",
      "\n",
      "Table 0 cleaned and saved!\n"
     ]
    }
   ],
   "source": [
    "# Clean Table 0 - extract only English part before newline\n",
    "df0_clean = pd.read_csv('./lake33/table_0.csv', sep='_', on_bad_lines='skip')\n",
    "\n",
    "# Split on actual newline character and take first part (English)\n",
    "clean_columns = []\n",
    "for col in df0_clean.columns:\n",
    "    # Split on newline and take the English part (before \\n)\n",
    "    english_part = col.split('\\n')[0]\n",
    "    clean_columns.append(english_part)\n",
    "\n",
    "df0_clean.columns = clean_columns\n",
    "\n",
    "print(\"Table 0 with English-only columns:\")\n",
    "print(df0_clean.columns.tolist()[:10])\n",
    "print(f\"\\nShape: {df0_clean.shape}\")\n",
    "print(df0_clean.head(2))\n",
    "\n",
    "# Save it\n",
    "df0_clean.to_csv('./cleaned_table_0.csv', index=False)\n",
    "print(\"\\nTable 0 cleaned and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets clean table 3, 4, and 5 which all have BOM characters, with table 4 having emojis as well, and table 3 having '_' as seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3 after BOM removal:\n",
      "Columns (first 5): ['REF_DATE', 'GEO', 'DGUID', 'North American Industry Classification System (NAICS)', 'Enterprise size']\n",
      "Saved\n",
      "Table 4 cleaned:\n",
      "Shape: (34057, 12)\n",
      "Columns: ['REF_DATE REF_DATE', 'GEO GEO', 'DGUID DGUID', 'Type of residential investment Type of residential investment', 'UOM UOM', 'UOM_ID UOM_ID', 'SCALAR_FACTOR SCALAR_FACTOR', 'SCALAR_ID SCALAR_ID', 'VECTOR VECTOR', 'COORDINATE COORDINATE', 'VALUE VALUE', 'DECIMALS DECIMALS']\n",
      "Saved\n",
      "\n",
      "Table 5 cleaned:\n",
      "Shape: (38016, 14)\n",
      "Columns: ['REF_DATE', 'GEO', 'Age group', 'Sex', 'Bocwadian standard', 'Characteristics', 'Ul', 'UOM_ID', 'SCALAR_FACTOR', 'SCALAR_ID', 'VECTOR', 'COORDINATE', 'VALUE', 'DECIMALS']\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Fix Table 3 BOM character\n",
    "df3 = pd.read_csv('./cleaned_table_3.csv')\n",
    "cols = df3.columns.tolist()\n",
    "cols[0] = cols[0].replace('\\ufeff', '').replace('ï»¿', '')\n",
    "df3.columns = cols\n",
    "\n",
    "print(\"Table 3 cleaned:\")\n",
    "print(f\"Columns (first 5): {df3.columns.tolist()[:5]}\")\n",
    "df3.to_csv('./cleaned_table_3.csv', index=False)\n",
    "print(\"Saved\")\n",
    "\n",
    "# Table 4  emoji pollution, split on newline and remove BOM\n",
    "df4 = pd.read_csv('./lake33/table_4.csv')\n",
    "clean_cols_4 = []\n",
    "for col in df4.columns:\n",
    "    col_str = str(col)\n",
    "    if '\\\\n' in col_str:\n",
    "        col_str = col_str.split('\\\\n')[0]\n",
    "    col_str = col_str.replace('\\ufeff', '').replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '')\n",
    "    col_str = ''.join(char for char in col_str if ord(char) < 128)\n",
    "    col_str = ' '.join(col_str.split()).strip()\n",
    "    clean_cols_4.append(col_str)\n",
    "\n",
    "df4.columns = clean_cols_4\n",
    "print(\"Table 4 cleaned:\")\n",
    "print(f\"Shape: {df4.shape}\")\n",
    "print(f\"Columns: {df4.columns.tolist()}\")\n",
    "df4.to_csv('./cleaned_table_4.csv', index=False)\n",
    "print(\"Saved\\n\")\n",
    "\n",
    "# Table 5  just fix first column BOM\n",
    "df5 = pd.read_csv('./lake33/table_5.csv')\n",
    "cols = df5.columns.tolist()\n",
    "cols[0] = 'REF_DATE'\n",
    "df5.columns = cols\n",
    "print(\"Table 5 cleaned:\")\n",
    "print(f\"Shape: {df5.shape}\")\n",
    "print(f\"Columns: {df5.columns.tolist()}\")\n",
    "df5.to_csv('./cleaned_table_5.csv', index=False)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 6 is missing headers and there seems to be no similar table. Lets clean other tables first and we might find a similar table we can use the headers from. Table 7 has obfuscated headers and there doesn't seem to be another similar table. Lets continue with cleaning other tables, in the hopes of finding a similar table. Lets clean table 8, which is using underscore as seperator, has duplicate headers and emojis. Table 17 has similar issues, so we can clean them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 8: (34868, 44)\n",
      "Columns (first 10): ['PERC_SINGLE_UNIT', 'ROUTE_NUMBER', 'DONUT_VOLUME_GROUP', 'IS_GROUPED', 'F_SYSTEM', 'FUT_AADT_YEAR', 'MEDIAN_TYPE', 'AADT', 'PEAK_LANES', 'SURV_C']\n",
      "Duplicates: 0\n",
      "After deduplication: 34868 rows\n",
      "Table 8 saved\n",
      "\n",
      "Table 17: (28781, 7)\n",
      "Columns: ['received_date', 'Auth_Description', 'Op_Name', 'Short Notice Short Notice', 'Description', 'Variation Number Variation Number', 'Service_Type_Other_Details']\n",
      "Duplicates: 6048\n",
      "After deduplication: 22733 rows\n",
      "Table 17 saved\n"
     ]
    }
   ],
   "source": [
    "def remove_emojis(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "    cleaned = ''.join(char for char in text if ord(char) < 128)\n",
    "    cleaned = re.sub(r\"\\[|\\]|'\", '', cleaned)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned if cleaned else text\n",
    "\n",
    "# Clean Table 8\n",
    "df8 = pd.read_csv('./lake33/table_8.csv', sep='_', on_bad_lines='skip')\n",
    "\n",
    "# Clean column names - remove emojis and duplicates\n",
    "clean_cols_8 = []\n",
    "for col in df8.columns:\n",
    "    col_str = str(col)\n",
    "    if '\\\\n' in col_str:\n",
    "        col_str = col_str.split('\\\\n')[0]\n",
    "    col_str = col_str.replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '')\n",
    "    col_str = ''.join(char for char in col_str if ord(char) < 128)\n",
    "    col_str = ' '.join(col_str.split()).strip()\n",
    "    \n",
    "    # Remove duplicate names (like \"ROUTE_NUMBER ROUTE_NUMBER\")\n",
    "    parts = col_str.split()\n",
    "    if len(parts) >= 2 and parts[0] == parts[1]:\n",
    "        col_str = parts[0]\n",
    "    \n",
    "    clean_cols_8.append(col_str)\n",
    "\n",
    "df8.columns = clean_cols_8\n",
    "\n",
    "print(f\"Table 8: {df8.shape}\")\n",
    "print(f\"Columns (first 10): {df8.columns.tolist()[:10]}\")\n",
    "\n",
    "# Clean data values\n",
    "for col in df8.columns:\n",
    "    df8[col] = df8[col].apply(remove_emojis)\n",
    "\n",
    "print(f\"Duplicates: {df8.duplicated().sum()}\")\n",
    "df8 = df8.drop_duplicates()\n",
    "print(f\"After deduplication: {df8.shape[0]} rows\")\n",
    "\n",
    "df8.to_csv('./cleaned_table_8.csv', index=False)\n",
    "print(\"Table 8 saved\\n\")\n",
    "\n",
    "# Clean Table 17\n",
    "df17 = pd.read_csv('./lake33/table_17.csv', sep='_', on_bad_lines='skip')\n",
    "\n",
    "# Clean column names\n",
    "clean_cols_17 = []\n",
    "for col in df17.columns:\n",
    "    col_str = str(col)\n",
    "    if '\\\\n' in col_str:\n",
    "        col_str = col_str.split('\\\\n')[0]\n",
    "    col_str = col_str.replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '')\n",
    "    col_str = ''.join(char for char in col_str if ord(char) < 128)\n",
    "    col_str = ' '.join(col_str.split()).strip()\n",
    "    \n",
    "    # Remove duplicate names\n",
    "    parts = col_str.split()\n",
    "    if len(parts) >= 2 and parts[0] == parts[1]:\n",
    "        col_str = parts[0]\n",
    "    \n",
    "    clean_cols_17.append(col_str)\n",
    "\n",
    "df17.columns = clean_cols_17\n",
    "\n",
    "print(f\"Table 17: {df17.shape}\")\n",
    "print(f\"Columns: {df17.columns.tolist()}\")\n",
    "\n",
    "# Clean data values\n",
    "for col in df17.columns:\n",
    "    df17[col] = df17[col].apply(remove_emojis)\n",
    "\n",
    "print(f\"Duplicates: {df17.duplicated().sum()}\")\n",
    "df17 = df17.drop_duplicates()\n",
    "print(f\"After deduplication: {df17.shape[0]} rows\")\n",
    "\n",
    "df17.to_csv('./cleaned_table_17.csv', index=False)\n",
    "print(\"Table 17 saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 9 looks like to have corrupt entries heavily polluted by emojis, and underscores as seperators, and quotes. It also seems to have a lot of MSNG values, think it is 'missing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 9 after underscore parsing:\n",
      "Shape: (26485, 14)\n",
      "Columns: ['STN_TYP', 'MAX_WND_DIR_10M_PST1HR', 'PCPN_AMT_PST6HRS', 'VAR_WND_DIR_UPBND_10M_P10MT', 'MSC_ID', 'CEILOMTR_ELEV', 'CLD_BAS_HGT_1', 'MAX_AIR_TEMP_PST6HRS', 'DNSTY_ALTD', 'AVG_BARO_READ_PST1MT', 'DER_PRES_TEND_CHAR_PST3HRS', 'AVG_WND_SPD_10M_PST2MTS', 'CLD_AMT_CODE_1', 'DATA_PAYLOAD_URI']\n",
      "\n",
      "Cleaned columns: ['STN_TYP', 'MAX_WND_DIR_10M_PST1HR', 'PCPN_AMT_PST6HRS', 'VAR_WND_DIR_UPBND_10M_P10MT', 'MSC_ID', 'CEILOMTR_ELEV', 'CLD_BAS_HGT_1', 'MAX_AIR_TEMP_PST6HRS', 'DNSTY_ALTD', 'AVG_BARO_READ_PST1MT', 'DER_PRES_TEND_CHAR_PST3HRS', 'AVG_WND_SPD_10M_PST2MTS', 'CLD_AMT_CODE_1', 'DATA_PAYLOAD_URI']\n",
      "\n",
      "First 2 rows:\n",
      "  STN_TYP MAX_WND_DIR_10M_PST1HR               PCPN_AMT_PST6HRS  \\\n",
      "0      12                    264                           MSNG   \n",
      "1      12                   MSNG  MSNG MSNG MSNG MSNG MSNG MSNG   \n",
      "\n",
      "  VAR_WND_DIR_UPBND_10M_P10MT   MSC_ID CEILOMTR_ELEV CLD_BAS_HGT_1  \\\n",
      "0                        MSNG  6127510         182.2        1189.0   \n",
      "1                         318  6115529         279.8        1494.0   \n",
      "\n",
      "  MAX_AIR_TEMP_PST6HRS DNSTY_ALTD AVG_BARO_READ_PST1MT  \\\n",
      "0                 MSNG     1100.0                993.0   \n",
      "1                 MSNG     2000.0                987.6   \n",
      "\n",
      "  DER_PRES_TEND_CHAR_PST3HRS AVG_WND_SPD_10M_PST2MTS CLD_AMT_CODE_1  \\\n",
      "0              1 1 1 1 1 1 1                    25.9            1.0   \n",
      "1                          8                     5.6            2.0   \n",
      "\n",
      "                                    DATA_PAYLOAD_URI  \n",
      "0  /data/nav_canada/observation/atmospheric/surfa...  \n",
      "1  /data/nav_canada/observation/atmospheric/surfa...  \n",
      "\n",
      "Table 9 cleaned and saved\n"
     ]
    }
   ],
   "source": [
    "# Parse Table 9 with underscore delimiter\n",
    "df9 = pd.read_csv('./lake33/table_9.csv', sep='_', on_bad_lines='skip')\n",
    "\n",
    "print(\"Table 9 after underscore parsing:\")\n",
    "print(f\"Shape: {df9.shape}\")\n",
    "print(f\"Columns: {df9.columns.tolist()}\")\n",
    "\n",
    "# Clean column names - remove quotes\n",
    "clean_cols = [col.replace('\"', '').strip() for col in df9.columns]\n",
    "df9.columns = clean_cols\n",
    "\n",
    "# Function to remove emojis from data values\n",
    "def remove_emojis(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "    # Remove emojis (characters outside basic ASCII)\n",
    "    cleaned = ''.join(char for char in text if ord(char) < 128)\n",
    "    # Clean up list-like patterns that remain\n",
    "    cleaned = re.sub(r\"\\[|\\]|'\", '', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned if cleaned else text\n",
    "\n",
    "# Apply emoji removal to all data columns\n",
    "for col in df9.columns:\n",
    "    df9[col] = df9[col].apply(remove_emojis)\n",
    "\n",
    "print(f\"\\nCleaned columns: {df9.columns.tolist()}\")\n",
    "print(f\"\\nFirst 2 rows:\")\n",
    "print(df9.head(2))\n",
    "\n",
    "# Save it\n",
    "df9.to_csv('./cleaned_table_9.csv', index=False)\n",
    "print(\"\\nTable 9 cleaned and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for table 11, it has missing headers. But it looks like it is the same as table 12. Lets clean table 12 first so we can use the headers for table 11 afterwards. Table 12 has underscores as seperators, emoji pollution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocean\\AppData\\Local\\Temp\\ipykernel_16600\\255.py:13: DtypeWarning: Columns (9,10,16,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df12 = pd.read_csv('./lake33/table_12.csv', sep='_', skiprows=1, header=None, on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AVG WND DIR 10M PST10MTS MAX AIR TEMP PST6HRS FLD ELEV PCPN AMT PST6HRS  \\\n",
      "0                      271                 MSNG    281.0             MSNG   \n",
      "1                      103                 MSNG    191.4             MSNG   \n",
      "\n",
      "                                     DATA ATTRIB NOT BARO ELEV  \\\n",
      "0                                                 []     279.4   \n",
      "1  Observational data provided by NAV CANADA. All...     190.9   \n",
      "\n",
      "  AVG MSLP PST1MT   MSC ID LTNG DIS RMK AVG VIS PST1MT  ...  \\\n",
      "0           995.6  6115529         MSNG         20.004  ...   \n",
      "1           999.2  6166415         MSNG         20.004  ...   \n",
      "\n",
      "  AVG WND DIR 10M PST2MTS MAX PK WND SPD 10M PST1HR  \\\n",
      "0                     270                      MSNG   \n",
      "1                    MSNG                      MSNG   \n",
      "\n",
      "              AVG WND SPD 10M PST2MTS  \\\n",
      "0                                20.4   \n",
      "1  MSNG MSNG MSNG MSNG MSNG MSNG MSNG   \n",
      "\n",
      "                                    DATA PAYLOAD URI AVG REL HUM PST1MT  \\\n",
      "0  /data/nav_canada/observation/atmospheric/surfa...               43.3   \n",
      "1  /data/nav_canada/observation/atmospheric/surfa...               61.5   \n",
      "\n",
      "  MIN AIR TEMP PST24HRS MAX AIR TEMP PST24HRS PCPN AMT PST1MT  \\\n",
      "0                  MSNG                  MSNG             0.0   \n",
      "1                  MSNG                  MSNG             0.0   \n",
      "\n",
      "  MAX WND SPD 10M PST1HR  OBS DATETIME UTC  \n",
      "0                   MSNG  2015/07/11 21:13  \n",
      "1                   MSNG  2015/07/09 19:15  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "\n",
      "Table 12 cleaned and saved\n"
     ]
    }
   ],
   "source": [
    "# Extract column names from header\n",
    "with open('./lake33/table_12.csv', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    header_line = f.readline().strip()\n",
    "\n",
    "column_parts = header_line.split('_\"')\n",
    "column_names = []\n",
    "for part in column_parts:\n",
    "    clean_name = part.replace('\"', '').replace('_', ' ').strip()\n",
    "    if clean_name:\n",
    "        column_names.append(clean_name)\n",
    "\n",
    "# Read data with underscore delimiter, skipping header\n",
    "df12 = pd.read_csv('./lake33/table_12.csv', sep='_', skiprows=1, header=None, on_bad_lines='skip')\n",
    "df12.columns = column_names\n",
    "\n",
    "\n",
    "\n",
    "# Function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "    cleaned = ''.join(char for char in text if ord(char) < 128)\n",
    "    cleaned = re.sub(r\"\\[|\\]|'\", '', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned if cleaned else text\n",
    "\n",
    "# Clean emojis from all columns\n",
    "for col in df12.columns:\n",
    "    df12[col] = df12[col].apply(remove_emojis)\n",
    "\n",
    "\n",
    "print(df12.head(2))\n",
    "\n",
    "df12.to_csv('./cleaned_table_12.csv', index=False)\n",
    "print(\"\\nTable 12 cleaned and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 12 and 11 do seem to be the same, howevewer table 12 has some out of place entries. For now lets just clean table 11 using the headers from table 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocean\\AppData\\Local\\Temp\\ipykernel_16600\\2339980841.py:2: DtypeWarning: Columns (9,10,16,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df12 = pd.read_csv('./cleaned_table_12.csv')\n",
      "C:\\Users\\ocean\\AppData\\Local\\Temp\\ipykernel_16600\\2339980841.py:9: DtypeWarning: Columns (9,10,16,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df11 = pd.read_csv('./lake33/table_11.csv', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using column names from table 12:\n",
      "['AVG WND DIR 10M PST10MTS', 'MAX AIR TEMP PST6HRS', 'FLD ELEV', 'PCPN AMT PST6HRS', 'DATA ATTRIB NOT', 'BARO ELEV', 'AVG MSLP PST1MT', 'MSC ID', 'LTNG DIS RMK', 'AVG VIS PST1MT', 'DNSTY ALTD', 'MX WND GST SPD 10M PST10MTS', 'AVG WND DIR 10M PST2MTS', 'MAX PK WND SPD 10M PST1HR', 'AVG WND SPD 10M PST2MTS', 'DATA PAYLOAD URI', 'AVG REL HUM PST1MT', 'MIN AIR TEMP PST24HRS', 'MAX AIR TEMP PST24HRS', 'PCPN AMT PST1MT', 'MAX WND SPD 10M PST1HR', 'OBS DATETIME UTC']\n",
      "\n",
      "Table 11 shape: (37750, 22)\n",
      "Columns assigned\n",
      "  AVG WND DIR 10M PST10MTS MAX AIR TEMP PST6HRS  FLD ELEV PCPN AMT PST6HRS  \\\n",
      "0                      271                 MSNG     281.0             MSNG   \n",
      "1                      103                 MSNG     191.4             MSNG   \n",
      "\n",
      "                                     DATA ATTRIB NOT  BARO ELEV  \\\n",
      "0  Observational data provided by NAV CANADA. All...      279.4   \n",
      "1  Observational data provided by NAV CANADA. All...      190.9   \n",
      "\n",
      "  AVG MSLP PST1MT   MSC ID LTNG DIS RMK AVG VIS PST1MT  ...  \\\n",
      "0           995.6  6115529         MSNG         20.004  ...   \n",
      "1           999.2  6166415         MSNG         20.004  ...   \n",
      "\n",
      "  AVG WND DIR 10M PST2MTS MAX PK WND SPD 10M PST1HR AVG WND SPD 10M PST2MTS  \\\n",
      "0                     270                      MSNG                    20.4   \n",
      "1                    MSNG                      MSNG                    MSNG   \n",
      "\n",
      "                                    DATA PAYLOAD URI AVG REL HUM PST1MT  \\\n",
      "0  /data/nav_canada/observation/atmospheric/surfa...               43.3   \n",
      "1  /data/nav_canada/observation/atmospheric/surfa...               61.5   \n",
      "\n",
      "  MIN AIR TEMP PST24HRS MAX AIR TEMP PST24HRS PCPN AMT PST1MT  \\\n",
      "0                  MSNG                  MSNG             0.0   \n",
      "1                  MSNG                  MSNG             0.0   \n",
      "\n",
      "  MAX WND SPD 10M PST1HR  OBS DATETIME UTC  \n",
      "0                   MSNG  2015/07/11 21:13  \n",
      "1                   MSNG  2015/07/09 19:15  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "\n",
      "Table 11 saved with proper headers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the column names from cleaned table 12\n",
    "df12 = pd.read_csv('./cleaned_table_12.csv')\n",
    "column_names = df12.columns.tolist()\n",
    "\n",
    "print(f\"Using column names from table 12:\")\n",
    "print(column_names)\n",
    "\n",
    "# Load table 11 WITHOUT treating first row as header\n",
    "df11 = pd.read_csv('./lake33/table_11.csv', header=None)\n",
    "\n",
    "print(f\"\\nTable 11 shape: {df11.shape}\")\n",
    "\n",
    "# Assign the proper column names\n",
    "df11.columns = column_names\n",
    "\n",
    "print(f\"Columns assigned\")\n",
    "print(df11.head(2))\n",
    "\n",
    "df11.to_csv('./cleaned_table_11.csv', index=False)\n",
    "print(\"\\nTable 11 saved with proper headers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 11 has a lot less rows than table 12, further hinting at duplicates or wrong entries. Lets further clean table 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ocean\\AppData\\Local\\Temp\\ipykernel_16600\\3812275199.py:2: DtypeWarning: Columns (9,10,16,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df12 = pd.read_csv('./cleaned_table_12.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before additional cleaning: 37,750 rows\n",
      "Duplicates before removal: 0\n",
      "After cleaning and deduplication: 37,750 rows\n",
      "Table 12 re-cleaned and saved\n"
     ]
    }
   ],
   "source": [
    "# Load current cleaned table 12\n",
    "df12 = pd.read_csv('./cleaned_table_12.csv')\n",
    "\n",
    "print(f\"Before additional cleaning: {df12.shape[0]:,} rows\")\n",
    "\n",
    "# Improved cleaning function\n",
    "def clean_value(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "    # Remove newlines and extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    # If it's repeated words/numbers separated by spaces, keep only one\n",
    "    parts = text.split()\n",
    "    if len(parts) > 1 and len(set(parts)) == 1:\n",
    "        text = parts[0]\n",
    "    return text.strip()\n",
    "\n",
    "# Apply improved cleaning\n",
    "for col in df12.columns:\n",
    "    df12[col] = df12[col].apply(clean_value)\n",
    "\n",
    "# Remove duplicates\n",
    "print(f\"Duplicates before removal: {df12.duplicated().sum():,}\")\n",
    "df12 = df12.drop_duplicates()\n",
    "\n",
    "print(f\"After cleaning and deduplication: {df12.shape[0]:,} rows\")\n",
    "\n",
    "df12.to_csv('./cleaned_table_12.csv', index=False)\n",
    "print(\"Table 12 re-cleaned and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now table 12 has the same amount of rows as 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 13 seems fine and doesn't need cleaning. Continue to Table 14 which has a few duplicates according to the report, and has the blank space as seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 14 after tab parsing:\n",
      "Shape: (43066, 37)\n",
      "Columns (37): ['GRADES_E', 'LANE_WIDTH', 'SURV_H', 'CURVES_A', 'YEAR_RECORD', 'PERC_COMBINATION', 'SURV_G', 'HORZ_ALIGN', 'WIDE_FEAS', 'URBAN_CODE', 'THROUGH_LANES', 'DONUT_VOLUME_GROUP', 'IS_DONUT', 'SPEED_LIMIT', 'SHOULDER_WIDTH_L', 'GF_SYSTEM', 'ACCESS_CONTROL', 'UNBUILT_FACILITY', 'AT_GRADE_SIGNS', 'F_SYSTEM', 'OWNERSHIP', 'AT_GRADE_OTHER', 'MEDIAN_TYPE', 'IS_SAMPLE', 'AT_GRADE_SIGNAL', 'PEAK_PARKING', 'IRI', 'GRADES_A', 'SURV_D', 'PSR', 'CURVES_B', 'ROUTE_QUALIFIER', 'SHOULDER_TYPE', 'YEAR_SURF_IMPROV', 'IS_GROUPED', 'SURV_I', 'VERT_ALIGN']\n",
      "\n",
      "Duplicate rows: 57\n",
      "Duplicate percentage: 0.13%\n",
      "After deduplication: 43,009 rows\n",
      "\n",
      "First 2 rows:\n",
      "   GRADES_E  LANE_WIDTH  SURV_H  CURVES_A  YEAR_RECORD  PERC_COMBINATION  \\\n",
      "0       0.0        12.0       0      0.15         2006                 3   \n",
      "1       0.0        12.0       0      0.32         2006                17   \n",
      "\n",
      "   SURV_G  HORZ_ALIGN  WIDE_FEAS  URBAN_CODE  ...  GRADES_A  SURV_D  PSR  \\\n",
      "0       0           0          3          34  ...      0.15       0  0.0   \n",
      "1       0           1          5           0  ...      0.10       0  0.0   \n",
      "\n",
      "   CURVES_B  ROUTE_QUALIFIER  SHOULDER_TYPE  YEAR_SURF_IMPROV  IS_GROUPED  \\\n",
      "0       0.0                0              4                 0           0   \n",
      "1       0.0                0              2              1997           0   \n",
      "\n",
      "   SURV_I  VERT_ALIGN  \n",
      "0       0           0  \n",
      "1       0           1  \n",
      "\n",
      "[2 rows x 37 columns]\n",
      "\n",
      "Table 14 cleaned and saved\n"
     ]
    }
   ],
   "source": [
    "# Parse table 14 with tab delimiter\n",
    "df14 = pd.read_csv('./lake33/table_14.csv', sep='\\t')\n",
    "\n",
    "print(\"Table 14 after tab parsing:\")\n",
    "print(f\"Shape: {df14.shape}\")\n",
    "print(f\"Columns ({len(df14.columns)}): {df14.columns.tolist()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df14.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates:,}\")\n",
    "print(f\"Duplicate percentage: {(duplicates / df14.shape[0] * 100):.2f}%\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df14 = df14.drop_duplicates()\n",
    "    print(f\"After deduplication: {df14.shape[0]:,} rows\")\n",
    "\n",
    "print(f\"\\nFirst 2 rows:\")\n",
    "print(df14.head(2))\n",
    "\n",
    "# Save it\n",
    "df14.to_csv('./cleaned_table_14.csv', index=False)\n",
    "print(\"\\nTable 14 cleaned and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now table 15 and 16 look similar. Lets start with removing their emojis and fixing the seperators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 15 saved\n",
      "\n",
      "Table 16 saved\n"
     ]
    }
   ],
   "source": [
    "def remove_emojis(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "    cleaned = ''.join(char for char in text if ord(char) < 128)\n",
    "    cleaned = re.sub(r\"\\[|\\]|'\", '', cleaned)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned if cleaned else text\n",
    "\n",
    "# Clean Table 15\n",
    "df15 = pd.read_csv('./lake33/table_15.csv', sep='_', on_bad_lines='skip')\n",
    "clean_cols_15 = [col.replace('\"', '').strip() for col in df15.columns]\n",
    "df15.columns = clean_cols_15\n",
    "\n",
    "\n",
    "\n",
    "for col in df15.columns:\n",
    "    df15[col] = df15[col].apply(remove_emojis)\n",
    "\n",
    "\n",
    "df15.to_csv('./cleaned_table_15.csv', index=False)\n",
    "print(\"Table 15 saved\\n\")\n",
    "\n",
    "# Clean Table 16\n",
    "df16 = pd.read_csv('./lake33/table_16.csv', sep='_', on_bad_lines='skip')\n",
    "clean_cols_16 = [col.replace('\"', '').strip() for col in df16.columns]\n",
    "df16.columns = clean_cols_16\n",
    "\n",
    "\n",
    "for col in df16.columns:\n",
    "    df16[col] = df16[col].apply(remove_emojis)\n",
    "\n",
    "\n",
    "df16.to_csv('./cleaned_table_16.csv', index=False)\n",
    "print(\"Table 16 saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 18 seems to be the same as 17 but with scrambled headers, so lets use the headers from clean table 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using column names from table 17:\n",
      "['received_date', 'Auth_Description', 'Op_Name', 'Short Notice Short Notice', 'Description', 'Variation Number Variation Number', 'Service_Type_Other_Details']\n",
      "\n",
      "Table 18 parsed shape: (34959, 7)\n",
      "Columns assigned: ['received_date', 'Auth_Description', 'Op_Name', 'Short Notice Short Notice', 'Description', 'Variation Number Variation Number', 'Service_Type_Other_Details']\n",
      "Duplicates: 8129\n",
      "After deduplication: 26830 rows\n",
      "  received_date     Auth_Description                      Op_Name  \\\n",
      "0      20/03/20  Dundee City Council  FIFE SCOTTISH OMNIBUSES LTD   \n",
      "1      14/02/10        Moray Council       BLUEBIRD BUSES LIMITED   \n",
      "\n",
      "  Short Notice Short Notice             Description  \\\n",
      "0                       Yes  Standard International   \n",
      "1                        No  Standard International   \n",
      "\n",
      "   Variation Number Variation Number  \\\n",
      "0                                  6   \n",
      "1                                  3   \n",
      "\n",
      "                          Service_Type_Other_Details  \n",
      "0  Service varied (reduced timetable) due to Covi...  \n",
      "1                                                NaN  \n",
      "\n",
      "Table 18 cleaned and saved\n"
     ]
    }
   ],
   "source": [
    "df17 = pd.read_csv('./cleaned_table_17.csv')\n",
    "column_names = df17.columns.tolist()\n",
    "\n",
    "print(f\"Using column names from table 17:\")\n",
    "print(column_names)\n",
    "\n",
    "# Parse table 18 with underscore delimiter, skip the scrambled header row\n",
    "df18 = pd.read_csv('./lake33/table_18.csv', sep='_', skiprows=1, header=None, on_bad_lines='skip')\n",
    "\n",
    "print(f\"\\nTable 18 parsed shape: {df18.shape}\")\n",
    "\n",
    "# Assign the proper column names\n",
    "df18.columns = column_names\n",
    "\n",
    "print(f\"Columns assigned: {df18.columns.tolist()}\")\n",
    "\n",
    "print(f\"Duplicates: {df18.duplicated().sum()}\")\n",
    "df18 = df18.drop_duplicates()\n",
    "print(f\"After deduplication: {df18.shape[0]} rows\")\n",
    "\n",
    "print(df18.head(2))\n",
    "\n",
    "df18.to_csv('./cleaned_table_18.csv', index=False)\n",
    "print(\"\\nTable 18 cleaned and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table_0.csv...\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Removed 6630 duplicate rows\n",
      "  Saved to cleaned_table_0.csv (shape: (15637, 52))\n",
      "Processing table_1.csv...\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Saved to cleaned_table_1.csv (shape: (23518, 64))\n",
      "Processing table_11.csv...\n",
      "Warning: table_12.csv not processed yet, using default headers for table_11\n",
      "  Saved to cleaned_table_11.csv (shape: (37750, 22))\n",
      "Processing table_12.csv...\n",
      "  Saved to cleaned_table_12.csv (shape: (37750, 22))\n",
      "Processing table_13.csv...\n",
      "  Saved to cleaned_table_13.csv (shape: (30176, 41))\n",
      "Processing table_14.csv...\n",
      "  Removed 57 duplicate rows\n",
      "  Saved to cleaned_table_14.csv (shape: (43009, 37))\n",
      "Processing table_15.csv...\n",
      "  Removed 3 duplicate rows\n",
      "  Saved to cleaned_table_15.csv (shape: (10892, 32))\n",
      "Processing table_16.csv...\n",
      "  Removed 1 duplicate rows\n",
      "  Saved to cleaned_table_16.csv (shape: (7167, 32))\n",
      "Processing table_17.csv...\n",
      "  Removed 6048 duplicate rows\n",
      "  Saved to cleaned_table_17.csv (shape: (22733, 7))\n",
      "Processing table_18.csv...\n",
      "  Removed 8149 duplicate rows\n",
      "  Saved to cleaned_table_18.csv (shape: (26810, 7))\n",
      "Processing table_2.csv...\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column '% of Businesses Reporting': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Warning: Error cleaning column 'Quality Indicator': The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "  Saved to cleaned_table_2.csv (shape: (23518, 64))\n",
      "Processing table_3.csv...\n",
      "  Saved to cleaned_table_3.csv (shape: (11448, 1))\n",
      "Processing table_4.csv...\n",
      "  Saved to cleaned_table_4.csv (shape: (34057, 12))\n",
      "Processing table_5.csv...\n",
      "  Saved to cleaned_table_5.csv (shape: (38016, 14))\n",
      "Processing table_6.csv...\n",
      "  Saved to cleaned_table_6.csv (shape: (545291, 14))\n",
      "Processing table_7.csv...\n",
      "  Saved to cleaned_table_7.csv (shape: (65649, 35))\n",
      "Processing table_8.csv...\n",
      "  Saved to cleaned_table_8.csv (shape: (34868, 44))\n",
      "Processing table_9.csv...\n",
      "  Removed 4 duplicate rows\n",
      "  Saved to cleaned_table_9.csv (shape: (26481, 14))\n",
      "\n",
      "Cleaning completed! Processed 18 tables.\n"
     ]
    }
   ],
   "source": [
    "## Cleaning data, scrubbing, washing, mopping\n",
    "\n",
    "def cleaningData(table_names=None, base_dir='./lake33', output_dir='./'):\n",
    "    \"\"\"Comprehensive data cleaning function that handles all table-specific issues\n",
    "    \n",
    "    Args:\n",
    "        table_names (list): List of table names to clean (e.g., ['table_0.csv', 'table_1.csv'])\n",
    "                           If None, cleans all tables in base_dir\n",
    "        base_dir (str): Directory containing the raw tables\n",
    "        output_dir (str): Directory to save cleaned tables\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping table names to cleaned DataFrames\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import os\n",
    "    \n",
    "    def remove_emojis(text):\n",
    "        \"\"\"Remove emojis and non-ASCII characters from text\"\"\"\n",
    "        # Handle various input types\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        if isinstance(text, (int, float)):\n",
    "            return text\n",
    "        \n",
    "        text = str(text)\n",
    "        # Remove emojis (characters outside basic ASCII)\n",
    "        try:\n",
    "            cleaned = ''.join(char for char in text if ord(char) < 128)\n",
    "            # Clean up list-like patterns\n",
    "            cleaned = re.sub(r'\\[|\\]|\\'', '', cleaned)\n",
    "            cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "            cleaned = cleaned.strip()\n",
    "            # Return original if cleaning resulted in empty string\n",
    "            if len(cleaned) == 0:\n",
    "                return text\n",
    "            return cleaned\n",
    "        except Exception:\n",
    "            return str(text)\n",
    "    \n",
    "    def clean_column_names(columns, table_name):\n",
    "        \"\"\"Clean column names based on table-specific issues\"\"\"\n",
    "        clean_cols = []\n",
    "        \n",
    "        for col in columns:\n",
    "            col_str = str(col)\n",
    "            \n",
    "            # Handle BOM characters (tables 3, 4, 5)\n",
    "            col_str = col_str.replace('\\ufeff', '').replace('ï»¿', '')\n",
    "            \n",
    "            # Handle multilingual text - extract English part before newline\n",
    "            if '\\\\\\\\n' in col_str:\n",
    "                col_str = col_str.split('\\\\\\\\n')[0]\n",
    "            elif '\\\\n' in col_str:\n",
    "                col_str = col_str.split('\\\\n')[0]\n",
    "            elif '\\n' in col_str:\n",
    "                col_str = col_str.split('\\n')[0]\n",
    "            \n",
    "            # Remove quotes, brackets, and list formatting\n",
    "            col_str = col_str.replace('\"', '').replace('[', '').replace(']', '').replace(\"'\", '')\n",
    "            \n",
    "            # Remove emojis and non-ASCII characters\n",
    "            col_str = ''.join(char for char in col_str if ord(char) < 128)\n",
    "            \n",
    "            # Clean up spaces\n",
    "            col_str = ' '.join(col_str.split()).strip()\n",
    "            \n",
    "            # Remove duplicate column names (like \"ROUTE_NUMBER ROUTE_NUMBER\")\n",
    "            parts = col_str.split()\n",
    "            if len(parts) >= 2 and parts[0] == parts[1]:\n",
    "                col_str = parts[0]\n",
    "            \n",
    "            clean_cols.append(col_str if col_str else f'col_{len(clean_cols)}')\n",
    "        \n",
    "        return clean_cols\n",
    "    \n",
    "    def get_separator(table_name):\n",
    "        \"\"\"Determine appropriate separator for each table\"\"\"\n",
    "        underscore_tables = ['table_0.csv', 'table_1.csv', 'table_8.csv', 'table_9.csv', \n",
    "                           'table_12.csv', 'table_15.csv', 'table_16.csv', 'table_17.csv', 'table_18.csv']\n",
    "        tab_tables = ['table_14.csv']\n",
    "        \n",
    "        if table_name in underscore_tables:\n",
    "            return '_'\n",
    "        elif table_name in tab_tables:\n",
    "            return '\\t'\n",
    "        else:\n",
    "            return ','\n",
    "    \n",
    "    def load_table_with_fallback(filepath, table_name):\n",
    "        \"\"\"Load table with appropriate parsing strategy\"\"\"\n",
    "        separator = get_separator(table_name)\n",
    "        \n",
    "        try:\n",
    "            # Special handling for tables with header issues\n",
    "            if table_name == 'table_11.csv':\n",
    "                # No header, will assign later\n",
    "                return pd.read_csv(filepath, header=None, low_memory=False)\n",
    "            elif table_name == 'table_12.csv':\n",
    "                # Extract header manually\n",
    "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    header_line = f.readline().strip()\n",
    "                column_parts = header_line.split('_\"')\n",
    "                column_names = [part.replace('\"', '').replace('_', ' ').strip() \n",
    "                              for part in column_parts if part.strip()]\n",
    "                df = pd.read_csv(filepath, sep=separator, skiprows=1, header=None, on_bad_lines='skip', low_memory=False)\n",
    "                if len(df.columns) == len(column_names):\n",
    "                    df.columns = column_names\n",
    "                return df\n",
    "            elif table_name == 'table_18.csv':\n",
    "                # Skip scrambled header\n",
    "                return pd.read_csv(filepath, sep=separator, skiprows=1, header=None, on_bad_lines='skip', low_memory=False)\n",
    "            else:\n",
    "                # Standard loading\n",
    "                return pd.read_csv(filepath, sep=separator, on_bad_lines='skip', low_memory=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {table_name}: {e}\")\n",
    "            # Fallback to basic loading\n",
    "            try:\n",
    "                return pd.read_csv(filepath, low_memory=False)\n",
    "            except:\n",
    "                return None\n",
    "    \n",
    "    # Get list of tables to process\n",
    "    if table_names is None:\n",
    "        table_names = [f for f in os.listdir(base_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    cleaned_datasets = {}\n",
    "    \n",
    "    # Process each table\n",
    "    for table_name in table_names:\n",
    "        print(f\"Processing {table_name}...\")\n",
    "        filepath = os.path.join(base_dir, table_name)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Warning: {filepath} not found\")\n",
    "            continue\n",
    "        \n",
    "        # Load the table\n",
    "        df = load_table_with_fallback(filepath, table_name)\n",
    "        if df is None:\n",
    "            print(f\"Failed to load {table_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Handle special cases for header assignment\n",
    "        if table_name == 'table_11.csv':\n",
    "            # Use headers from table 12 if available\n",
    "            if 'table_12.csv' in cleaned_datasets:\n",
    "                df.columns = cleaned_datasets['table_12.csv'].columns\n",
    "            else:\n",
    "                print(\"Warning: table_12.csv not processed yet, using default headers for table_11\")\n",
    "        elif table_name == 'table_18.csv':\n",
    "            # Use headers from table 17 if available\n",
    "            if 'table_17.csv' in cleaned_datasets:\n",
    "                if len(df.columns) == len(cleaned_datasets['table_17.csv'].columns):\n",
    "                    df.columns = cleaned_datasets['table_17.csv'].columns\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = clean_column_names(df.columns, table_name)\n",
    "        \n",
    "        # Handle special column fixes\n",
    "        if table_name == 'table_5.csv' and len(df.columns) > 0:\n",
    "            # Fix BOM in first column\n",
    "            cols = df.columns.tolist()\n",
    "            cols[0] = 'REF_DATE'\n",
    "            df.columns = cols\n",
    "        \n",
    "        # Define additional cleaning function\n",
    "        def clean_value(text):\n",
    "            if pd.isna(text):\n",
    "                return text\n",
    "            text = str(text)\n",
    "            # Remove newlines and extra whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            # If it's repeated words/numbers separated by spaces, keep only one\n",
    "            parts = text.split()\n",
    "            if len(parts) > 1 and len(set(parts)) == 1:\n",
    "                text = parts[0]\n",
    "            return text.strip()\n",
    "        \n",
    "        # Clean data values - only apply text cleaning to object/string columns\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                # Only apply text cleaning to non-numeric columns\n",
    "                if df[col].dtype == 'object' or df[col].dtype.name == 'string':\n",
    "                    # Check if column actually contains text data\n",
    "                    sample_values = df[col].dropna().head(5)\n",
    "                    if len(sample_values) > 0:\n",
    "                        # Only apply cleaning if we have string-like data\n",
    "                        first_val = str(sample_values.iloc[0])\n",
    "                        if any(ord(char) > 127 for char in first_val) or '[' in first_val or '\"' in first_val:\n",
    "                            df[col] = df[col].apply(remove_emojis)\n",
    "                        df[col] = df[col].apply(clean_value)\n",
    "                else:\n",
    "                    # For numeric columns, just convert to string and back to clean formatting\n",
    "                    pass\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Error cleaning column '{col}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Remove duplicates\n",
    "        initial_rows = len(df)\n",
    "        df = df.drop_duplicates()\n",
    "        duplicates_removed = initial_rows - len(df)\n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"  Removed {duplicates_removed} duplicate rows\")\n",
    "        \n",
    "        # Store cleaned dataset\n",
    "        cleaned_datasets[table_name] = df\n",
    "        \n",
    "        # Save cleaned version\n",
    "        clean_filename = f\"cleaned_{table_name}\"\n",
    "        output_path = os.path.join(output_dir, clean_filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"  Saved to {clean_filename} (shape: {df.shape})\")\n",
    "    \n",
    "    print(f\"\\nCleaning completed! Processed {len(cleaned_datasets)} tables.\")\n",
    "    return cleaned_datasets\n",
    "\n",
    "cleaned_data = cleaningData()\n",
    "# Clean specific tables: cleaned_data = cleaningData(['table_1.csv', 'table_2.csv'])\n",
    "# Clean with custom directories: cleaned_data = cleaningData(base_dir='./raw_data', output_dir='./clean_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running the above discovery methods on cleaned data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T14:49:01.988282400Z",
     "start_time": "2025-10-01T14:42:07.812210400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [06:35<00:00, 24.71s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 10.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:04<00:00,  3.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reading data from the cleaned tables. Since tables 6,7 and 10 have been removed, we no longer attempt to read them.\n",
    "tables_clean = []\n",
    "for i in range(0, 19):\n",
    "    if i in [6,7,10]:\n",
    "        continue\n",
    "    ct = read_csv(f'lake33c/cleaned_table_{i}.csv')\n",
    "    ct.name = f'cleaned_table_{i}.csv'\n",
    "    tables_clean.append(ct)\n",
    "\n",
    "result_sc_clean = set_containment_method(tables_clean)\n",
    "result_sc_clean.to_csv('outputs/set_containment_results_clean.csv', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "result_cn_clean = column_names(tables_clean)\n",
    "result_cn_clean.to_csv('outputs/column_name_results_clean.csv', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "result_josie_clean = josie(tables_clean)\n",
    "result_josie_clean.to_csv('outputs/josie_results_clean.csv', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "1)  Different aspects of the data can effect the data discovery process. Write a short report on your findings. Such as which data quality issues had the largest effect on data discovery. Which data quality problem was repairable and how you choose to do the repair.\n",
    "\n",
    "<!-- For the set of considerations that you have outlined for the choice of data discovery methods, choose one and identify under this new constraint, how would you identify and resolve this problem? -->\n",
    "\n",
    "Max 400 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
